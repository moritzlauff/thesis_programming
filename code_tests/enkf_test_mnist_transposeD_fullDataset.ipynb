{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../python/functions\")\n",
    "sys.path.insert(2, \"../python/architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_prep_functions import mnist_prep\n",
    "from model_functions import *\n",
    "from plotting_functions import *\n",
    "import no_gpu\n",
    "import reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = mnist_prep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnKF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 600     # len(X_train)\n",
    "epochs = 20\n",
    "particles = 1000\n",
    "early_stopping = False\n",
    "early_stopping_diff = 0.001\n",
    "batch_normal = False # evtl. noch einbauen, obwohl im Paper nicht gemacht (aber Achtung mit den Dimensionen unten!!!)\n",
    "shuffle = True\n",
    "randomization = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 5\n",
    "neurons = [128, 128, 64, 32, 10]\n",
    "n_cols = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.005\n",
    "h_0 = 2\n",
    "epsilon = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X_train)\n",
    "num_batches = int(np.ceil(n / batch_size))\n",
    "batch_indices = np.cumsum([0] + list(np.ones(num_batches) * batch_size))\n",
    "batch_indices[-1] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "weights_dict = {}\n",
    "y_pred_dict = {}\n",
    "jacobian_dict = {}\n",
    "weights_vector_dict = {}\n",
    "train_acc_dict = {}\n",
    "test_acc_dict = {}\n",
    "iteration_dict = {}\n",
    "\n",
    "# init_model already has weights and biases following the Glorot distribution\n",
    "# it can already be used to predict and evaluate, but it is very bad (<10% accuracy)\n",
    "# only used to determine shapes and shape_elements via its weights\n",
    "init_model = nn_model_structure(layers = layers,\n",
    "                                neurons = neurons,\n",
    "                                n_cols = n_cols)\n",
    "init_model = nn_model_compile(init_model,\n",
    "                              optimizer = \"sgd\")\n",
    "weights = init_model.get_weights()\n",
    "# shape contains the shapes of the weight matrices and bias vectors as a list of arrays\n",
    "shapes = [np.array(params.shape) for params in weights]\n",
    "# shape_elements contains the indices of the weights as a vector and tells where to cut\n",
    "shape_elements = np.cumsum([0] + [np.prod(shape) for shape in shapes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(particles):\n",
    "    # just an initial model with the correct structure regarding neurons, layers, activation functions, Glorot initialization\n",
    "    model = nn_model_structure(layers = layers,\n",
    "                               neurons = neurons,\n",
    "                               n_cols = n_cols)\n",
    "    model = nn_model_compile(model,\n",
    "                             optimizer = \"sgd\")\n",
    "    # for every particle write the model in a dictionary\n",
    "    model_dict[\"model_{}\".format(str(i+1))] = model\n",
    "    \n",
    "    # for every particles write the weights and biases in a dictionary\n",
    "    weights_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                    .get_weights()\n",
    "    \n",
    "    train_acc_dict[\"model_{}\".format(str(i+1))] = []\n",
    "    test_acc_dict[\"model_{}\".format(str(i+1))] = []\n",
    "    iteration_dict[\"model_{}\".format(str(i+1))] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_model as the model with the mean of the weights of all particle models\n",
    "mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "mean_model = init_model\n",
    "mean_model.set_weights(mean_weights)\n",
    "\n",
    "mean_model_train_acc = np.array(mean_model.evaluate(X_train, y_train)[1])\n",
    "mean_model_test_acc = np.array(mean_model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(particles):\n",
    "    print(model_dict[\"model_{}\".format(str(i+1))].evaluate(X_val_small, y_val_small)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all epochs\n",
    "for epoch in range(epochs):\n",
    "    # shuffle the data\n",
    "    if shuffle:\n",
    "        indices = y_train.sample(frac=1).index\n",
    "        X_batches = [X_train[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "        y_batches = [y_train.iloc[indices].reset_index(drop = True)[int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]   \n",
    "    # loop over all batches\n",
    "    for b in range(num_batches):    \n",
    "        for i in range(particles):\n",
    "            # set new weights for model\n",
    "            model_dict[\"model_{}\".format(str(i+1))].set_weights(weights_dict[\"model_{}\".format(str(i+1))])\n",
    "            \n",
    "            # for every particle write the predictions on the training batches in a dictionary\n",
    "            y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                            .predict(X_batches[b])\n",
    "\n",
    "            # for every particle write the Jacobian in a dictionary\n",
    "            jacobian_dict[\"model_{}\".format(str(i+1))] = (-1) * np.multiply(np.array(y_batches[b]), \n",
    "                                                                            np.array(1 / (y_pred_dict[\"model_{}\".format(str(i+1))] + delta)))\n",
    "            \n",
    "        # compute the mean of the predictions\n",
    "        y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "        \n",
    "        # compute the matrix D elementwise\n",
    "        d = np.zeros(shape = (particles, particles))\n",
    "        for k in range(particles):\n",
    "            y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "            for j in range(particles):\n",
    "                d[k][j] = np.sum(np.multiply(y_pred_centered, jacobian_dict[\"model_{}\".format(str(j+1))]))\n",
    "        d = np.transpose(d)  \n",
    "        \n",
    "        # compute the scalar h_t\n",
    "        h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "        \n",
    "        # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "        for i in range(particles):\n",
    "            weights_array = np.array([])\n",
    "            for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "            weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "          \n",
    "        # matrix with particle parameters as row vectors\n",
    "        weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "        # compute the matrix with the updates for each particle\n",
    "        weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "        for i in range(particles):\n",
    "            # write the updates back into the dictionary\n",
    "            weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "            # reshape the updates, so that they are of the original matrx and vector shape\n",
    "            for l in range(len(shape_elements)-1):\n",
    "                start = shape_elements[l]\n",
    "                end = shape_elements[l+1]\n",
    "                weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "            if randomization:\n",
    "                # add randomization/ noise to each particle\n",
    "                new_weights = []\n",
    "                # standard deviation for scaled Glorot distribution\n",
    "                for s in range(len(shapes)):\n",
    "                    if shapes[s].shape[0] == 2:\n",
    "                        fan_in = shapes[s][0]\n",
    "                        fan_out = shapes[s][1]\n",
    "                    if shapes[s].shape[0] == 1:\n",
    "                        fan_in = shapes[s-1][0]\n",
    "                        fan_out = shapes[s][0]\n",
    "                    stddev = np.sqrt(np.sqrt(h_t)) * np.sqrt(2 / (fan_in + fan_out))\n",
    "                    noise = np.random.normal(loc = 0.0,\n",
    "                                             scale = stddev,\n",
    "                                             size = tuple(shapes[s]))\n",
    "                    new_weights.append(weights_dict[\"model_{}\".format(str(i+1))][s] + noise)\n",
    "                weights_dict[\"model_{}\".format(str(i+1))] = new_weights\n",
    "                \n",
    "    if randomization:\n",
    "        # randomize particles around their mean\n",
    "        weights_mean = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "        for i in range(particles):\n",
    "            new_weights = []\n",
    "            # standard deviation for Glorot distribution\n",
    "            for s in range(len(shapes)):\n",
    "                if shapes[s].shape[0] == 2:\n",
    "                    fan_in = shapes[s][0]\n",
    "                    fan_out = shapes[s][1]\n",
    "                if shapes[s].shape[0] == 1:\n",
    "                    fan_in = shapes[s-1][0]\n",
    "                    fan_out = shapes[s][0]\n",
    "                stddev = np.sqrt(2 / (fan_in + fan_out))\n",
    "                noise = np.random.normal(loc = 0.0,\n",
    "                                         scale = stddev,\n",
    "                                         size = tuple(shapes[s]))\n",
    "                new_weights.append(weights_mean[s] + noise)\n",
    "            weights_dict[\"model_{}\".format(str(i+1))] = new_weights\n",
    "            \n",
    "    for i in range(particles):\n",
    "        # for every particle write the training accuracy of the current iteration in a dictionary\n",
    "        train_acc_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                  .evaluate(X_train, y_train, verbose = 0)[1])\n",
    "\n",
    "        # for every particle write the test accuracy of the current iteration in a dictionary\n",
    "        test_acc_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                  .evaluate(X_test, y_test, verbose = 0)[1])\n",
    "\n",
    "        # for every particle write the current iteration in a dictionary\n",
    "        iteration_dict[\"model_{}\".format(str(i+1))].append(\"Epoch: {}, Batch: {}.\".format(epoch+1, b+1))\n",
    "            \n",
    "    # update the mean_model\n",
    "    mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "    mean_model.set_weights(mean_weights)\n",
    "    \n",
    "    mean_model_train_acc = np.append(mean_model_train_acc, np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1]))\n",
    "    mean_model_test_acc = np.append(mean_model_test_acc, np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1]))\n",
    "    \n",
    "    print(\"Training Accuracy after Epoch {}: {}\".format(str(epoch+1), str(np.round(mean_model_train_acc[-1], 3))))\n",
    "    \n",
    "    # early stopping\n",
    "    if early_stopping:\n",
    "        if epoch == 0:\n",
    "            test_acc_old = 0\n",
    "        else:\n",
    "            test_acc_new = mean_model_test_acc[epoch]\n",
    "            if np.absolute(test_acc_new - test_acc_old) <= early_stopping_diff:\n",
    "                print(\"STOP: Early Stopping after epoch {} because improvement in test accuracy is only {}.\"\\\n",
    "                                                                     .format(epoch+1, test_acc_new - test_acc_old))\n",
    "                break\n",
    "            test_acc_old = test_acc_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(\"Calculation time: {} minutes.\".format((end_time - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dict[\"model_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian_dict[\"model_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(mean_model_train_acc, 3))\n",
    "print(np.round(mean_model_test_acc, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_plot_iter_acc(train_acc_dict[\"model_1\"],\n",
    "                 test_acc_dict[\"model_1\"],\n",
    "                 iteration_dict[\"model_1\"],\n",
    "                 mean_comparison = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_plot_epoch_acc(mean_model_train_acc,\n",
    "                  mean_model_test_acc,\n",
    "                  mean_comparison = 0.1,\n",
    "                  savefig = True,\n",
    "                  file = \"../img/enkf_model_mnist_full_E{}_B{}_P{}_H{}.png\".format(epochs, batch_size, particles, h_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_save(mean_model, \n",
    "        \"../models/enkf_model_mnist_full_E{}_B{}_P{}_H{}.h5\".format(epochs, batch_size, particles, h_0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
