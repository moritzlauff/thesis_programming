{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../python/functions\")\n",
    "sys.path.insert(2, \"../python/architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../python/architecture\\reproducible.py:14: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_prep_functions import wine_prep\n",
    "from plotting_functions import nn_plot_mse\n",
    "from enkf_functions import enkf_regressor\n",
    "from saving_functions import load_objects, save_objects\n",
    "from model_functions import nn_load, nn_model_structure, nn_model_compile\n",
    "import reproducible\n",
    "import no_gpu\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = wine_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use samller dataset for increased speed\n",
    "X_train = X_train[:1000]\n",
    "X_test = X_test[:500]\n",
    "y_train = y_train[:1000]\n",
    "y_test = y_test[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 4\n",
    "neurons = [32, 32, 16, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enkf_regressor_analysis(X_train,\n",
    "                            X_test,\n",
    "                            y_train,\n",
    "                            y_test,\n",
    "                            layers,\n",
    "                            neurons,\n",
    "                            setting_dict,\n",
    "                            analysis_dict,\n",
    "                            save_all = False,\n",
    "                            file_var = \"file.pckl\",\n",
    "                            file_model = \"file.h5\",\n",
    "                            verbose = 0\n",
    "                            ):\n",
    "\n",
    "    \"\"\" Ensemble Kalman Filter algorithm analysis for regression problems.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train (np.ndarray): Training data X.\n",
    "    X_test (np.ndarray): Test data X.\n",
    "    y_train (pd.DataFrame): Training data y.\n",
    "    y_test (pd.DataFrame): Test data y.\n",
    "    layers (int): Number of layers.\n",
    "    neurons (list): Number of neurons in each layer.\n",
    "    setting_dict (dict): Dictionary containing\n",
    "        particles (int): Number of particles in the ensemble.\n",
    "        epochs (int): Number of epochs.\n",
    "        batch_size (None or int): Size of the batches. Must be between 0 and the number of observations in the training set.\n",
    "        h_0 (int or float): Starting step size.\n",
    "        delta (float): Constant for numerical stability in the jacobian.\n",
    "        epsilon (float): Constant for numerical stability in the step size.\n",
    "        randomization (bool): Whether or not to add noise to the particles and randomize them around their mean.\n",
    "        shuffle (bool): Whether or not to shuffle the data prior to each epoch.\n",
    "        early_stopping (bool): Whether or not to stop the calculation when the changes get small.\n",
    "        early_stopping_diff (bool): Minimum change before early stopping is applied.\n",
    "    analysis_dict (dict): Dictionary containing\n",
    "        disjoint_batch (bool): Whether or not to use disjoint batches. If False then each batch is sampled with replacement.\n",
    "        multiple_updates_same_batch (int or None): Number of consecutive updates on the same batch.\n",
    "        batch_particle_connection (dict): Dictionary containing\n",
    "            connect (bool): Whether or not to connect particles and batches.\n",
    "            shuffle (bool): Whether or not to shuffle the connection.\n",
    "            update_all (bool): Whether or not to update after all particles have seen some data.\n",
    "        batch_evaluation (dict): Dictionary containing\n",
    "            mean_model (bool): Whether or not evaluate the mean model after every batch on that batch. Warning: the computational costs are very high.\n",
    "            particles (bool): Whether or not evaluate every particle model after every batch on that batch. Warning: the computational costs are incredibly high.\n",
    "            file (str): Path and name of the file to save evaluations into \n",
    "        tikhonov (dict): Dictionary containing\n",
    "            regularize (bool): Whether or not to use Tikhonov regularization.\n",
    "            lambda (None or float): Lambda parameter in Tikhonov regularization.\n",
    "            reg_mse_stop (bool): Whether or not to stop when MSE + Tikhonov regularization starts to rise again.\n",
    "    save_all (bool): Whether or not to save all important variables and models.\n",
    "    file_var (str): Path and name of the file to save variables into.\n",
    "    file_model (str): Path and name of the file to save the final model into.\n",
    "    verbose (int): If 0, then don't print anything throughout the training process. If 1, then print training and test accuracy after each epoch.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    mean_model (tensorflow.python.keras.engine.sequential.Sequential): The final model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    particles = setting_dict[\"particles\"]\n",
    "    epochs = setting_dict[\"epochs\"]\n",
    "    batch_size = setting_dict[\"batch_size\"]\n",
    "    h_0 = setting_dict[\"h_0\"]\n",
    "    delta = None\n",
    "    epsilon = setting_dict[\"epsilon\"]\n",
    "    randomization = setting_dict[\"randomization\"]\n",
    "    shuffle = setting_dict[\"shuffle\"]\n",
    "    early_stopping = setting_dict[\"early_stopping\"]\n",
    "    early_stopping_diff = setting_dict[\"early_stopping_diff\"]\n",
    "    \n",
    "    disjoint_batch = analysis_dict[\"disjoint_batch\"]\n",
    "    batch_evaluation = analysis_dict[\"batch_evaluation\"]\n",
    "    batch_particle_connection = analysis_dict[\"batch_particle_connection\"][\"connect\"]\n",
    "    batch_particle_shuffle = analysis_dict[\"batch_particle_connection\"][\"shuffle\"]\n",
    "    update_all = analysis_dict[\"batch_particle_connection\"][\"update_all\"]\n",
    "    mult_updates_batch = analysis_dict[\"multiple_updates_same_batch\"]\n",
    "    tik_regularize = analysis_dict[\"tikhonov\"][\"regularize\"]\n",
    "    tik_lambda = analysis_dict[\"tikhonov\"][\"lambda\"]\n",
    "    reg_mse_stop = analysis_dict[\"tikhonov\"][\"reg_mse_stop\"]\n",
    "    \n",
    "    if tik_lambda is None:\n",
    "        tik_lambda = 0\n",
    "    \n",
    "    if mult_updates_batch is None:\n",
    "        mult_updates_batch = 1\n",
    "\n",
    "    if batch_size == None:\n",
    "        batch_size = len(X_train)\n",
    "\n",
    "    n_cols = X_train.shape[1]\n",
    "    \n",
    "    if disjoint_batch:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        batch_indices = np.cumsum([0] + list(np.ones(num_batches) * batch_size))\n",
    "        batch_indices[-1] = n\n",
    "    else:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        last_batch_size = n % batch_size\n",
    "        \n",
    "    if batch_particle_connection:\n",
    "        batch_particle_dict = {}\n",
    "        batch_particle_indices = np.arange(particles) + 1\n",
    "        np.random.shuffle(batch_particle_indices)\n",
    "        if particles == num_batches:\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "        elif particles > num_batches:\n",
    "            base_batches = particles // num_batches\n",
    "            add_batches = particles % num_batches\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "            for i in range(add_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "        elif num_batches > particles:\n",
    "            num_reps = int(np.ceil(num_batches / particles))\n",
    "            particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "    else:\n",
    "        batch_particle_dict = None\n",
    "\n",
    "    model_dict = {}\n",
    "    weights_dict = {}\n",
    "    y_pred_dict = {}\n",
    "    jacobian_dict = {}\n",
    "    weights_vector_dict = {}\n",
    "    train_mse_dict = {}\n",
    "    test_mse_dict = {}\n",
    "    \n",
    "    if batch_evaluation[\"mean_model\"]:\n",
    "        train_batch_mse_mean_dict = {}\n",
    "    if batch_evaluation[\"particles\"]:\n",
    "        train_batch_mse_particle_dict = {}\n",
    "\n",
    "    # init_model already has weights and biases following the Glorot distribution\n",
    "    # it can already be used to predict and evaluate, but it is very bad\n",
    "    # only used to determine shapes and shape_elements via its weights\n",
    "    init_model = nn_model_structure(layers = layers,\n",
    "                                    neurons = neurons,\n",
    "                                    n_cols = n_cols,\n",
    "                                    classification = False)\n",
    "    init_model = nn_model_compile(init_model,\n",
    "                                  optimizer = \"sgd\")\n",
    "    weights = init_model.get_weights()\n",
    "    # shape contains the shapes of the weight matrices and bias vectors as a list of arrays\n",
    "    shapes = [np.array(params.shape) for params in weights]\n",
    "    # shape_elements contains the indices of the weights as a vector and tells where to cut\n",
    "    shape_elements = np.cumsum([0] + [np.prod(shape) for shape in shapes])\n",
    "\n",
    "    for i in range(particles):\n",
    "        \n",
    "        # just an initial model with the correct structure regarding neurons, layers, activation functions, Glorot initialization\n",
    "        model = nn_model_structure(layers = layers,\n",
    "                                   neurons = neurons,\n",
    "                                   n_cols = n_cols,\n",
    "                                   classification = False)\n",
    "        model = nn_model_compile(model,\n",
    "                                 optimizer = \"sgd\")\n",
    "        # for every particle write the model in a dictionary\n",
    "        model_dict[\"model_{}\".format(str(i+1))] = model\n",
    "\n",
    "        # for every particles write the weights and biases in a dictionary\n",
    "        weights_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .get_weights()\n",
    "        \n",
    "        # for every particle write the predictions on the training batches in a dictionary\n",
    "        y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .predict(X_train[:batch_size,:])\n",
    "\n",
    "        # for every particle write the Jacobian in a dictionary\n",
    "        jacobian_dict[\"model_{}\".format(str(i+1))] = 1/len(y_train[:batch_size]) * (-2)*(y_train[:batch_size] - y_pred_dict[\"model_{}\".format(str(i+1))].ravel())\n",
    "\n",
    "        train_mse_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        test_mse_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        \n",
    "        if batch_evaluation[\"particles\"]:\n",
    "            train_batch_mse_particle_dict[\"particle_{}\".format(str(i+1))] = {}\n",
    "\n",
    "    # mean_model as the model with the mean of the weights of all particle models\n",
    "    mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "    mean_model = init_model\n",
    "    mean_model.set_weights(mean_weights)\n",
    "\n",
    "    mean_model_train_mse = np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1])\n",
    "    mean_model_test_mse = np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1])\n",
    "    if tik_regularize:\n",
    "        mean_weights_raveled = [arr.ravel() for arr in mean_weights]\n",
    "        mean_model_train_mse_reg = mean_model_train_mse + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2)\n",
    "        mean_model_test_mse_reg = mean_model_test_mse + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2)\n",
    "\n",
    "    # loop over all epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # early stopping\n",
    "        if early_stopping:\n",
    "            if epoch == 0:\n",
    "                train_mse_old = 0\n",
    "                test_mse_old = 0\n",
    "            else:\n",
    "                train_mse_new = mean_model_train_mse[epoch]\n",
    "                test_mse_new = mean_model_test_mse[epoch]\n",
    "                if np.absolute(test_mse_new - test_mse_old) <= early_stopping_diff and np.absolute(train_mse_new - train_mse_old) <= early_stopping_diff:\n",
    "                    print(\"STOP: Early Stopping after epoch {} because improvement in training mse is only {} and in test mse only {}.\"\\\n",
    "                                                                         .format(epoch, train_mse_new - train_mse_old, test_mse_new - test_mse_old))\n",
    "                    break\n",
    "                test_mse_old = test_mse_new\n",
    "                                                                            \n",
    "        # Tikhonov regularization stopping\n",
    "        if tik_regularize and reg_mse_stop:\n",
    "            if epoch >= 1:\n",
    "                if mean_model_train_mse_reg[epoch] > mean_model_train_mse_reg[epoch-1] and mean_model_test_mse_reg[epoch] > mean_model_test_mse_reg[epoch-1]:\n",
    "                    print(\"Training and test MSEs containing Tikhonov regularization start to rise. Algorithm is stopped after epoch {}.\".format(epoch))\n",
    "                    break\n",
    "                \n",
    "        # shuffle the data\n",
    "        if shuffle:\n",
    "            indices = y_train.sample(frac=1).index\n",
    "        else:\n",
    "            indices = y_train.index\n",
    "            \n",
    "        if disjoint_batch:\n",
    "            X_batches = [np.array(X_train)[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [y_train.iloc[indices].reset_index(drop = True)[int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [np.array(i) for i in y_batches]\n",
    "        else:\n",
    "            if last_batch_size != 0:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches-1)]\n",
    "                indices.append(np.random.choice(len(X_train), size = last_batch_size, replace = True))\n",
    "            else:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches)]\n",
    "            X_batches = [X_train[indices[i]] for i in range(len(indices))]\n",
    "            y_batches = [y_train[indices[i]] for i in range(len(indices))]\n",
    "            \n",
    "        if batch_particle_connection and batch_particle_shuffle:\n",
    "            shuffled_indices = np.hstack(list(batch_particle_dict.values()))\n",
    "            np.random.shuffle(shuffled_indices)\n",
    "            batch_particle_values = list(batch_particle_dict.values())\n",
    "            for i in range(len(batch_particle_values)):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = shuffled_indices[i*len(batch_particle_values[i]):(i+1)*len(batch_particle_values[i])]\n",
    "            \n",
    "        if batch_evaluation[\"mean_model\"]:\n",
    "            train_batch_mse_mean_dict[\"Epoch_{}\".format(str(epoch+1))] = {}\n",
    "        if batch_evaluation[\"particles\"]:\n",
    "            train_batch_mse_particle_dict[\"particle_{}\".format(str(i+1))][\"Epoch_{}\".format(str(epoch+1))] = {}\n",
    "                    \n",
    "        # loop over all batches\n",
    "        for b in range(num_batches):\n",
    "            for mult_updates in range(mult_updates_batch):\n",
    "                batch_particles = []\n",
    "                y_pred_batch_dict = {}\n",
    "                jacobian_batch_dict = {}\n",
    "                for i in range(particles):\n",
    "                    if batch_particle_connection: \n",
    "                        if num_batches == particles or num_batches > particles:\n",
    "                            if batch_particle_dict[\"batch_{}\".format(str(b+1))] != i+1:\n",
    "                                continue\n",
    "                        else:\n",
    "                            if i+1 not in batch_particle_dict[\"batch_{}\".format(str(b+1))]:\n",
    "                                continue\n",
    "                    if batch_particle_connection:\n",
    "                        batch_particles.append(i+1)\n",
    "                    \n",
    "                    # set new weights for model\n",
    "                    model_dict[\"model_{}\".format(str(i+1))].set_weights(weights_dict[\"model_{}\".format(str(i+1))])\n",
    "\n",
    "                    # for every particle write the predictions on the training batches in a dictionary\n",
    "                    y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                    .predict(X_batches[b])\n",
    "                    y_pred_batch_dict[\"model_{}\".format(i+1)] = y_pred_dict[\"model_{}\".format(i+1)]\n",
    "\n",
    "                    # for every particle write the Jacobian in a dictionary\n",
    "                    jacobian_dict[\"model_{}\".format(str(i+1))] = 1/len(y_batches[b]) * (-2)*(y_batches[b] - y_pred_dict[\"model_{}\".format(str(i+1))].ravel())\n",
    "                    jacobian_batch_dict[\"model_{}\".format(i+1)] = jacobian_dict[\"model_{}\".format(i+1)]\n",
    "\n",
    "                    if batch_evaluation[\"mean_model\"]:\n",
    "                        train_batch_mse_mean_dict[\"Epoch_{}\".format(str(epoch+1))][\"Batch_{}\".format(str(b+1))] = mean_model.evaluate(X_batches[b], y_batches[b], verbose = 0)[1]\n",
    "                    if batch_evaluation[\"particles\"]:\n",
    "                        train_batch_mse_particle_dict[\"particle_{}\".format(str(i+1))][\"Epoch_{}\".format(str(epoch+1))][\"Batch_{}\".format(str(b+1))] = model_dict[\"model_{}\".format(str(i+1))].evaluate(X_batches[b], y_batches[b], verbose = 0)[1]\n",
    "\n",
    "                        \n",
    "                if not batch_particle_connection:        \n",
    "                    # compute the mean of the predictions\n",
    "                    y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "                    # compute the matrix D elementwise\n",
    "                    d = np.zeros(shape = (particles, particles))\n",
    "                    for k in range(particles):\n",
    "                        y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                        for j in range(particles):\n",
    "                            d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_dict[\"model_{}\".format(str(j+1))])\n",
    "                    d = np.transpose(d)\n",
    "\n",
    "                    # compute the scalar h_t\n",
    "                    h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "                    # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "                    for i in range(particles):\n",
    "                        weights_array = np.array([])\n",
    "                        for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                            weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "                    # matrix with particle parameters as row vectors\n",
    "                    weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "                    # compute the matrix with the updates for each particle\n",
    "                    weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "                    for i in range(particles):\n",
    "                        # write the updates back into the dictionary\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                        # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                        for l in range(len(shape_elements)-1):\n",
    "                            start = shape_elements[l]\n",
    "                            end = shape_elements[l+1]\n",
    "                            weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                \n",
    "                elif batch_particle_connection and not update_all:\n",
    "                    # compute the mean of the predictions\n",
    "                    y_pred_mean = np.mean(list(y_pred_batch_dict.values()), axis = 0)\n",
    "                    \n",
    "                    # compute the matrix D elementwise\n",
    "                    d = np.zeros(shape = (len(batch_particles), len(batch_particles)))\n",
    "                    for k in range(len(batch_particles)):\n",
    "                        y_pred_centered = y_pred_batch_dict[\"model_{}\".format(batch_particles[k])] - y_pred_mean\n",
    "                        for j in range(len(batch_particles)):\n",
    "                            d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_batch_dict[\"model_{}\".format(batch_particles[j])])\n",
    "                    d = np.transpose(d)\n",
    "\n",
    "                    # compute the scalar h_t\n",
    "                    h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "                    # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "                    for i in range(particles):\n",
    "                        weights_array = np.array([])\n",
    "                        for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                            weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "                    # matrix with particle parameters as row vectors\n",
    "                    weights_vector_batch_dict = {}\n",
    "                    for i in range(len(batch_particles)):\n",
    "                        weights_vector_batch_dict[\"model_{}\".format(batch_particles[i])] = weights_vector_dict[\"model_{}\".format(batch_particles[i])]\n",
    "                        weights_all_ptcls = np.array(list(weights_vector_batch_dict.values()))\n",
    "\n",
    "                    # compute the matrix with the updates for each particle\n",
    "                    weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "                    for i in range(len(batch_particles)):\n",
    "                        # write the updates back into the dictionary\n",
    "                        weights_vector_dict[\"model_{}\".format(batch_particles[i])] = weights_all_ptcls[i]\n",
    "                        # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                        for l in range(len(shape_elements)-1):\n",
    "                            start = shape_elements[l]\n",
    "                            end = shape_elements[l+1]\n",
    "                            weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "\n",
    "        if batch_particle_connection and update_all:\n",
    "            # compute the mean of the predictions\n",
    "            y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "            # compute the matrix D elementwise\n",
    "            d = np.zeros(shape = (particles, particles))\n",
    "            for k in range(particles):\n",
    "                y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                for j in range(particles):\n",
    "                    d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_dict[\"model_{}\".format(str(j+1))])\n",
    "            d = np.transpose(d)\n",
    "\n",
    "            # compute the scalar h_t\n",
    "            h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "            # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "            for i in range(particles):\n",
    "                weights_array = np.array([])\n",
    "                for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                    weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "            # matrix with particle parameters as row vectors\n",
    "            weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "            # compute the matrix with the updates for each particle\n",
    "            weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "            for i in range(particles):\n",
    "                # write the updates back into the dictionary\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                for l in range(len(shape_elements)-1):\n",
    "                    start = shape_elements[l]\n",
    "                    end = shape_elements[l+1]\n",
    "                    weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                                                            \n",
    "        for i in range(particles):\n",
    "            # for every particle write the training mse of the current iteration in a dictionary\n",
    "            train_mse_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_train, y_train, verbose = 0)[1])\n",
    "\n",
    "            # for every particle write the test mse of the current iteration in a dictionary\n",
    "            test_mse_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_test, y_test, verbose = 0)[1])\n",
    "\n",
    "        # update the mean_model\n",
    "        mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "        mean_model.set_weights(mean_weights)\n",
    "\n",
    "        mean_model_train_mse = np.append(mean_model_train_mse, np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1]))\n",
    "        mean_model_test_mse = np.append(mean_model_test_mse, np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1]))\n",
    "        if tik_regularize and reg_mse_stop:\n",
    "            mean_weights_raveled = [arr.ravel() for arr in mean_weights]\n",
    "            mean_model_train_mse_reg = np.append(mean_model_train_mse_reg, mean_model_train_mse[-1] + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2))\n",
    "            mean_model_test_mse_reg = np.append(mean_model_test_mse_reg, mean_model_test_mse[-1] + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2))\n",
    "        \n",
    "\n",
    "        if verbose == 1:\n",
    "            print(\"Epoch {}. Training MSE: {}, Test MSE: {}.\".format(epoch+1,\n",
    "                                                                     np.round(mean_model_train_mse[-1], 3),\n",
    "                                                                     np.round(mean_model_test_mse[-1], 3)))\n",
    "            \n",
    "    mean_model.history.history = {\"mse\": mean_model_train_mse[1:],\n",
    "                                  \"val_mse\": mean_model_test_mse[1:]}\n",
    "        \n",
    "    if save_all:\n",
    "        param_dict = param_to_dict(X_train,\n",
    "                                   X_test,\n",
    "                                   y_train,\n",
    "                                   y_test,\n",
    "                                   layers,\n",
    "                                   neurons,\n",
    "                                   particles,\n",
    "                                   epochs,\n",
    "                                   batch_size,\n",
    "                                   h_0,\n",
    "                                   delta,\n",
    "                                   epsilon,\n",
    "                                   randomization,\n",
    "                                   shuffle,\n",
    "                                   early_stopping,\n",
    "                                   early_stopping_diff\n",
    "                                   )\n",
    "        results_dict = results_to_dict(mean_model_train_mse,\n",
    "                                       mean_model_test_mse,\n",
    "                                       train_mse_dict,\n",
    "                                       test_mse_dict,\n",
    "                                       weights_dict,\n",
    "                                       y_pred_dict,\n",
    "                                       False\n",
    "                                       )\n",
    "\n",
    "        saving_dict = {}\n",
    "        saving_dict[\"parameters\"] = param_dict\n",
    "        saving_dict[\"results\"] = results_dict\n",
    "        saving_dict[\"analysis\"] = analysis_dict\n",
    "\n",
    "        save_objects(obj_dict = saving_dict,\n",
    "                     file = file_var)\n",
    "\n",
    "        nn_save(model = mean_model,\n",
    "                path_name = file_model)\n",
    "        \n",
    "        if batch_evaluation[\"mean_model\"]:\n",
    "            full_dict = {}\n",
    "            full_dict[\"mean_model\"] = train_batch_mse_mean_dict\n",
    "            if batch_evaluation[\"particles\"]:\n",
    "                full_dict[\"particles\"] = train_batch_mse_particle_dict\n",
    "            save_objects(obj_dict = full_dict,\n",
    "                         file = batch_evaluation[\"file\"])\n",
    "        elif batch_evaluation[\"particles\"]:\n",
    "            full_dict = {}\n",
    "            full_dict[\"particles\"] = train_batch_mse_particle_dict\n",
    "            if batch_evaluation[\"mean_model\"]:\n",
    "                full_dict[\"mean_model\"] = train_batch_mse_mean_dict\n",
    "            save_objects(obj_dict = full_dict,\n",
    "                         file = batch_evaluation[\"file\"])\n",
    " \n",
    "    return mean_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting_dict = {\"particles\": 20,\n",
    "                \"epochs\": 10,\n",
    "                \"batch_size\": 100,    # len(X_train)\n",
    "                \"h_0\": 2,\n",
    "                \"epsilon\": 0.5,\n",
    "                \"randomization\": False,\n",
    "                \"shuffle\": True,\n",
    "                \"early_stopping\": False,\n",
    "                \"early_stopping_diff\": 0.001\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dict = {\"disjoint_batch\": True,\n",
    "                 \"multiple_updates_same_batch\": None,\n",
    "                 \"batch_particle_connection\": {\"connect\": True,\n",
    "                                               \"shuffle\": False,\n",
    "                                               \"update_all\": True},\n",
    "                 \"batch_evaluation\": {\"mean_model\": False,\n",
    "                                      \"particles\": False,\n",
    "                                      \"file\": \"../objects/wine/batch_mse.pckl\"},\n",
    "                 \"tikhonov\": {\"regularize\": False,\n",
    "                              \"lambda\": None,\n",
    "                              \"reg_mse_stop\": False}\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training MSE: 11.312, Test MSE: 11.288.\n",
      "Epoch 2. Training MSE: 4.444, Test MSE: 4.436.\n",
      "Epoch 3. Training MSE: 2.257, Test MSE: 2.275.\n",
      "Epoch 4. Training MSE: 1.923, Test MSE: 1.976.\n",
      "Epoch 5. Training MSE: 1.368, Test MSE: 1.42.\n",
      "Epoch 6. Training MSE: 0.947, Test MSE: 1.05.\n",
      "Epoch 7. Training MSE: 0.879, Test MSE: 0.992.\n",
      "Epoch 8. Training MSE: 0.795, Test MSE: 0.916.\n",
      "Epoch 9. Training MSE: 0.763, Test MSE: 0.881.\n",
      "Epoch 10. Training MSE: 0.74, Test MSE: 0.853.\n"
     ]
    }
   ],
   "source": [
    "mean_model = enkf_regressor_analysis(X_train,\n",
    "                                     X_test,\n",
    "                                     y_train,\n",
    "                                     y_test,\n",
    "                                     layers,\n",
    "                                     neurons,\n",
    "                                     setting_dict,\n",
    "                                     analysis_dict,\n",
    "                                     save_all = False,\n",
    "                                     file_var = \"../objects/wine/wine_enkf_E{}_B{}_P{}_H{}.pckl\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                     file_model = \"../models/wine/wine_enkf_E{}_B{}_P{}_H{}.h5\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                     verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation time: 0.39174928665161135 minutes.\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "print(\"Calculation time: {} minutes.\".format((end_time - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe4AAAE9CAYAAADNvYHXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyrklEQVR4nO3deXyU5b338c9vJivZgASSQICwg0AAiaiANqhYXGtbe6zH9rHLeVza01bbc1ptj61dzjl2e86pbdXqca2t2lo97iuaUpeKIIhsAcUIgQTCEkggCZnJ9fwxQwiQZUhmcmcy3/frNa+555r7vu5fMPF775c55xAREZH44PO6ABEREYmcgltERCSOKLhFRETiiIJbREQkjii4RURE4oiCW0REJI4keV1AJPLy8lxxcXHU+jtw4AAZGRlR60/Ea/qdFvFWtP8GV6xYscs5N6yj7+IiuIuLi1m+fHnU+isvL6esrCxq/Yl4Tb/TIt6K9t+gmX3U2Xc6VC4iIhJHFNwiIiJxRMEtIiISR+LiHLeIiMSXlpYWqqqqaGpq8rqUPpGTk8P69etPeLm0tDSKiopITk6OeBkFt4iIRF1VVRVZWVkUFxdjZl6XE3P19fVkZWWd0DLOOXbv3k1VVRVjx46NeDkdKhcRkahramoiNzc3IUK7p8yM3NzcEz4qoeAWEZGYUGh3ryf/RglzqLz0Jy+xq+HQkYbnnwEgLzOF5f+2yKOqREQkFnbv3s3ZZ58NQE1NDX6/n2HDQs8zWbZsGSkpKZ0uu3z5ch544AFuvfXWLtcxb9483njjjegVHaGECe7nWr7MsLR9x7XXtuQAW/q+IBERiZnc3FxWrVoFwM0330xmZib/8i//0vZ9IBAgKanjCCwtLaW0tLTbdXgR2pBAwT3Mjg/trtpFRKRvHHdENCzaR0S/8IUvMHToUFauXMnJJ5/MZZddxnXXXUdjYyPp6ence++9TJ48mfLycn7xi1/w9NNPc/PNN7NlyxY2b97Mli1buO666/j6178OQGZmJg0NDZSXl3PTTTeRn5/PmjVrmDNnDg8++CBmxrPPPss3v/lN8vLyOPnkk9m8eTNPP/10r36OhAluERHpnzoK7a7ae2Pjxo28/PLL+P1+9u/fz9KlS0lKSuLll1/mu9/9Ln/5y1+OW2bDhg28+uqr1NfXM3nyZK699trjbt9avXo1a9euZcSIEcyfP5/XX3+d0tJSrr76apYuXcrYsWO5/PLLo/IzKLhFRCSmfvjUWtZt39+jZS/73Zsdtp80IpsfXDTthPv7zGc+g9/vB2Dfvn1ceeWVbNq0CTOjpaWlw2UuuOACUlNTSU1NZfjw4ezYsYOioqKj5pkzZ05b26xZs6isrCQzM5Nx48a13ep1+eWXc+edd55wzcfSVeUiIpIw2o/gddNNN7Fw4ULWrFnDU0891eltWampqW3Tfr+fQCBw3DztL3Y7PI9zLoqVH6E9bhERianu9oyLb3im0+8eufr0aJfTZt++fYwcORKA++67L+r9T5kyhc2bN1NZWUlxcTGPPPJIVPpNnD3ujOEn1i4iIgPat7/9bW688Ubmz59PMBiMev/p6encdtttLF68mAULFpCfn09OTk6v+7VY7cpHU2lpqYvmeNyvPXIrC9bfxHPT/4vzLv1S1PoV8YrG45b+Zv369UydOjWiefvqqvJY6uyRpw0NDWRmZuKc46tf/SoTJ07k+uuvP2qejv6tzGyFc67De9IS8lB5cNhUDq5Px795CaDgFhHxUryEc0/cdddd3H///Rw6dIjZs2dz9dVX97rPhAxu50umasgpTN3zFg1NLWSmRT4qi4iISKSuv/764/aweytxznEfwz9pEaOsllWr3va6FBERkYglbHCPKr0IgLrVz3lciYiISOQSNrhTho2lOnkUeTV/i9m9diIiItGWsMENUDfiTGYF11BZs8vrUkRERCKS0MGdO/N80qyFjcte9LoUERGJot27dzNr1ixmzZpFQUEBI0eObPt86FD3z0AvLy8/avSvO+64gwceeCCWJUcsIa8qP2z4jLNpfjIF3n8ZuMLrckREJEq6G9azO+Xl5WRmZjJv3jwArrnmmliU2SMJHdwkp7Ml+2Qm7Ps7TS1B0pL9XlckIpJ4fj4RDuw8vj1jOPzrpqitZsWKFXzzm9+koaGBvLw87rvvPgoLC7n11lu54447SEpK4qSTTuKWW27hjjvuwO/38+CDD/LrX/+aJUuWtIV/WVkZp556Kq+++ip1dXXcfffdzJo1i4MHD/KFL3yBDRs2MHXqVCorK/ntb38b0djeJyKxgxtgwtmMf+ffeeu91Zx68myvqxERSTwdhXZX7T3gnONrX/saTzzxBMOGDeORRx7he9/7Hvfccw+33HILH374IampqdTV1TF48GCuueaao/bSlyxZclR/gUCAZcuW8eyzz/LDH/6Qxx9/nNtuu40hQ4awevVq1qxZw6xZs6JWf3sJH9yjTrkY3vl3dr37LCi4RUSi77kboOa9ni177wUdtxfMgPNuibib5uZm1qxZw6JFoae0BYNBCgsLASgpKeGKK67gkksu4ZJLLomov0996lNAaDjPyspKAF577TW+8Y1vADB9+nRKSkoiru9EJHxwpxVMptafT862pcD3vC5HRERiwDnHtGnTePPN48f3fuaZZ1i6dClPPvkkP/7xj1m7dm23/R0e6rP9MJ99dWtxwgc3ZuwqOIOZVc9QtauOorzBXlckIjKwdLdnfHMXI2Z9sfMhP09EamoqtbW1vPnmm5x++um0tLSwceNGpk6dytatW1m4cCELFizgj3/8Iw0NDWRlZbF///4TWseCBQv405/+xMKFC1m3bh3vvdfDowzdSOjbwQ7LmXEeWdbIhmVLup9ZRETijs/n49FHH+U73/kOM2fOZNasWbzxxhsEg0E+97nPMWPGDGbPns3111/P4MGDueiii3j88ceZNWsWf/vb3yJax1e+8hVqa2spKSnhpz/9KSUlJVEZxvNY2uMGCmedS+B5Py0bX4LzP+11OSIiiSVjeOdXlUfBzTff3Da9dOnS475/7bXXjmubNGkSq1evbvt8xhlntE2Xl5e3Tefl5VFZWUl9fT1paWk8+OCDpKWl8cEHH3D22WczZsyYqPwM7Sm4AUvL5qOMEsbufYOWYCvJfh2IEBHpM1G85ctLBw8eZOHChbS0tOCc4/bbbyclJSXq61FwhwXGnsWUNb9k5YYKZk+LbPB3ERGRw7Kysli+fHnM16Ndy7CRp4RGC6t+JzoXQoiIiMSCgjssc/Qs9vqGkLm13OtSREQGBI282L2e/BspuA8zo2bYAkqa36F230GvqxERiWtpaWns3r1b4d0F5xy7d+8mLS3thJbTOe52MqZ9nME7nuLVt19l4TmdPK1HRES6VVRURFVVFbW1tV6X0ieamppOOIAhtIFTVFR0QsvELLjN7B7gQmCnc256uG0o8AhQDFQC/+Cc2xurGk5U0ZwLCL7ydZrWvwAKbhGRHktOTmbs2LFel9FnysvLmT27bx6bHctD5fcBi49puwFY4pybCCwJf+43fBlD2ZI+lZG73yDYqsM7IiLS/8QsuJ1zS4E9xzR/Arg/PH0/cEms1t9TTWMWMt29z/oPKr0uRURE5Dh9fXFavnOuGiD8Hp3H4kTRiDkX4jPH1uW6LUxERPqffntxmpldBVwFkJ+ff9Qj5nqroaGh8/5ckNlk4dv0AuXl06O2TpFY6vJ3WkRiri//Bvs6uHeYWaFzrtrMCoFOR0l3zt0J3AlQWlrqysrKolZEeXk5XfW3fu3pnFz7FimnnE5ORmrU1isSK939TotIbPXl32BfHyp/ErgyPH0l8EQfrz8iaVPOZZjt4713jn/wvIiIiJdiFtxm9hDwJjDZzKrM7MvALcAiM9sELAp/7ndGnXIhAPVrnve4EhERkaPF7FC5c+7yTr46O1brjJaknEK2pE4gf+drOOcwM69LEhERAfTI0041FJUxo7WC97ds97oUERGRNgruTuSffAHJFuTD5c96XYqIiEgbBXcncqecwQHS8W9+xetSRERE2ii4O+NPpmrIXKY0LONAU4vX1YiIiAAK7i75Ji5ipO1i9bvLvC5FREQEUHB3adTciwGoW/2cx5WIiIiEKLi7kJY3hm3JY8it+ZvXpYiIiAAK7m7VjTiTmYG1fFSdGIPBi4hI/6bg7kbuzPNJtRY2vf2C16WIiIgouLtTMOMsmkiBTS95XYqIiIiCu1vJaWzJnsOE/W/RHAh6XY2IiCQ4BXcE3PizKbZq1ry3yutSREQkwSm4I3D4trDaVXr8qYiIeEvBHYFBBZPY4S8kZ9tSr0sREZEEp+COhBm7ChZQ0vIu1bvrvK5GREQSmII7QjkzziPDmtmwTFeXi4iIdxTcERo5+1xaSCKwUcEtIiLeUXBHyFKzqMwoYczeNwkEW70uR0REEpSC+wQExp7NJLawrmKD16WIiEiCUnCfgKJTQreFVa94xuNKREQkUSm4T0DW6Bns9uWSsbXc61JERCRBKbhPhBk1wxYwo/kddu8/4HU1IiKSgBTcJyhj2sfJsYOse/sVr0sREZEEpOA+QaNLzyeAj8Z1L3pdioiIJCAF9wnyDRrClvSTGLn7dVpbndfliIhIglFw90DjmIVM4wMqNm/2uhQREUkwCu4eGDHnIgCqluu2MBER6VsK7h4YMv4U9lk2qZWvel2KiIgkGAV3T/h8VOXOY1rjcvY3NntdjYiIJBAFdw+lTjmXXNvPmuV/87oUERFJIAruHhoz90IA6tc873ElIiKSSBTcPZScnc9HqZMo2Pkazum2MBER6RsK7l6oLypjWmsFH1Zt87oUERFJEAruXhh+8gUkWSublz3rdSkiIpIgFNy9MHzKAhrIIGnzEq9LERGRBKHg7g1/EluGzGVKwzIamwNeVyMiIglAwd1LSZMWUWB7eG/V370uRUREEoCCu5dGzw09/rRu9XMeVyIiIonAk+A2s+vNbK2ZrTGzh8wszYs6oiEtdzRbk4vJrVnqdSkiIpIAugxuM/OZ2bxortDMRgJfB0qdc9MBP/DZaK6jr9WNOJPpgXVU1dR6XYqIiAxwXQa3c64V+GUM1psEpJtZEjAI2B6DdfSZ3Jnnk2oBNi3T4XIREYmtSA6Vv2hmnzYzi8YKnXPbgF8AW4BqYJ9z7sVo9O2VwhkLaSQV9/7LXpciIiIDnHX3uE4zqwcygCDQCBjgnHPZPVqh2RDgL8BlQB3wZ+BR59yDx8x3FXAVQH5+/pyHH364J6vrUENDA5mZmVHrD2DwGz9kSPM2Nn3sdyT5orKNIxKxWPxOi0jkov03uHDhwhXOudKOvkvqbmHnXFbUKgk5B/jQOVcLYGaPAfOAo4LbOXcncCdAaWmpKysri1oB5eXlRLM/gPX7VjBm5Y/YMySb2bPnRLVvke7E4ndaRCLXl3+DEV1VbmYXm9kvwq8Le7nOLcBpZjYofPj9bGB9L/v03OhTLwagdpUefyoiIrHTbXCb2S3AN4B14dc3wm094px7C3gUeAd4L1zDnT3tr7/IKJhItX8kOdv/6nUpIiIygHV7qBw4H5gVvsIcM7sfWAnc0NOVOud+APygp8v3V7UFZ1BS9Rg79tSRP3Sw1+WIiMgAFOkDWAa3m86JQR0DQs6MxaTbITa89YLXpYiIyAAVSXD/B7DSzO4L722vCLfJMUbPXkQzyQQ2vuR1KSIiMkB1eajczHxAK3AacAqhW8G+45yr6YPa4o6lZlKZMZPivW8SbHX4dVuYiIhEWSRPTvtn51y1c+5J59wTCu2utYw9m/FUsX7DWq9LERGRASiSQ+Uvmdm/mNkoMxt6+BXzyuLUqFNCo4XVvPOMx5WIiMhAFMlV5V8Kv3+1XZsDxkW/nPiXM3o6tb5hZGwtB77jcTUiIjLQRHKO+wbn3CN9VE/8M6Nm2Hym17zA3v0HGJKd4XVFIiIygERyjvurXc0jx8uYtpgsa2Td20u8LkVERAYYneOOgTGl5xHAR+P6uB70TERE+iGd444B/6DBvJ8+nZG7Xsc5R5RGRBUREel+j9s5N7aDl0K7G42jy5jKZjZ+sNnrUkREZADpNLjN7Nvtpj9zzHd6clo3RpwSGi2savnTHlciIiIDSVd73J9tN33jMd8tjkEtA0ruuDnsscGkffSK16WIiMgA0lVwWyfTHX2WY/l8bMudx9SDy2lobPa6GhERGSC6Cm7XyXRHn6UDqVMWMdQaWLtcY3SLiEh0dBXcM81sv5nVAyXh6cOfZ/RRfXGteO5FtDqjYe3zXpciIiIDRKe3gznn/H1ZyECUkj2MzWmTGb7zNd0WJiIiURHJA1ikF+qLPsZJwY18VFXldSkiIjIAKLhjLH/2hfjN8eEyjRYmIiK9p+COsYKp89hPJv7Nui1MRER6T8Eda/4ktgw+lSkNy2g6FPC6GhERiXNdPTmtvt2V5Me9+rLIeOeffA7DbS9rV77pdSkiIhLnOg1u51yWcy4b+G/gBmAkUAR8B/hJn1Q3QBTPDT3+dO/q5zyuRERE4l0kh8o/7py7zTlX75zb75y7Hfh0rAsbSNJzi9iSPI7cmqVelyIiInEukuAOmtkVZuY3M5+ZXQEEY13YQLN3xJlMC6xj+85ar0sREZE4Fklw/yPwD8CO8Osz4TY5AbkzzyfFgrz/lm4LExGRnotkPO5K59wnnHN5zrlhzrlLnHOVfVDbgDKypIwDpOE2vex1KSIiEse6DW4zm2RmS8xsTfhziZn9W+xLG1gsKZXK7FLG7/s7LQGdaRARkZ6J5FD5XYTG424BcM6t5uixuiVS48+hyGpZv+YdrysREZE4FUlwD3LOLTumTU8S6YHRp14EQO0qnecWEZGeiSS4d5nZeMJjcJvZpUB1TKsaoLIKJrDNX0TONt0WJiIiPRNJcH8V+B0wxcy2AdcB18SyqIFsV8EZTD+0mto9dV6XIiIicajL4DYzP3Ctc+4cYBgwxTm3wDn3UZ9UNwDlzDiPNGuhYtnzXpciIiJxqMvgds4FgTnh6QPOufo+qWoAGz17EU2kEKh4yetSREQkDiVFMM9KM3sS+DNw4HCjc+6xmFU1gPlSB/FhxizG7H2T1laHz2delyQiInEkknPcQ4HdwFnAReHXhbEsaqALjD2LsWyjomKN16WIiEic6XaP2zn3xb4oJJEUnXIRrLmFmhVPM3XqDK/LERGRONJtcJtZGvBlYBqQdrjdOfelGNY1oA0ZPY0dvuFkbP0roWfbiIiIRCaSQ+W/BwqAjwN/JTQmd68uUjOzwWb2qJltMLP1ZnZ6b/qLO2ZUD1vASU0r2Vd/oPv5RUREwiIJ7gnOuZuAA865+4ELgN4e3/0V8LxzbgowE1jfy/7iTua0j5NpTax/W1eXi4hI5CIJ7pbwe52ZTQdygOKertDMsoEzgbsBnHOHnHN1Pe0vXhWXnkcLfhrXveh1KSIiEkciCe47zWwIcBPwJLAO+Fkv1jkOqAXuNbOVZvY/ZpbRi/7iUtKgHD5Mn8HI3a/jnPO6HBERiRPW16FhZqXA34H5zrm3zOxXwP7w4fj2810FXAWQn58/5+GHH45aDQ0NDWRmZkatv54KvPcnztn9B/4y425yc/O8LkfiWH/5nRZJVNH+G1y4cOEK51xpR99FclX59ztqd879qIf1VAFVzrm3wp8fBW7ooP87gTsBSktLXVlZWQ9Xd7zy8nKi2V9P1RZlwoN/IDewlbKyS70uR+JYf/mdFklUffk3GMmh8gPtXkHgPHpxjts5VwNsNbPJ4aazCR1+TzjDxs9htw0htfJVr0sREZE4EckDWH7Z/rOZ/YLQue7e+BrwBzNLATYDifmQFzOqcucxtbacg03NDEpL9boiERHp5yLZ4z7WIEIXmPWYc26Vc67UOVfinLvEObe3N/3Fs9TJ5zLYDrBuebnXpYiISBzoNrjN7D0zWx1+rQUqCN2HLVFQPPcCgs6oX6NhPkVEpHuRjA7WfkCRALDDOReIUT0JJy1nGO+nTSF/59+8LkVEROJAJIfK69u9GoFsMxt6+BXT6hJEQ1EZU4Lvs2XrFq9LERGRfi6S4H6H0ANTNgKbwtMrwq/lsSstcQyffQE+c1S+/YzXpYiISD8XSXA/D1zknMtzzuUSOnT+mHNurHOuVxepSciIk+ZRRxb+D5Z4XYqIiPRzkQT3Kc65Zw9/cM49B3wsdiUlIJ+fLUNOY1LDMppbWrqfX0REElYkwb3LzP7NzIrNbIyZfQ/YHevCEo1/4jkMs32sX/mG16WIiEg/FklwXw4MAx4H/hcYHm6TKBp76kUA7F39nMeViIhIfxbJk9P2AN8ACI8SVuc0nFXUDcodyYfJE8itXup1KSIi0o91usdtZt83synh6VQzewV4H9hhZuf0VYGJpK7wDKYGNrBj506vSxERkX6qq0PllxF6ShrAleF5hxO6MO0/YlxXQho663ySLcj7b+m2MBER6VhXwX2o3SHxjwMPOeeCzrn1RPbENTlBo0vKOEA6btPLXpciIiL9VFfB3Wxm081sGLAQeLHdd4NiW1ZisqQUNmeXMm7/3wkEgl6XIyIi/VBXwf0N4FFgA/BfzrkPAczsfGBlH9SWkNz4cxjBLirW6qF0IiJyvE6D2zn3lnNuinMu1zn343btzzrndDtYjBTPvRiA2pU6zy0iIsfryXjcEkPZhePY6h9FzjbdFiYiIsdTcPdDuwrO4KRD77F7zx6vSxERkX5Gwd0PZU8/j1QLsHHZ816XIiIi/UxEt3WZ2TyguP38zrkHYlRTwiues4jGF1IIVLwEi//R63JERKQf6Ta4zez3wHhgFXD4HiUHKLhjxJ+SzuaMkxm9901aWx0+n3ldkoiI9BOR7HGXAifp+eR9q2XsWYxZ8x9srFjNpKkzvS5HRET6iUjOca8BCmJdiBxt1NzQaGHVK3RbmIiIHBHJHncesM7MlgHNhxudcxfHrCohd9RUqn0FZGwtB77rcTUiItJfRBLcN8e6COmAGdXD5jO15mn2NzSQnZnpdUUiItIPRDIe91/7ohA5XsZJi8nY8ReWLXuJuWd90utyRESkH+j2HLeZnWZmb5tZg5kdMrOgme3vi+IS3bi5i2lxfhrXv9j9zCIikhAiuTjtN8DlwCYgHfincJvEWHJ6Nh8MKqFo12voon4REYEIn5zmnHsf8IfH474XKItpVdKmcfRCxrstVH640etSRESkH4jk4rSDZpYCrDKznwHVQEZsyxKAXTePZjb7ABj7wNwj7eSQd/MWr8oSEREPRbLH/fnwfP8MHABGAZ+OZVESkhcO7UjbRURk4IvkqvKPzCwdKHTO/bAPahIREZFORHJV+UWEnlP+fPjzLDN7MsZ1iYiISAciOVR+MzAXqANwzq0iNFKYiIiI9LFIgjvgnNNJ1X5Gt4eJiCSmiAYZMbN/BPxmNtHMfg28EeO6hNDV45156ff/2YeViIhIfxHJ7WBfA75HaICRh4AXgB/HsigJ6eiWLxc4RMWvLubsD35G+ZMjKLv4C31fmIiIeKbbPW7n3EHn3Pecc6c450rD0019UZwcz5JSmPCVP/NR6iROXfGvvL30ea9LEhGRPtTpHnd3V473dlhPM/MDy4FtzrkLe9NXoklKz6Lg2ifY++uzmLjky6wf8gRTZ5R6XZaIiPSBrg6Vnw5sJXR4/C3AorzubwDrgewo95sQBg0ppPGLj9N697nk/OWzbM1+kVFjxnldloiIxFhXh8oLgO8C04FfAYuAXc65v/Z2qE8zKwIuAP6nN/0kutxRUzj4mYcYzH6a7/8ke3bv8rokERGJsU6DOzygyPPOuSuB04D3gXIz+1oU1vvfwLeB1ij0ldCKps1n26LfMSa4la13fJrGxkavSxIRkRjq8qpyM0sltGd8OaGHrtwKPNabFZrZhcBO59wKMyvrYr6rgKsA8vPzKS8v781qj9LQ0BDV/rw3hPdHXMt51b/hr7/8NMF538Ln83tdlPShgfc7LRJf+vJv0Dp7kIeZ3U/oMPlzwMPOuTVRWaHZfxIauCQApBE6x/2Yc+5znS1TWlrqli9fHo3VA1BeXk5ZWVnU+usvVjx4E3Pev5XXhl3O/K/cjlm0L0uQ/mqg/k6LxIto/w2a2QrnXIdXHXd1jvvzwCRCF5G9YWb7w696M9vf02Kcczc654qcc8XAZ4FXugptidycK37EiuGXsqD2Id74g261FxEZiLo6x+1zzmWFX9ntXlnOOV0J3h+ZMfuq37Eq80xO3/T/ePsZXfsnIjLQRPLI05hxzpXrHu7o8iUlMfWrD1GRchIzl32HNa8/5XVJIiISRZ4Gt8RGanomI6/5X6r9hYx56f/y4dplXpckIiJRouAeoLJzh5P6hcc5SDqZf76MnVvf97okERGJAgX3AFYweiL1lz5Mmmuk8d5L2L93p9cliYhILym4B7gJ00/lw0V3URisZvvtl3Co8YDXJYmISC8ouBNAyYKLeKf0p0w5tJYNt12GCwa8LklERHpIwZ0gTrvon1g67puU1P+NVXdeBZ08eEdERPo3BXcCOePz32dp3meZveMvrHro+16XIyIiPaDgTiBmxrxrfsubg85i1sZbWfvs7V6XJCIiJ0jBnWCSkpIo+ec/sCp5JpPe+h4fvPm/XpckIiInQMGdgDIGDaLo6seo9I2i8IWrqV73htcliYhIhBTcCSovL4/kKx+jjixS//RZ9lZt8LokERGJgII7gRUXj2fPpx7CXJDGez9J494ar0sSEZFuKLgT3PSSU6g46y6GBmqpueNigk0NXpckIiJdUHALp33sfF6f9XNGN23kg9suxQUOeV2SiIh0QsEtAJz9yS/yQvG3mbT/TSru/ic9oEVEpJ9ScEubxVfeyLND/w9Tqp+g4uEbvS5HREQ6oOCWNj6fcdY1/8WS9I8zueJ2PnjuVq9LEhGRYyi45ShpKUmUfvV+3koqpfit77PtzUe9LklERNpRcMtxcjLTKbrqEdbbBPJeuJbd6//mdUkiIhKm4JYOjRyeR9Ln/kS1yyX5T5dzYNs6r0sSEREU3NKFKePHsfMTf+BQq9F4zyc4tHeb1yWJiCQ8Bbd0ae7Jc1h5xl2kBfZTe8fFuMY6r0sSEUloCm7p1qJzFvPyjF8wvOlDttz+aQg0e12SiEjCUnBLRD7x6c/x+KgbGLN/OZV3XwmtrV6XJCKSkBTcEhEz41Nf/Bf+PPjLFFc/x0ePfMvrkkREEpKCWyKW5Pdx/jU/5am0CxlTcQ/bnvuF1yWJiCQcBbeckIy0ZE699k5e9Z3OyLd+TO2bf/S6JBGRhKLglhM2PCeDMf/3QVYwlZwXvsb+9a94XZKISMJQcEuPjCvMw3f5Q3zkCvD/6QqaqlZ7XZKISEJQcEuPzZ48lqrzf8/+1jQa772E4N4tXpckIjLgKbilVxaeejJ/P/13+AON7P7dRbiDe7wuSURkQFNwS699cvG5PDH55+Q0VrHjd5+ElkavSxIRGbAU3BIVV3z2Ch4c8V2G171L9T1XQGvQ65JERAakJK8LkIHB5zOu+NJ1HPzJzyisXgI/GnrU97vIIe9mnQMXEektBbdETVqyH6ypw+/y2NfH1YiIDEwKbukzzU99m9SCyZA3KfTKHA5mXpclIhJXFNzSZ4LL7wM7MrJYS1ImLUMmkFowBf/wSZA7MRToQ8dBUop3hYqI9GMKbukz95e9zs5tH9Jcs4H0fR8wOrCN8TXbGb/zRQrt4bb5Ws1PIHs0ScMn4Rs26cgeeu5EyMj18CcQEfFenwe3mY0CHgAKgFbgTufcr/q6Dul71y6cCEwEziUQbKVy90Eqaup5aEc9H23fQVPNBtL3b2acbWf8nu1MqFvH2E2vkEJLWx/BtKH4hk3C8sJ754ffB48Bv7ZDRWTg8+L/dAHgW865d8wsC1hhZi8559Z5UItE2S5yOrwQLdR+RJLfx4ThmUwYnskFFAKTgDNoagny/s4GNtTU85cd9WysrqN+x2ayGj5kvG1nfKCaSY3VTNj2FINb69r6c75kLHc85E44soeeNwnyJkBaTqx/bBGRPtPnwe2cqwaqw9P1ZrYeGAkouAeAzm75yuuw9XhpyX6mj8xh+sj2YXs6+w62sHFnPRU19TxRU0/Fjnq2V1czrHkL433bGW/bmbqrhol736Vgw3P4aXcfeWb+kb3z3HZ76jmjwBd+lMHPJ8KBnccXlDEc/nVThNWLiMSep8cWzawYmA285WUd0v/lDErmlOKhnFJ85P5w5xw765upqKln4456ngkH+uYdexkeqGG8bWecVVPSuJPJ1dWM3Pou6cH9RzpNSgvvoXcS2tB5u4iIR8w5582KzTKBvwL/7px7rIPvrwKuAsjPz5/z8MMPHztLjzU0NJCZmRm1/qR/aXWOXY2OrfWtbGtopaq+laqGVmoOtDLE7WecVTPRt52SlO1M8lczmu3kBXZ02l952RN9WH3P6HdaxFvR/htcuHDhCudcaUffeRLcZpYMPA284Jz7f93NX1pa6pYvXx619ZeXl1NWVha1/iQ+NAeCfLjrABU19W176RU76tm6p5HKtH/sfMGiU2DiuTDhHCicdeTwej+i32kRb0X7b9DMOg1uL64qN+BuYH0koS0SLalJfqYUZDOlIPuo9obmAPxn58tt39tA4av/gb3675AxDCYsgomLYPxCSB8S46pFRI7mxTnu+cDngffMbFW47bvOuWc9qEWEzNSu/ww+Vvd9soN1LEpZw6dYx8x1T5P67h/B/DBqbijEJ54L+dP1JDgRiTkvrip/DdD/3aRfqXU5DLPjb2OrdTms+v65vPHBbl6tmMF1G3ayo/4AM+0DPpO9joW73qVgy49gyY8gqzB0OH3iuTCuDNKyj1+RiEgv6YkVIsB5yXezq+HQce15mSksT01i0Un5LDopH+ccm3Y28OqGaTxZcTo3Ve5hSOseFqeu4ZLWtcx473FSVv4efEkw+vRQiE9cBMOmaG9cRKJCwS0CLP+3RRHNZ2ZMys9iUn4WV39sPPVNLbz+/m7KK0r4SsVOdjcc4GTbxKXZ61m4cxXDKm+Cl24K3TN++JD62DMhJSPGP5GIDFQKbpFeyEpLZvH0AhZPL8A5x4aael6tmMajG+Zz45a9DG/dxeK097gksI5pKx8iafk94E+BMfPDe+PnQu547Y2LSMQU3CJRYmZMLcxmamE2XymbwL7GFl7btIvyipn808Za9h1ooNRXwWdy1nNm9SpyN98IL9wIQ4qPhHjxAkhO9/pHEZF+TMEtEiM56clcUFLIBSWFtLY61lXvp7xiGg9W1PKtLXsZwU7OT1vDxYfWMmXFAyQtuzP0NLexZx655WzoWK9/DBHpZxTcIn3A57O2Z7D/81kTqTt4iKWbdlG+YTZXbqyl4UADp/nWc2n6BhZsXcmQTS/Cc4SerT7xXJh4TujwelKq1z+KiHhMwS3igcGDUrh45ggunjmC1lbHe9v28WrFNO6uqOXrVXWMoZoL0tZwUdNaJi67C//ffwvJGTDuY6E98QmL4K6z2p6lXgZQHu5cA6OIDGgKbhGP+XzGzFGDmTlqMNedM4ndDc0s3VRLecUcPruxlqaDDcz3r+PS9PXMr3yH7IpunlWkgVFEBjQFt0g/k5uZyidnF/HJ2UUEWx2rttbx14pp3FZRy7Xb6hhv27kwfQ3Xt97feScfvAqFM2HQ0M7nEZG4pOAW6cf8PmPOmCHMGTOEb547mZ31TSzduItXK0phYxfB/ftLQu/ZRVBYAgUloSAvLIHskbr9TCSOKbhF4sjwrDQunVPEpXOK4ObO57vKvs9ZOTXMTv6IUdUVpFc8hxEeCTB96NFhXlASupfc5++Tn0FEekfBLTIADZm2iHu27mVTVQPOQTpNnDN0Fwtzqinxb2Hk/o2kfXQHFgw/5jU5A/KnhQL9cJgPn6qr2EX6IQW3SJzqamCUn15aAkB9Uwurq/axcsteVm6p4ydb69hz4BQAslMci/P38bGsaqb7Kils3ETKu4/A2/8T6siXBMOmtts7L4GCGZCa1Wc/o4gcT8EtEqe6HBglPJ2Vlsz8CXnMn5AHgHOOLXsOsnJLXSjMt9bx2IYhBFqnAucxanAq54xt5IzM7ZxklQxv2IBv04uw6g9HVjB0/DFhPhMyh8X+BxYRQMEtErfaD4xSXl5OWVlZt8uYGWNyMxiTm8Els0cC0NQSZM22faEw37qXF7YY9+4rAopI8Z/JtBFZnDEuyPzMbUzhQ7Lr1mPb3oG1jx/pOKuwXZCHD7cPHn38RXA/n9jx7Wq691wkYgpukQSXluyntHgopcVHbh2r2dfEqq17w3vmddy5qp5bW3KAWQzLOpVZowZz2jQfpw3azoTWzaTWroHq1fD+S+Bawx0PDh1aP3zOvLCk83vMde+5SMQU3CJynIKcNBbnFLJ4eiEALcFWKmrq286Vr9xax0vrDgCGz8YzuWA2s0cPZk5pGnMH1TCyaRO+mndDYf72/0CgqfuVLr839Kz2pNQjL39qJ23t2mN9NbyOEkg/o+AWkW4l+31tz1r//Omhtj0HDvHu1iPnyp9atZ0/vhUAIDttJLNGT2f22K9y8hmZzB5US3bdenj86s5X8vR1PSvOlxQKcX9KOMxTjoS6v13gH9fWbl5/yjEbCO3aujpKEAyAvx/+b1QbGwNaP/yNE5F4MDQjhYVThrNwynAAWlsdm3c18E748PrKLXv59SubaA3fPj5uWAGvdNXhN9eH9swDzUdewcPT7dubIHgo3HboyHfB5k7amuHggc6XDTb3/B/hx7mhgE9Oh+RBx7ynh26zO66to/kGQcqgzr/zp5zYQ3Pi8ZSENjYipuAWkajw+YwJw7OYMDyLfygdBUBDc4DVVXVt58qp73z5yT97l7RkP2nJvtB7Umg6NTmTtOQc0pJ8pCb7SUvyHT1fsp/U1CPTacm+8LJH5kkNL5Paru9kv2Fm4FzXGwJ3ndV50Qu/By0H4dDB0HtLY/gVnm6sO74t0Hji/7jmO7ENgq6seSx0lMKfDL7k0KmGtumk0BEEX3K4zd9uOqndcuF5fL4T/1k6E+cbG2XQZwP9KLhFJGYyU5OYNz6PeeNDt6PV/qDze8+/MK+YppYgTS2tNAWCR6ZbguxrbGFny5G25sCR+ZzrWW0+40jYtwV7++D3kZrk544u+vjf7CsYlOJnUEoS6Sn+8HTo86AUP+nJfny+Y/aUW1tD4d0+zI97b+x6g6B928Hd4ekDR9q68ugXe/YP1hHzHQlxf1K76WNDv7MNgHYbDl156QdHlvP5w68kMP/xbYdfbbUltfvu2OWSQhsfPV3Oo40NBbeI9JlTmm/v9LvK86eecH/OOQ4FW0Nh3knoN7UEaQqEppuPDf6WYHj+1mM2CoLsagjQ1BLscv3XPbKq2xrTkn2hYE/2k5HqJz0liUHJoYBPT/GTkZJEekoKg1LSGZQyLPT94Q2AnKS2+Qa1zRuaTkvqYKPgsJtzOi/oK3+H1gAEW0LvbdMt0BrsZDoQOp/fNn3sssf0Ewx/bm0JtwWPng40QXP90ct15e+3hZZzXf/3SBQKbhGJW2ZGapKf1CQ/pHez19ZDXR0lWPKtj9F4KMjBQ0EOHgrQeCjIgUNBGg8Fwm1BGltC3x1sDs/XEvq+Zn9L27IHwssGWk/s8EF6eANgUKqfQclHQv2PXSzzy1U+/L5UknxpJPl9JPkMv89I8hlJfl/bdOjdR1LyMZ/9J/65fZ/W2bn6rjY2bqoNvTt3JMAPh35rMPwKf3btPx8zX0+Wc63tljlmuaU/O6H/XtGi4BaRPpOXmdLp0976qy6PEgzLjOq6DgVaQ2HeEg7+5nDotwTbQv7wRkH7DYS2jYfwRkFXj8O9rfwDgie4gRBNbRsJh8M9vPGwrItlPvGb1/D5DL9Z27vfd3g61KfvqDZr1wZ+XxI+Sz5qvqO/94WXoYPlw336LXR0v93yn0DBLSIDXPunvcWLvtzYSEnykZLkI4feHT0ovqGLjY1bzqe11RF0jmCrI9DqCARbCbR28jnoCLQe+dwSbG2bL3jMd8fOe9TnYLjvDj4HWx2173a+sTF4UAqt4XqD4WWaA46gC93NEGx1R753ru3na23luLZg69HfB1pb6el2zCfSerZcbym4RUS6EI8bG93x+QwfRnI/Gsm1eFkXGxtfmhvTdTvnaHV0vAHQfiPgmLba33S+sRHLp/cruEVEBph4PCXhJbMjh9xPRHFXp1F6WVNXFNwiIgNMPB4liMeNDa9qVnCLiIjn4nFjoycj9EVDFB97IyIiIrGm4BYREYkjCm4REZE4ouAWERGJIwpuERGROKLgFhERiSMKbhERkTii4BYREYkj5no6Cn0fMrNa4KModpkH7IpifyJe0++0iLei/Tc4xjnX4SPP4yK4o83MljvnSr2uQyRa9Dst4q2+/BvUoXIREZE4ouAWERGJI4ka3Hd6XYBIlOl3WsRbffY3mJDnuEVEROJVou5xi4iIxKWECm4zu8fMdprZGq9rEYkGM6s0s/fMbJWZLfe6HpFE0FGWmNlQM3vJzDaF34fEav0JFdzAfcBir4sQibKFzrlZuh1MpM/cx/FZcgOwxDk3EVgS/hwTCRXczrmlwB6v6xARkfjVSZZ8Arg/PH0/cEms1p9QwS0yADngRTNbYWZXeV2MSALLd85VA4Tfh8dqRUmx6lhE+sR859x2MxsOvGRmG8J7AyIyQGmPWySOOee2h993Ao8Dc72tSCRh7TCzQoDw+85YrUjBLRKnzCzDzLIOTwPnArpjQsQbTwJXhqevBJ6I1YoSKrjN7CHgTWCymVWZ2Ze9rkmkF/KB18zsXWAZ8Ixz7nmPaxIZ8DrJkluARWa2CVgU/hyb9evJaSIiIvEjofa4RURE4p2CW0REJI4ouEVEROKIgltERCSOKLhFRETiiIJbJAGYWTA8gtjhV9QGQDCzYo24J9J39MhTkcTQ6Jyb5XURItJ72uMWSWDh8bx/ambLwq8J4fYxZrbEzFaH30eH2/PN7HEzezf8mhfuym9md5nZWjN70czSPfuhRAY4BbdIYkg/5lD5Ze2+2++cmwv8BvjvcNtvgAeccyXAH4Bbw+23An91zs0ETgbWhtsnAr91zk0D6oBPx/SnEUlgenKaSAIwswbnXGYH7ZXAWc65zWaWDNQ453LNbBdQ6JxrCbdXO+fyzKwWKHLONbfroxh4yTk3Mfz5O0Cyc+4nffCjiSQc7XGLiOtkurN5OtLcbjqIrp8RiRkFt4hc1u79zfD0G8Bnw9NXAK+Fp5cA1wKYmd/MsvuqSBEJ0VaxSGJIN7NV7T4/75w7fEtYqpm9RWhD/vJw29eBe8zsX4Fa4Ivh9m8Ad4ZHQwoSCvHqWBcvIkfoHLdIAguf4y51zu3yuhYRiYwOlYuIiMQR7XGLiIjEEe1xi4iIxBEFt4iISBxRcIuIiMQRBbeIiEgcUXCLiIjEEQW3iIhIHPn/ftOX/Vi/V+UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_plot_mse(mean_model,\n",
    "            mse_mean = None,\n",
    "            start_epoch = 1,\n",
    "            savefig = False,\n",
    "            file = \"../img/wine/wine_enkf_E{}_B{}_P{}_H{}.png\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
