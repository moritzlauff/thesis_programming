{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../python/functions\")\n",
    "sys.path.insert(2, \"../python/architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../python/architecture\\reproducible.py:14: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_prep_functions import wine_prep\n",
    "from plotting_functions import nn_plot_mse\n",
    "from enkf_functions import enkf_regressor\n",
    "from saving_functions import load_objects, save_objects\n",
    "from model_functions import nn_load, nn_model_structure, nn_model_compile\n",
    "import reproducible\n",
    "import no_gpu\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = wine_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use samller dataset for increased speed\n",
    "X_train = X_train[:1000]\n",
    "X_test = X_test[:500]\n",
    "y_train = y_train[:1000]\n",
    "y_test = y_test[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 4\n",
    "neurons = [32, 32, 16, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enkf_regressor_analysis(X_train,\n",
    "                            X_test,\n",
    "                            y_train,\n",
    "                            y_test,\n",
    "                            layers,\n",
    "                            neurons,\n",
    "                            setting_dict,\n",
    "                            analysis_dict,\n",
    "                            save_all = False,\n",
    "                            file_var = \"file.pckl\",\n",
    "                            file_model = \"file.h5\",\n",
    "                            verbose = 0\n",
    "                            ):\n",
    "\n",
    "    \"\"\" Ensemble Kalman Filter algorithm analysis for regression problems.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train (np.ndarray): Training data X.\n",
    "    X_test (np.ndarray): Test data X.\n",
    "    y_train (pd.DataFrame): Training data y.\n",
    "    y_test (pd.DataFrame): Test data y.\n",
    "    layers (int): Number of layers.\n",
    "    neurons (list): Number of neurons in each layer.\n",
    "    setting_dict (dict): Dictionary containing\n",
    "        particles (int): Number of particles in the ensemble.\n",
    "        epochs (int): Number of epochs.\n",
    "        batch_size (None or int): Size of the batches. Must be between 0 and the number of observations in the training set.\n",
    "        h_0 (int or float): Starting step size.\n",
    "        delta (float): Constant for numerical stability in the jacobian.\n",
    "        epsilon (float): Constant for numerical stability in the step size.\n",
    "        randomization (bool): Whether or not to add noise to the particles and randomize them around their mean.\n",
    "        shuffle (bool): Whether or not to shuffle the data prior to each epoch.\n",
    "        early_stopping (bool): Whether or not to stop the calculation when the changes get small.\n",
    "        early_stopping_diff (bool): Minimum change before early stopping is applied.\n",
    "    analysis_dict (dict): Dictionary containing\n",
    "        disjoint_batch (bool): Whether or not to use disjoint batches. If False then each batch is sampled with replacement.\n",
    "        multiple_updates_same_batch (int or None): Number of consecutive updates on the same batch.\n",
    "        batch_particle_connection (dict): Dictionary containing\n",
    "            connect (bool): Whether or not to connect particles and batches.\n",
    "            shuffle (bool): Whether or not to shuffle the connection.\n",
    "            update_all (bool): Whether or not to update after all particles have seen some data.\n",
    "        batch_evaluation (dict): Dictionary containing\n",
    "            mean_model (bool): Whether or not evaluate the mean model after every batch on that batch. Warning: the computational costs are very high.\n",
    "            particles (bool): Whether or not evaluate every particle model after every batch on that batch. Warning: the computational costs are incredibly high.\n",
    "            file (str): Path and name of the file to save evaluations into \n",
    "        tikhonov (dict): Dictionary containing\n",
    "            regularize (bool): Whether or not to use Tikhonov regularization.\n",
    "            lambda (None or float): Lambda parameter in Tikhonov regularization.\n",
    "            reg_mse_stop (bool): Whether or not to stop when MSE + Tikhonov regularization starts to rise again.\n",
    "    save_all (bool): Whether or not to save all important variables and models.\n",
    "    file_var (str): Path and name of the file to save variables into.\n",
    "    file_model (str): Path and name of the file to save the final model into.\n",
    "    verbose (int): If 0, then don't print anything throughout the training process. If 1, then print training and test accuracy after each epoch.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    mean_model (tensorflow.python.keras.engine.sequential.Sequential): The final model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    particles = setting_dict[\"particles\"]\n",
    "    epochs = setting_dict[\"epochs\"]\n",
    "    batch_size = setting_dict[\"batch_size\"]\n",
    "    h_0 = setting_dict[\"h_0\"]\n",
    "    delta = None\n",
    "    epsilon = setting_dict[\"epsilon\"]\n",
    "    randomization = setting_dict[\"randomization\"]\n",
    "    shuffle = setting_dict[\"shuffle\"]\n",
    "    early_stopping = setting_dict[\"early_stopping\"]\n",
    "    early_stopping_diff = setting_dict[\"early_stopping_diff\"]\n",
    "    \n",
    "    disjoint_batch = analysis_dict[\"disjoint_batch\"]\n",
    "    batch_evaluation = analysis_dict[\"batch_evaluation\"]\n",
    "    batch_particle_connection = analysis_dict[\"batch_particle_connection\"][\"connect\"]\n",
    "    batch_particle_shuffle = analysis_dict[\"batch_particle_connection\"][\"shuffle\"]\n",
    "    update_all = analysis_dict[\"batch_particle_connection\"][\"update_all\"]\n",
    "    mult_updates_batch = analysis_dict[\"multiple_updates_same_batch\"]\n",
    "    tik_regularize = analysis_dict[\"tikhonov\"][\"regularize\"]\n",
    "    tik_lambda = analysis_dict[\"tikhonov\"][\"lambda\"]\n",
    "    reg_mse_stop = analysis_dict[\"tikhonov\"][\"reg_mse_stop\"]\n",
    "    \n",
    "    if tik_lambda is None:\n",
    "        tik_lambda = 0\n",
    "    \n",
    "    if mult_updates_batch is None:\n",
    "        mult_updates_batch = 1\n",
    "\n",
    "    if batch_size == None:\n",
    "        batch_size = len(X_train)\n",
    "\n",
    "    n_cols = X_train.shape[1]\n",
    "    \n",
    "    if disjoint_batch:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        batch_indices = np.cumsum([0] + list(np.ones(num_batches) * batch_size))\n",
    "        batch_indices[-1] = n\n",
    "    else:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        last_batch_size = n % batch_size\n",
    "        \n",
    "    if batch_particle_connection:\n",
    "        batch_particle_dict = {}\n",
    "        batch_particle_indices = np.arange(particles) + 1\n",
    "        np.random.shuffle(batch_particle_indices)\n",
    "        if particles == num_batches:\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "        elif particles > num_batches:\n",
    "            base_batches = particles // num_batches\n",
    "            add_batches = particles % num_batches\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "            for i in range(add_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "        elif num_batches > particles:\n",
    "            num_reps = int(np.ceil(num_batches / particles))\n",
    "            particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "    else:\n",
    "        batch_particle_dict = None\n",
    "\n",
    "    model_dict = {}\n",
    "    weights_dict = {}\n",
    "    y_pred_dict = {}\n",
    "    jacobian_dict = {}\n",
    "    weights_vector_dict = {}\n",
    "    train_mse_dict = {}\n",
    "    test_mse_dict = {}\n",
    "    \n",
    "    if batch_evaluation[\"mean_model\"]:\n",
    "        train_batch_mse_mean_dict = {}\n",
    "    if batch_evaluation[\"particles\"]:\n",
    "        train_batch_mse_particle_dict = {}\n",
    "\n",
    "    # init_model already has weights and biases following the Glorot distribution\n",
    "    # it can already be used to predict and evaluate, but it is very bad\n",
    "    # only used to determine shapes and shape_elements via its weights\n",
    "    init_model = nn_model_structure(layers = layers,\n",
    "                                    neurons = neurons,\n",
    "                                    n_cols = n_cols,\n",
    "                                    classification = False)\n",
    "    init_model = nn_model_compile(init_model,\n",
    "                                  optimizer = \"sgd\")\n",
    "    weights = init_model.get_weights()\n",
    "    # shape contains the shapes of the weight matrices and bias vectors as a list of arrays\n",
    "    shapes = [np.array(params.shape) for params in weights]\n",
    "    # shape_elements contains the indices of the weights as a vector and tells where to cut\n",
    "    shape_elements = np.cumsum([0] + [np.prod(shape) for shape in shapes])\n",
    "\n",
    "    for i in range(particles):\n",
    "        \n",
    "        # just an initial model with the correct structure regarding neurons, layers, activation functions, Glorot initialization\n",
    "        model = nn_model_structure(layers = layers,\n",
    "                                   neurons = neurons,\n",
    "                                   n_cols = n_cols,\n",
    "                                   classification = False)\n",
    "        model = nn_model_compile(model,\n",
    "                                 optimizer = \"sgd\")\n",
    "        # for every particle write the model in a dictionary\n",
    "        model_dict[\"model_{}\".format(str(i+1))] = model\n",
    "\n",
    "        # for every particles write the weights and biases in a dictionary\n",
    "        weights_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .get_weights()\n",
    "        \n",
    "        # for every particle write the predictions on the training batches in a dictionary\n",
    "        y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .predict(X_train[:batch_size,:])\n",
    "\n",
    "        # for every particle write the Jacobian in a dictionary\n",
    "        jacobian_dict[\"model_{}\".format(str(i+1))] = 1/len(y_train[:batch_size]) * (-2)*(y_train[:batch_size] - y_pred_dict[\"model_{}\".format(str(i+1))].ravel())\n",
    "\n",
    "        train_mse_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        test_mse_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        \n",
    "        if batch_evaluation[\"particles\"]:\n",
    "            train_batch_mse_particle_dict[\"particle_{}\".format(str(i+1))] = {}\n",
    "\n",
    "    # mean_model as the model with the mean of the weights of all particle models\n",
    "    mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "    mean_model = init_model\n",
    "    mean_model.set_weights(mean_weights)\n",
    "\n",
    "    mean_model_train_mse = np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1])\n",
    "    mean_model_test_mse = np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1])\n",
    "    if tik_regularize:\n",
    "        mean_weights_raveled = [arr.ravel() for arr in mean_weights]\n",
    "        mean_model_train_mse_reg = mean_model_train_mse + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2)\n",
    "        mean_model_test_mse_reg = mean_model_test_mse + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2)\n",
    "\n",
    "    # loop over all epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # early stopping\n",
    "        if early_stopping:\n",
    "            if epoch == 0:\n",
    "                train_mse_old = 0\n",
    "                test_mse_old = 0\n",
    "            else:\n",
    "                train_mse_new = mean_model_train_mse[epoch]\n",
    "                test_mse_new = mean_model_test_mse[epoch]\n",
    "                if np.absolute(test_mse_new - test_mse_old) <= early_stopping_diff and np.absolute(train_mse_new - train_mse_old) <= early_stopping_diff:\n",
    "                    print(\"STOP: Early Stopping after epoch {} because improvement in training mse is only {} and in test mse only {}.\"\\\n",
    "                                                                         .format(epoch, train_mse_new - train_mse_old, test_mse_new - test_mse_old))\n",
    "                    break\n",
    "                test_mse_old = test_mse_new\n",
    "                                                                            \n",
    "        # Tikhonov regularization stopping\n",
    "        if tik_regularize and reg_mse_stop:\n",
    "            if epoch >= 1:\n",
    "                if mean_model_train_mse_reg[epoch] > mean_model_train_mse_reg[epoch-1] and mean_model_test_mse_reg[epoch] > mean_model_test_mse_reg[epoch-1]:\n",
    "                    print(\"Training and test MSEs containing Tikhonov regularization start to rise. Algorithm is stopped after epoch {}.\".format(epoch))\n",
    "                    break\n",
    "                \n",
    "        # shuffle the data\n",
    "        if shuffle:\n",
    "            indices = y_train.sample(frac=1).index\n",
    "        else:\n",
    "            indices = y_train.index\n",
    "            \n",
    "        if disjoint_batch:\n",
    "            X_batches = [np.array(X_train)[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [y_train.iloc[indices].reset_index(drop = True)[int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [np.array(i) for i in y_batches]\n",
    "        else:\n",
    "            if last_batch_size != 0:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches-1)]\n",
    "                indices.append(np.random.choice(len(X_train), size = last_batch_size, replace = True))\n",
    "            else:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches)]\n",
    "            X_batches = [X_train[indices[i]] for i in range(len(indices))]\n",
    "            y_batches = [y_train[indices[i]] for i in range(len(indices))]\n",
    "            \n",
    "        if batch_particle_connection and batch_particle_shuffle:\n",
    "            shuffled_indices = np.hstack(list(batch_particle_dict.values()))\n",
    "            np.random.shuffle(shuffled_indices)\n",
    "            batch_particle_values = list(batch_particle_dict.values())\n",
    "            for i in range(len(batch_particle_values)):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = shuffled_indices[i*len(batch_particle_values[i]):(i+1)*len(batch_particle_values[i])]\n",
    "            \n",
    "        if batch_evaluation[\"mean_model\"]:\n",
    "            train_batch_mse_mean_dict[\"Epoch_{}\".format(str(epoch+1))] = {}\n",
    "        if batch_evaluation[\"particles\"]:\n",
    "            train_batch_mse_particle_dict[\"particle_{}\".format(str(i+1))][\"Epoch_{}\".format(str(epoch+1))] = {}\n",
    "                    \n",
    "        # loop over all batches\n",
    "        for b in range(num_batches):\n",
    "            for mult_updates in range(mult_updates_batch):\n",
    "                batch_particles = []\n",
    "                y_pred_batch_dict = {}\n",
    "                jacobian_batch_dict = {}\n",
    "                for i in range(particles):\n",
    "                    if batch_particle_connection: \n",
    "                        if num_batches == particles or num_batches > particles:\n",
    "                            if batch_particle_dict[\"batch_{}\".format(str(b+1))] != i+1:\n",
    "                                continue\n",
    "                        else:\n",
    "                            if i+1 not in batch_particle_dict[\"batch_{}\".format(str(b+1))]:\n",
    "                                continue\n",
    "                    if batch_particle_connection:\n",
    "                        batch_particles.append(i+1)\n",
    "                    \n",
    "                    # set new weights for model\n",
    "                    model_dict[\"model_{}\".format(str(i+1))].set_weights(weights_dict[\"model_{}\".format(str(i+1))])\n",
    "\n",
    "                    # for every particle write the predictions on the training batches in a dictionary\n",
    "                    y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                    .predict(X_batches[b])\n",
    "                    y_pred_batch_dict[\"model_{}\".format(i+1)] = y_pred_dict[\"model_{}\".format(i+1)]\n",
    "\n",
    "                    # for every particle write the Jacobian in a dictionary\n",
    "                    jacobian_dict[\"model_{}\".format(str(i+1))] = 1/len(y_batches[b]) * (-2)*(y_batches[b] - y_pred_dict[\"model_{}\".format(str(i+1))].ravel())\n",
    "                    jacobian_batch_dict[\"model_{}\".format(i+1)] = jacobian_dict[\"model_{}\".format(i+1)]\n",
    "\n",
    "                    if batch_evaluation[\"mean_model\"]:\n",
    "                        train_batch_mse_mean_dict[\"Epoch_{}\".format(str(epoch+1))][\"Batch_{}\".format(str(b+1))] = mean_model.evaluate(X_batches[b], y_batches[b], verbose = 0)[1]\n",
    "                    if batch_evaluation[\"particles\"]:\n",
    "                        train_batch_mse_particle_dict[\"particle_{}\".format(str(i+1))][\"Epoch_{}\".format(str(epoch+1))][\"Batch_{}\".format(str(b+1))] = model_dict[\"model_{}\".format(str(i+1))].evaluate(X_batches[b], y_batches[b], verbose = 0)[1]\n",
    "\n",
    "                        \n",
    "                if not batch_particle_connection:        \n",
    "                    # compute the mean of the predictions\n",
    "                    y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "                    # compute the matrix D elementwise\n",
    "                    d = np.zeros(shape = (particles, particles))\n",
    "                    for k in range(particles):\n",
    "                        y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                        for j in range(particles):\n",
    "                            d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_dict[\"model_{}\".format(str(j+1))])\n",
    "                    d = np.transpose(d)\n",
    "\n",
    "                    # compute the scalar h_t\n",
    "                    h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "                    # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "                    for i in range(particles):\n",
    "                        weights_array = np.array([])\n",
    "                        for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                            weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "                    # matrix with particle parameters as row vectors\n",
    "                    weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "                    # compute the matrix with the updates for each particle\n",
    "                    weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "                    for i in range(particles):\n",
    "                        # write the updates back into the dictionary\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                        # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                        for l in range(len(shape_elements)-1):\n",
    "                            start = shape_elements[l]\n",
    "                            end = shape_elements[l+1]\n",
    "                            weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                \n",
    "                elif batch_particle_connection and not update_all:\n",
    "                    # compute the mean of the predictions\n",
    "                    y_pred_mean = np.mean(list(y_pred_batch_dict.values()), axis = 0)\n",
    "\n",
    "                    # compute the matrix D elementwise\n",
    "                    d = np.zeros(shape = (len(y_pred_batch_dict), len(y_pred_batch_dict)))\n",
    "                    for k in range(len(y_pred_batch_dict)):\n",
    "                        y_pred_centered = y_pred_batch_dict[\"model_{}\".format(batch_particles[k])] - y_pred_mean\n",
    "                        for j in range(len(y_pred_batch_dict)):\n",
    "                            d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_batch_dict[\"model_{}\".format(batch_particles[j])])\n",
    "                    d = np.transpose(d)\n",
    "\n",
    "                    # compute the scalar h_t\n",
    "                    h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "                    # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "                    for i in range(particles):\n",
    "                        weights_array = np.array([])\n",
    "                        for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                            weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "                    # matrix with particle parameters as row vectors\n",
    "                    weights_vector_batch_dict = {}\n",
    "                    for i in range(len(batch_particles)):\n",
    "                        weights_vector_batch_dict[\"model_{}\".format(batch_particles[i])] = weights_vector_dict[\"model_{}\".format(batch_particles[i])]\n",
    "                        weights_all_ptcls = np.array(list(weights_vector_batch_dict.values()))\n",
    "\n",
    "                    # compute the matrix with the updates for each particle\n",
    "                    weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "                    for i in range(particles):\n",
    "                        # write the updates back into the dictionary\n",
    "                        k = -1\n",
    "                        for i in range(particles):\n",
    "                            if batch_particle_connection:\n",
    "                                if i not in batch_particles:\n",
    "                                    continue\n",
    "                                k += 1\n",
    "                                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[k]\n",
    "                        # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                        for l in range(len(shape_elements)-1):\n",
    "                            start = shape_elements[l]\n",
    "                            end = shape_elements[l+1]\n",
    "                            weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "\n",
    "        if batch_particle_connection and update_all:\n",
    "            # compute the mean of the predictions\n",
    "            y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "            # compute the matrix D elementwise\n",
    "            d = np.zeros(shape = (particles, particles))\n",
    "            for k in range(particles):\n",
    "                y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                for j in range(particles):\n",
    "                    d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_dict[\"model_{}\".format(str(j+1))])\n",
    "            d = np.transpose(d)\n",
    "\n",
    "            # compute the scalar h_t\n",
    "            h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "            # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "            for i in range(particles):\n",
    "                weights_array = np.array([])\n",
    "                for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                    weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "            # matrix with particle parameters as row vectors\n",
    "            weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "            # compute the matrix with the updates for each particle\n",
    "            weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "            for i in range(particles):\n",
    "                # write the updates back into the dictionary\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                for l in range(len(shape_elements)-1):\n",
    "                    start = shape_elements[l]\n",
    "                    end = shape_elements[l+1]\n",
    "                    weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                                                            \n",
    "        for i in range(particles):\n",
    "            # for every particle write the training mse of the current iteration in a dictionary\n",
    "            train_mse_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_train, y_train, verbose = 0)[1])\n",
    "\n",
    "            # for every particle write the test mse of the current iteration in a dictionary\n",
    "            test_mse_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_test, y_test, verbose = 0)[1])\n",
    "\n",
    "        # update the mean_model\n",
    "        mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "        mean_model.set_weights(mean_weights)\n",
    "\n",
    "        mean_model_train_mse = np.append(mean_model_train_mse, np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1]))\n",
    "        mean_model_test_mse = np.append(mean_model_test_mse, np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1]))\n",
    "        if tik_regularize and reg_mse_stop:\n",
    "            mean_weights_raveled = [arr.ravel() for arr in mean_weights]\n",
    "            mean_model_train_mse_reg = np.append(mean_model_train_mse_reg, mean_model_train_mse[-1] + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2))\n",
    "            mean_model_test_mse_reg = np.append(mean_model_test_mse_reg, mean_model_test_mse[-1] + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2))\n",
    "        \n",
    "\n",
    "        if verbose == 1:\n",
    "            print(\"Epoch {}. Training MSE: {}, Test MSE: {}.\".format(epoch+1,\n",
    "                                                                     np.round(mean_model_train_mse[-1], 3),\n",
    "                                                                     np.round(mean_model_test_mse[-1], 3)))\n",
    "            \n",
    "    mean_model.history.history = {\"mse\": mean_model_train_mse[1:],\n",
    "                                  \"val_mse\": mean_model_test_mse[1:]}\n",
    "        \n",
    "    if save_all:\n",
    "        param_dict = param_to_dict(X_train,\n",
    "                                   X_test,\n",
    "                                   y_train,\n",
    "                                   y_test,\n",
    "                                   layers,\n",
    "                                   neurons,\n",
    "                                   particles,\n",
    "                                   epochs,\n",
    "                                   batch_size,\n",
    "                                   h_0,\n",
    "                                   delta,\n",
    "                                   epsilon,\n",
    "                                   randomization,\n",
    "                                   shuffle,\n",
    "                                   early_stopping,\n",
    "                                   early_stopping_diff\n",
    "                                   )\n",
    "        results_dict = results_to_dict(mean_model_train_mse,\n",
    "                                       mean_model_test_mse,\n",
    "                                       train_mse_dict,\n",
    "                                       test_mse_dict,\n",
    "                                       weights_dict,\n",
    "                                       y_pred_dict,\n",
    "                                       False\n",
    "                                       )\n",
    "\n",
    "        saving_dict = {}\n",
    "        saving_dict[\"parameters\"] = param_dict\n",
    "        saving_dict[\"results\"] = results_dict\n",
    "        saving_dict[\"analysis\"] = analysis_dict\n",
    "\n",
    "        save_objects(obj_dict = saving_dict,\n",
    "                     file = file_var)\n",
    "\n",
    "        nn_save(model = mean_model,\n",
    "                path_name = file_model)\n",
    "        \n",
    "        if batch_evaluation[\"mean_model\"]:\n",
    "            full_dict = {}\n",
    "            full_dict[\"mean_model\"] = train_batch_mse_mean_dict\n",
    "            if batch_evaluation[\"particles\"]:\n",
    "                full_dict[\"particles\"] = train_batch_mse_particle_dict\n",
    "            save_objects(obj_dict = full_dict,\n",
    "                         file = batch_evaluation[\"file\"])\n",
    "        elif batch_evaluation[\"particles\"]:\n",
    "            full_dict = {}\n",
    "            full_dict[\"particles\"] = train_batch_mse_particle_dict\n",
    "            if batch_evaluation[\"mean_model\"]:\n",
    "                full_dict[\"mean_model\"] = train_batch_mse_mean_dict\n",
    "            save_objects(obj_dict = full_dict,\n",
    "                         file = batch_evaluation[\"file\"])\n",
    " \n",
    "    return mean_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting_dict = {\"particles\": 20,\n",
    "                \"epochs\": 10,\n",
    "                \"batch_size\": 50,    # len(X_train)\n",
    "                \"h_0\": 2,\n",
    "                \"epsilon\": 0.5,\n",
    "                \"randomization\": False,\n",
    "                \"shuffle\": True,\n",
    "                \"early_stopping\": False,\n",
    "                \"early_stopping_diff\": 0.001\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dict = {\"disjoint_batch\": True,\n",
    "                 \"multiple_updates_same_batch\": None,\n",
    "                 \"batch_particle_connection\": {\"connect\": True,\n",
    "                                               \"shuffle\": False,\n",
    "                                               \"update_all\": True},\n",
    "                 \"batch_evaluation\": {\"mean_model\": False,\n",
    "                                      \"particles\": False,\n",
    "                                      \"file\": \"../objects/wine/batch_mse.pckl\"},\n",
    "                 \"tikhonov\": {\"regularize\": False,\n",
    "                              \"lambda\": None,\n",
    "                              \"reg_mse_stop\": False}\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training MSE: 14.861, Test MSE: 14.83.\n",
      "Epoch 2. Training MSE: 6.863, Test MSE: 6.854.\n",
      "Epoch 3. Training MSE: 3.737, Test MSE: 3.737.\n",
      "Epoch 4. Training MSE: 2.584, Test MSE: 2.588.\n",
      "Epoch 5. Training MSE: 1.625, Test MSE: 1.648.\n",
      "Epoch 6. Training MSE: 1.603, Test MSE: 1.63.\n",
      "Epoch 7. Training MSE: 1.608, Test MSE: 1.633.\n",
      "Epoch 8. Training MSE: 1.592, Test MSE: 1.617.\n",
      "Epoch 9. Training MSE: 1.597, Test MSE: 1.623.\n",
      "Epoch 10. Training MSE: 1.587, Test MSE: 1.608.\n"
     ]
    }
   ],
   "source": [
    "mean_model = enkf_regressor_analysis(X_train,\n",
    "                                     X_test,\n",
    "                                     y_train,\n",
    "                                     y_test,\n",
    "                                     layers,\n",
    "                                     neurons,\n",
    "                                     setting_dict,\n",
    "                                     analysis_dict,\n",
    "                                     save_all = False,\n",
    "                                     file_var = \"../objects/wine/wine_enkf_E{}_B{}_P{}_H{}.pckl\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                     file_model = \"../models/wine/wine_enkf_E{}_B{}_P{}_H{}.h5\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                     verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation time: 3.16470334927241 minutes.\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "print(\"Calculation time: {} minutes.\".format((end_time - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_plot_mse(mean_model,\n",
    "            mse_mean = None,\n",
    "            start_epoch = 1,\n",
    "            savefig = False,\n",
    "            file = \"../img/wine/wine_enkf_E{}_B{}_P{}_H{}.png\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
