{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../python/functions\")\n",
    "sys.path.insert(2, \"../python/architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../python/architecture\\reproducible.py:14: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_prep_functions import wine_prep\n",
    "from plotting_functions import nn_plot_mse\n",
    "from enkf_functions import enkf_regressor\n",
    "from saving_functions import load_objects, save_objects\n",
    "from model_functions import nn_load, nn_model_structure, nn_model_compile\n",
    "import reproducible\n",
    "import no_gpu\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = wine_prep()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# use samller dataset for increased speed\n",
    "X_train = X_train[:1000]\n",
    "X_test = X_test[:500]\n",
    "y_train = y_train[:1000]\n",
    "y_test = y_test[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 4\n",
    "neurons = [32, 32, 16, 1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def enkf_regressor_analysis(X_train,\n",
    "                            X_test,\n",
    "                            y_train,\n",
    "                            y_test,\n",
    "                            layers,\n",
    "                            neurons,\n",
    "                            setting_dict,\n",
    "                            analysis_dict,\n",
    "                            save_all = False,\n",
    "                            file_var = \"file.pckl\",\n",
    "                            file_model = \"file.h5\",\n",
    "                            verbose = 0\n",
    "                            ):\n",
    "\n",
    "    \"\"\" Ensemble Kalman Filter algorithm analysis for regression problems.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train (np.ndarray): Training data X.\n",
    "    X_test (np.ndarray): Test data X.\n",
    "    y_train (pd.DataFrame): Training data y.\n",
    "    y_test (pd.DataFrame): Test data y.\n",
    "    layers (int): Number of layers.\n",
    "    neurons (list): Number of neurons in each layer.\n",
    "    setting_dict (dict): Dictionary containing\n",
    "        particles (int): Number of particles in the ensemble.\n",
    "        epochs (int): Number of epochs.\n",
    "        batch_size (None or int): Size of the batches. Must be between 0 and the number of observations in the training set.\n",
    "        h_0 (int or float): Starting step size.\n",
    "        delta (float): Constant for numerical stability in the jacobian.\n",
    "        epsilon (float): Constant for numerical stability in the step size.\n",
    "        randomization (bool): Whether or not to add noise to the particles and randomize them around their mean.\n",
    "        shuffle (bool): Whether or not to shuffle the data prior to each epoch.\n",
    "        early_stopping (bool): Whether or not to stop the calculation when the changes get small.\n",
    "        early_stopping_diff (bool): Minimum change before early stopping is applied.\n",
    "    analysis_dict (dict): Dictionary containing\n",
    "        disjoint_batch (bool): Whether or not to use disjoint batches. If False then each batch is sampled with replacement.\n",
    "        multiple_updates_same_batch (int or None): Number of consecutive updates on the same batch.\n",
    "        batch_particle_connection (dict): Dictionary containing\n",
    "            connect (bool): Whether or not to connect particles and batches.\n",
    "            shuffle (str or None): Whether or not and how to shuffle the connection. None = no shuffle. \"batch\" = shuffle the batch for fixed particle sets. \"full\" = shuffle the particle sets and their corresponding batch.\n",
    "            update_all (bool): Whether or not to update after all particles have seen some data.\n",
    "        batch_evaluation (dict): Dictionary containing\n",
    "            mean_model (bool): Whether or not evaluate the mean model after every batch on that batch. Warning: the computational costs are very high.\n",
    "            particles (bool): Whether or not evaluate every particle model after every batch on that batch. Warning: the computational costs are incredibly high.\n",
    "            file (str): Path and name of the file to save evaluations into \n",
    "        tikhonov (dict): Dictionary containing\n",
    "            regularize (bool): Whether or not to use Tikhonov regularization.\n",
    "            lambda (None or float): Lambda parameter in Tikhonov regularization.\n",
    "            reg_mse_stop (bool): Whether or not to stop when MSE + Tikhonov regularization starts to rise again.\n",
    "    save_all (bool): Whether or not to save all important variables and models.\n",
    "    file_var (str): Path and name of the file to save variables into.\n",
    "    file_model (str): Path and name of the file to save the final model into.\n",
    "    verbose (int): If 0, then don't print anything throughout the training process. If 1, then print training and test accuracy after each epoch.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    mean_model (tensorflow.python.keras.engine.sequential.Sequential): The final model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    particles = setting_dict[\"particles\"]\n",
    "    epochs = setting_dict[\"epochs\"]\n",
    "    batch_size = setting_dict[\"batch_size\"]\n",
    "    h_0 = setting_dict[\"h_0\"]\n",
    "    delta = None\n",
    "    epsilon = setting_dict[\"epsilon\"]\n",
    "    randomization = setting_dict[\"randomization\"]\n",
    "    shuffle = setting_dict[\"shuffle\"]\n",
    "    early_stopping = setting_dict[\"early_stopping\"]\n",
    "    early_stopping_diff = setting_dict[\"early_stopping_diff\"]\n",
    "    \n",
    "    disjoint_batch = analysis_dict[\"disjoint_batch\"]\n",
    "    batch_evaluation = analysis_dict[\"batch_evaluation\"]\n",
    "    batch_particle_connection = analysis_dict[\"batch_particle_connection\"][\"connect\"]\n",
    "    batch_particle_shuffle = analysis_dict[\"batch_particle_connection\"][\"shuffle\"]\n",
    "    update_all = analysis_dict[\"batch_particle_connection\"][\"update_all\"]\n",
    "    mult_updates_batch = analysis_dict[\"multiple_updates_same_batch\"]\n",
    "    tik_regularize = analysis_dict[\"tikhonov\"][\"regularize\"]\n",
    "    tik_lambda = analysis_dict[\"tikhonov\"][\"lambda\"]\n",
    "    reg_mse_stop = analysis_dict[\"tikhonov\"][\"reg_mse_stop\"]\n",
    "    \n",
    "    if tik_lambda is None:\n",
    "        tik_lambda = 0\n",
    "    \n",
    "    if mult_updates_batch is None:\n",
    "        mult_updates_batch = 1\n",
    "\n",
    "    if batch_size == None:\n",
    "        batch_size = len(X_train)\n",
    "\n",
    "    n_cols = X_train.shape[1]\n",
    "    \n",
    "    if disjoint_batch:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        batch_indices = np.cumsum([0] + list(np.ones(num_batches) * batch_size))\n",
    "        batch_indices[-1] = n\n",
    "    else:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        last_batch_size = n % batch_size\n",
    "        \n",
    "    if batch_particle_connection:\n",
    "        batch_particle_dict = {}\n",
    "        batch_particle_indices = np.arange(particles) + 1\n",
    "        np.random.shuffle(batch_particle_indices)\n",
    "        if particles == num_batches:\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "        elif particles > num_batches:\n",
    "            base_batches = particles // num_batches\n",
    "            add_batches = particles % num_batches\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "            for i in range(add_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "        elif num_batches > particles:\n",
    "            num_reps = int(np.ceil(num_batches / particles))\n",
    "            particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "    else:\n",
    "        batch_particle_dict = None\n",
    "\n",
    "    model_dict = {}\n",
    "    weights_dict = {}\n",
    "    y_pred_dict = {}\n",
    "    jacobian_dict = {}\n",
    "    weights_vector_dict = {}\n",
    "    train_mse_dict = {}\n",
    "    test_mse_dict = {}\n",
    "    \n",
    "    if batch_evaluation[\"mean_model\"]:\n",
    "        train_batch_mse_mean_dict = {}\n",
    "    if batch_evaluation[\"particles\"]:\n",
    "        train_batch_mse_particle_dict = {}\n",
    "\n",
    "    # init_model already has weights and biases following the Glorot distribution\n",
    "    # it can already be used to predict and evaluate, but it is very bad\n",
    "    # only used to determine shapes and shape_elements via its weights\n",
    "    init_model = nn_model_structure(layers = layers,\n",
    "                                    neurons = neurons,\n",
    "                                    n_cols = n_cols,\n",
    "                                    classification = False)\n",
    "    init_model = nn_model_compile(init_model,\n",
    "                                  optimizer = \"sgd\")\n",
    "    weights = init_model.get_weights()\n",
    "    # shape contains the shapes of the weight matrices and bias vectors as a list of arrays\n",
    "    shapes = [np.array(params.shape) for params in weights]\n",
    "    # shape_elements contains the indices of the weights as a vector and tells where to cut\n",
    "    shape_elements = np.cumsum([0] + [np.prod(shape) for shape in shapes])\n",
    "\n",
    "    for i in range(particles):\n",
    "        \n",
    "        # just an initial model with the correct structure regarding neurons, layers, activation functions, Glorot initialization\n",
    "        model = nn_model_structure(layers = layers,\n",
    "                                   neurons = neurons,\n",
    "                                   n_cols = n_cols,\n",
    "                                   classification = False)\n",
    "        model = nn_model_compile(model,\n",
    "                                 optimizer = \"sgd\")\n",
    "        # for every particle write the model in a dictionary\n",
    "        model_dict[\"model_{}\".format(str(i+1))] = model\n",
    "\n",
    "        # for every particles write the weights and biases in a dictionary\n",
    "        weights_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .get_weights()\n",
    "        \n",
    "        # for every particle write the predictions on the training batches in a dictionary\n",
    "        y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .predict(X_train[:batch_size,:])\n",
    "\n",
    "        # for every particle write the Jacobian in a dictionary\n",
    "        jacobian_dict[\"model_{}\".format(str(i+1))] = 1/len(y_train[:batch_size]) * (-2)*(y_train[:batch_size] - y_pred_dict[\"model_{}\".format(str(i+1))].ravel())\n",
    "\n",
    "        train_mse_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        test_mse_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        \n",
    "        if batch_evaluation[\"particles\"]:\n",
    "            train_batch_mse_particle_dict[\"particle_{}\".format(str(i+1))] = {}\n",
    "\n",
    "    # mean_model as the model with the mean of the weights of all particle models\n",
    "    mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "    mean_model = init_model\n",
    "    mean_model.set_weights(mean_weights)\n",
    "\n",
    "    mean_model_train_mse = np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1])\n",
    "    mean_model_test_mse = np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1])\n",
    "    if tik_regularize:\n",
    "        mean_weights_raveled = [arr.ravel() for arr in mean_weights]\n",
    "        mean_model_train_mse_reg = mean_model_train_mse + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2)\n",
    "        mean_model_test_mse_reg = mean_model_test_mse + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2)\n",
    "\n",
    "    # loop over all epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # early stopping\n",
    "        if early_stopping:\n",
    "            if epoch == 0:\n",
    "                train_mse_old = 0\n",
    "                test_mse_old = 0\n",
    "            else:\n",
    "                train_mse_new = mean_model_train_mse[epoch]\n",
    "                test_mse_new = mean_model_test_mse[epoch]\n",
    "                if np.absolute(test_mse_new - test_mse_old) <= early_stopping_diff and np.absolute(train_mse_new - train_mse_old) <= early_stopping_diff:\n",
    "                    print(\"STOP: Early Stopping after epoch {} because improvement in training MSE is only {} and in test mse only {}.\"\\\n",
    "                                                                         .format(epoch, train_mse_new - train_mse_old, test_mse_new - test_mse_old))\n",
    "                    break\n",
    "                test_mse_old = test_mse_new\n",
    "                                                                            \n",
    "        # Tikhonov regularization stopping\n",
    "        if tik_regularize and reg_mse_stop:\n",
    "            if epoch >= 1:\n",
    "                if mean_model_train_mse_reg[epoch] > mean_model_train_mse_reg[epoch-1] and mean_model_test_mse_reg[epoch] > mean_model_test_mse_reg[epoch-1]:\n",
    "                    print(\"Training and test MSEs containing Tikhonov regularization start to rise. Algorithm is stopped after epoch {}.\".format(epoch))\n",
    "                    break\n",
    "                \n",
    "        # shuffle the data\n",
    "        if shuffle:\n",
    "            indices = y_train.sample(frac=1).index\n",
    "        else:\n",
    "            indices = y_train.index\n",
    "            \n",
    "        if disjoint_batch:\n",
    "            X_batches = [np.array(X_train)[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [y_train.iloc[indices].reset_index(drop = True)[int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [np.array(i) for i in y_batches]\n",
    "        else:\n",
    "            if last_batch_size != 0:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches-1)]\n",
    "                indices.append(np.random.choice(len(X_train), size = last_batch_size, replace = True))\n",
    "            else:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches)]\n",
    "            X_batches = [X_train[indices[i]] for i in range(len(indices))]\n",
    "            y_batches = [y_train[indices[i]] for i in range(len(indices))]\n",
    "            \n",
    "        if batch_particle_connection and batch_particle_shuffle == \"batch\":\n",
    "            shuffled_indices = np.hstack(list(batch_particle_dict.values()))\n",
    "            np.random.shuffle(shuffled_indices)\n",
    "            batch_particle_values = list(batch_particle_dict.values())\n",
    "            for i in range(len(batch_particle_values)):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = shuffled_indices[i*len(batch_particle_values[i]):(i+1)*len(batch_particle_values[i])]\n",
    "        elif batch_particle_connection and batch_particle_shuffle == \"full\":\n",
    "            batch_particle_dict = {}\n",
    "            batch_particle_indices = np.arange(particles) + 1\n",
    "            np.random.shuffle(batch_particle_indices)\n",
    "            if particles == num_batches:\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "            elif particles > num_batches:\n",
    "                base_batches = particles // num_batches\n",
    "                add_batches = particles % num_batches\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                    batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "                for i in range(add_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "            elif num_batches > particles:\n",
    "                num_reps = int(np.ceil(num_batches / particles))\n",
    "                particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "            \n",
    "        if batch_evaluation[\"mean_model\"]:\n",
    "            train_batch_mse_mean_dict[\"Epoch_{}\".format(str(epoch+1))] = {}\n",
    "        if batch_evaluation[\"particles\"]:\n",
    "            train_batch_mse_particle_dict[\"particle_{}\".format(str(i+1))][\"Epoch_{}\".format(str(epoch+1))] = {}\n",
    "                    \n",
    "        # loop over all batches\n",
    "        for b in range(num_batches):\n",
    "            for mult_updates in range(mult_updates_batch):\n",
    "                batch_particles = []\n",
    "                y_pred_batch_dict = {}\n",
    "                jacobian_batch_dict = {}\n",
    "                for i in range(particles):\n",
    "                    if batch_particle_connection: \n",
    "                        if num_batches == particles or num_batches > particles:\n",
    "                            if batch_particle_dict[\"batch_{}\".format(str(b+1))] != i+1:\n",
    "                                continue\n",
    "                        else:\n",
    "                            if i+1 not in batch_particle_dict[\"batch_{}\".format(str(b+1))]:\n",
    "                                continue\n",
    "                    if batch_particle_connection:\n",
    "                        batch_particles.append(i+1)\n",
    "                    \n",
    "                    # set new weights for model\n",
    "                    model_dict[\"model_{}\".format(str(i+1))].set_weights(weights_dict[\"model_{}\".format(str(i+1))])\n",
    "\n",
    "                    # for every particle write the predictions on the training batches in a dictionary\n",
    "                    y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                    .predict(X_batches[b])\n",
    "                    y_pred_batch_dict[\"model_{}\".format(i+1)] = y_pred_dict[\"model_{}\".format(i+1)]\n",
    "\n",
    "                    # for every particle write the Jacobian in a dictionary\n",
    "                    jacobian_dict[\"model_{}\".format(str(i+1))] = 1/len(y_batches[b]) * (-2)*(y_batches[b] - y_pred_dict[\"model_{}\".format(str(i+1))].ravel())\n",
    "                    jacobian_batch_dict[\"model_{}\".format(i+1)] = jacobian_dict[\"model_{}\".format(i+1)]\n",
    "\n",
    "                    if batch_evaluation[\"mean_model\"]:\n",
    "                        train_batch_mse_mean_dict[\"Epoch_{}\".format(str(epoch+1))][\"Batch_{}\".format(str(b+1))] = mean_model.evaluate(X_batches[b], y_batches[b], verbose = 0)[1]\n",
    "                    if batch_evaluation[\"particles\"]:\n",
    "                        train_batch_mse_particle_dict[\"particle_{}\".format(str(i+1))][\"Epoch_{}\".format(str(epoch+1))][\"Batch_{}\".format(str(b+1))] = model_dict[\"model_{}\".format(str(i+1))].evaluate(X_batches[b], y_batches[b], verbose = 0)[1]\n",
    "\n",
    "                        \n",
    "                if not batch_particle_connection:        \n",
    "                    # compute the mean of the predictions\n",
    "                    y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "                    # compute the matrix D elementwise\n",
    "                    d = np.zeros(shape = (particles, particles))\n",
    "                    for k in range(particles):\n",
    "                        y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                        for j in range(particles):\n",
    "                            d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_dict[\"model_{}\".format(str(j+1))])\n",
    "                    d = np.transpose(d)\n",
    "\n",
    "                    # compute the scalar h_t\n",
    "                    h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "                    # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "                    for i in range(particles):\n",
    "                        weights_array = np.array([])\n",
    "                        for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                            weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "                    # matrix with particle parameters as row vectors\n",
    "                    weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "                    # compute the matrix with the updates for each particle\n",
    "                    weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "                    for i in range(particles):\n",
    "                        # write the updates back into the dictionary\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                        # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                        for l in range(len(shape_elements)-1):\n",
    "                            start = shape_elements[l]\n",
    "                            end = shape_elements[l+1]\n",
    "                            weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                \n",
    "                elif batch_particle_connection and not update_all:\n",
    "                    # compute the mean of the predictions\n",
    "                    y_pred_mean = np.mean(list(y_pred_batch_dict.values()), axis = 0)\n",
    "                    \n",
    "                    # compute the matrix D elementwise\n",
    "                    d = np.zeros(shape = (len(batch_particles), len(batch_particles)))\n",
    "                    for k in range(len(batch_particles)):\n",
    "                        y_pred_centered = y_pred_batch_dict[\"model_{}\".format(batch_particles[k])] - y_pred_mean\n",
    "                        for j in range(len(batch_particles)):\n",
    "                            d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_batch_dict[\"model_{}\".format(batch_particles[j])])\n",
    "                    d = np.transpose(d)\n",
    "\n",
    "                    # compute the scalar h_t\n",
    "                    h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "                    # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "                    for i in range(particles):\n",
    "                        weights_array = np.array([])\n",
    "                        for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                            weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "                    # matrix with particle parameters as row vectors\n",
    "                    weights_vector_batch_dict = {}\n",
    "                    for i in range(len(batch_particles)):\n",
    "                        weights_vector_batch_dict[\"model_{}\".format(batch_particles[i])] = weights_vector_dict[\"model_{}\".format(batch_particles[i])]\n",
    "                        weights_all_ptcls = np.array(list(weights_vector_batch_dict.values()))\n",
    "\n",
    "                    # compute the matrix with the updates for each particle\n",
    "                    weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "                    for i in range(len(batch_particles)):\n",
    "                        # write the updates back into the dictionary\n",
    "                        weights_vector_dict[\"model_{}\".format(batch_particles[i])] = weights_all_ptcls[i]\n",
    "                        # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                        for l in range(len(shape_elements)-1):\n",
    "                            start = shape_elements[l]\n",
    "                            end = shape_elements[l+1]\n",
    "                            weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "\n",
    "        if batch_particle_connection and update_all:\n",
    "            # compute the mean of the predictions\n",
    "            y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "            # compute the matrix D elementwise\n",
    "            d = np.zeros(shape = (particles, particles))\n",
    "            for k in range(particles):\n",
    "                y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                for j in range(particles):\n",
    "                    d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_dict[\"model_{}\".format(str(j+1))])\n",
    "            d = np.transpose(d)\n",
    "\n",
    "            # compute the scalar h_t\n",
    "            h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "            # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "            for i in range(particles):\n",
    "                weights_array = np.array([])\n",
    "                for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                    weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "            # matrix with particle parameters as row vectors\n",
    "            weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "            # compute the matrix with the updates for each particle\n",
    "            weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "            for i in range(particles):\n",
    "                # write the updates back into the dictionary\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                for l in range(len(shape_elements)-1):\n",
    "                    start = shape_elements[l]\n",
    "                    end = shape_elements[l+1]\n",
    "                    weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                                                            \n",
    "        for i in range(particles):\n",
    "            # for every particle write the training MSE of the current iteration in a dictionary\n",
    "            train_mse_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_train, y_train, verbose = 0)[1])\n",
    "\n",
    "            # for every particle write the test MSE of the current iteration in a dictionary\n",
    "            test_mse_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_test, y_test, verbose = 0)[1])\n",
    "\n",
    "        # update the mean_model\n",
    "        mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "        mean_model.set_weights(mean_weights)\n",
    "\n",
    "        mean_model_train_mse = np.append(mean_model_train_mse, np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1]))\n",
    "        mean_model_test_mse = np.append(mean_model_test_mse, np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1]))\n",
    "        if tik_regularize and reg_mse_stop:\n",
    "            mean_weights_raveled = [arr.ravel() for arr in mean_weights]\n",
    "            mean_model_train_mse_reg = np.append(mean_model_train_mse_reg, mean_model_train_mse[-1] + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2))\n",
    "            mean_model_test_mse_reg = np.append(mean_model_test_mse_reg, mean_model_test_mse[-1] + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2))\n",
    "        \n",
    "\n",
    "        if verbose == 1:\n",
    "            print(\"Epoch {}. Training MSE: {}, Test MSE: {}.\".format(epoch+1,\n",
    "                                                                     np.round(mean_model_train_mse[-1], 3),\n",
    "                                                                     np.round(mean_model_test_mse[-1], 3)))\n",
    "            \n",
    "    mean_model.history.history = {\"mse\": mean_model_train_mse[1:],\n",
    "                                  \"val_mse\": mean_model_test_mse[1:]}\n",
    "        \n",
    "    if save_all:\n",
    "        param_dict = param_to_dict(X_train,\n",
    "                                   X_test,\n",
    "                                   y_train,\n",
    "                                   y_test,\n",
    "                                   layers,\n",
    "                                   neurons,\n",
    "                                   particles,\n",
    "                                   epochs,\n",
    "                                   batch_size,\n",
    "                                   h_0,\n",
    "                                   delta,\n",
    "                                   epsilon,\n",
    "                                   randomization,\n",
    "                                   shuffle,\n",
    "                                   early_stopping,\n",
    "                                   early_stopping_diff\n",
    "                                   )\n",
    "        results_dict = results_to_dict(mean_model_train_mse,\n",
    "                                       mean_model_test_mse,\n",
    "                                       train_mse_dict,\n",
    "                                       test_mse_dict,\n",
    "                                       weights_dict,\n",
    "                                       y_pred_dict,\n",
    "                                       False\n",
    "                                       )\n",
    "\n",
    "        saving_dict = {}\n",
    "        saving_dict[\"parameters\"] = param_dict\n",
    "        saving_dict[\"results\"] = results_dict\n",
    "        saving_dict[\"analysis\"] = analysis_dict\n",
    "\n",
    "        save_objects(obj_dict = saving_dict,\n",
    "                     file = file_var)\n",
    "\n",
    "        nn_save(model = mean_model,\n",
    "                path_name = file_model)\n",
    "        \n",
    "        if batch_evaluation[\"mean_model\"]:\n",
    "            full_dict = {}\n",
    "            full_dict[\"mean_model\"] = train_batch_mse_mean_dict\n",
    "            if batch_evaluation[\"particles\"]:\n",
    "                full_dict[\"particles\"] = train_batch_mse_particle_dict\n",
    "            save_objects(obj_dict = full_dict,\n",
    "                         file = batch_evaluation[\"file\"])\n",
    "        elif batch_evaluation[\"particles\"]:\n",
    "            full_dict = {}\n",
    "            full_dict[\"particles\"] = train_batch_mse_particle_dict\n",
    "            if batch_evaluation[\"mean_model\"]:\n",
    "                full_dict[\"mean_model\"] = train_batch_mse_mean_dict\n",
    "            save_objects(obj_dict = full_dict,\n",
    "                         file = batch_evaluation[\"file\"])\n",
    " \n",
    "    return mean_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enkf_regressor_analysis(X_train,\n",
    "                            X_test,\n",
    "                            y_train,\n",
    "                            y_test,\n",
    "                            layers,\n",
    "                            neurons,\n",
    "                            setting_dict,\n",
    "                            analysis_dict,\n",
    "                            save_all = False,\n",
    "                            file_var = \"file.pckl\",\n",
    "                            file_model = \"file.h5\",\n",
    "                            verbose = 0\n",
    "                            ):\n",
    "\n",
    "    \"\"\" Ensemble Kalman Filter algorithm analysis for regression problems.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train (np.ndarray): Training data X.\n",
    "    X_test (np.ndarray): Test data X.\n",
    "    y_train (pd.DataFrame): Training data y.\n",
    "    y_test (pd.DataFrame): Test data y.\n",
    "    layers (int): Number of layers.\n",
    "    neurons (list): Number of neurons in each layer.\n",
    "    setting_dict (dict): Dictionary containing\n",
    "        particles (int): Number of particles in the ensemble.\n",
    "        epochs (int): Number of epochs.\n",
    "        batch_size (None or int): Size of the batches. Must be between 0 and the number of observations in the training set.\n",
    "        h_0 (int or float): Starting step size.\n",
    "        delta (float): Constant for numerical stability in the jacobian.\n",
    "        epsilon (float): Constant for numerical stability in the step size.\n",
    "        shuffle (bool): Whether or not to shuffle the data prior to each epoch.\n",
    "        early_stopping (float or None): Minimum change before early stopping is applied.\n",
    "    analysis_dict (dict): Dictionary containing\n",
    "        disjoint_batch (bool): Whether or not to use disjoint batches. If False then each batch is sampled with replacement.\n",
    "        batch_particle_connection (dict): Dictionary containing\n",
    "            connect (bool): Whether or not to connect particles and batches.\n",
    "            shuffle (str or None): Whether or not and how to shuffle the connection. None = no shuffle. \"permute\" = change the allocation of the existing batches and particle sets. \"particle\" = shuffle the particle sets for fixed batches. \"batch\" = shuffle the batch for fixed particle sets. \"full\" = shuffle the particle sets and their corresponding batch.\n",
    "        tikhonov (dict): Dictionary containing\n",
    "            regularize (bool): Whether or not to use Tikhonov regularization.\n",
    "            lambda (None or float): Lambda parameter in Tikhonov regularization. \n",
    "    save_all (bool): Whether or not to save all important variables and models.\n",
    "    file_var (str): Path and name of the file to save variables into.\n",
    "    file_model (str): Path and name of the file to save the final model into.\n",
    "    verbose (int): If 0, then don't print anything throughout the training process. If 1, then print training and test accuracy after each epoch.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    mean_model (tensorflow.python.keras.engine.sequential.Sequential): The final model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    particles = setting_dict[\"particles\"]\n",
    "    epochs = setting_dict[\"epochs\"]\n",
    "    batch_size = setting_dict[\"batch_size\"]\n",
    "    h_0 = setting_dict[\"h_0\"]\n",
    "    delta = None\n",
    "    epsilon = setting_dict[\"epsilon\"]\n",
    "    shuffle = setting_dict[\"shuffle\"]\n",
    "    early_stopping = setting_dict[\"early_stopping\"]\n",
    "    \n",
    "    disjoint_batch = analysis_dict[\"disjoint_batch\"]\n",
    "    batch_particle_connection = analysis_dict[\"batch_particle_connection\"][\"connect\"]\n",
    "    batch_particle_shuffle = analysis_dict[\"batch_particle_connection\"][\"shuffle\"]\n",
    "    tik_regularize = analysis_dict[\"tikhonov\"][\"regularize\"]\n",
    "    tik_lambda = analysis_dict[\"tikhonov\"][\"lambda\"]\n",
    "    if not tik_regularize:\n",
    "        tik_lambda = None\n",
    "    if tik_regularize and tik_lambda is None:\n",
    "        tik_lambda = 0\n",
    "        \n",
    "    def regularize_pred(y_pred, weights):\n",
    "        if tik_regularize and tik_lambda != 0:\n",
    "            return np.hstack([y_pred.ravel(), np.sqrt(tik_lambda) * weights])\n",
    "        else:\n",
    "            return y_pred.ravel()\n",
    "    \n",
    "    def regularize_true(y_true, weights):\n",
    "        if tik_regularize and tik_lambda != 0:\n",
    "            return np.hstack([y_true, np.zeros(shape = weights.shape)])\n",
    "        else:\n",
    "            return y_true.ravel()\n",
    "    \n",
    "    if batch_size == None:\n",
    "        batch_size = len(X_train)\n",
    "\n",
    "    n_cols = X_train.shape[1]\n",
    "    \n",
    "    if disjoint_batch:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        batch_indices = np.cumsum([0] + list(np.ones(num_batches) * batch_size))\n",
    "        batch_indices[-1] = n\n",
    "    else:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        last_batch_size = n % batch_size\n",
    "     \n",
    "    if batch_particle_connection:\n",
    "        batch_particle_dict = {}\n",
    "        batch_particle_indices = np.arange(particles) + 1\n",
    "        np.random.shuffle(batch_particle_indices)\n",
    "        if particles == num_batches:\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "        elif particles > num_batches:\n",
    "            base_batches = particles // num_batches\n",
    "            add_batches = particles % num_batches\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "            for i in range(add_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "        elif num_batches > particles:\n",
    "            num_reps = int(np.ceil(num_batches / particles))\n",
    "            particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "    else:\n",
    "        batch_particle_dict = None\n",
    "\n",
    "    model_dict = {}\n",
    "    weights_dict = {}\n",
    "    y_pred_dict = {}\n",
    "    jacobian_dict = {}\n",
    "    weights_vector_dict = {}\n",
    "    train_mse_dict = {}\n",
    "    test_mse_dict = {}\n",
    "\n",
    "    # init_model already has weights and biases following the Glorot distribution\n",
    "    # it can already be used to predict and evaluate, but it is very bad\n",
    "    # only used to determine shapes and shape_elements via its weights\n",
    "    init_model = nn_model_structure(layers = layers,\n",
    "                                    neurons = neurons,\n",
    "                                    n_cols = n_cols,\n",
    "                                    kernel_regularizer_lambda = tik_lambda,\n",
    "                                    bias_regularizer_lambda = tik_lambda,\n",
    "                                    classification = False)\n",
    "    init_model = nn_model_compile(init_model,\n",
    "                                  optimizer = \"sgd\")\n",
    "    weights = init_model.get_weights()\n",
    "    # shape contains the shapes of the weight matrices and bias vectors as a list of arrays\n",
    "    shapes = [np.array(params.shape) for params in weights]\n",
    "    # shape_elements contains the indices of the weights as a vector and tells where to cut\n",
    "    shape_elements = np.cumsum([0] + [np.prod(shape) for shape in shapes])\n",
    "\n",
    "    for i in range(particles):\n",
    "        \n",
    "        # just an initial model with the correct structure regarding neurons, layers, activation functions, Glorot initialization, Tikhonov regularization\n",
    "        model = nn_model_structure(layers = layers,\n",
    "                                   neurons = neurons,\n",
    "                                   n_cols = n_cols,\n",
    "                                   kernel_regularizer_lambda = tik_lambda,\n",
    "                                   bias_regularizer_lambda = tik_lambda,\n",
    "                                   classification = False)\n",
    "        model = nn_model_compile(model,\n",
    "                                 optimizer = \"sgd\")\n",
    "        # for every particle write the model in a dictionary\n",
    "        model_dict[\"model_{}\".format(str(i+1))] = model\n",
    "\n",
    "        # for every particles write the weights and biases in a dictionary\n",
    "        weights_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .get_weights()\n",
    "        \n",
    "        # for every particle write the predictions on the training batches in a dictionary\n",
    "        weights_array_tik = np.array([])\n",
    "        for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "            weights_array_tik = np.append(weights_array_tik, weights_dict[\"model_{}\".format(str(i+1))][j].ravel())        \n",
    "        y_pred_dict[\"model_{}\".format(str(i+1))] = regularize_pred(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                        .predict(X_train[:batch_size,:]),\n",
    "                                                                   weights_array_tik)\n",
    "\n",
    "        # for every particle write the Jacobian in a dictionary\n",
    "        jacobian_dict[\"model_{}\".format(str(i+1))] = 1/len(regularize_true(y_train[:batch_size], weights_array_tik)) * (-2)*(regularize_true(y_train[:batch_size], weights_array_tik) - y_pred_dict[\"model_{}\".format(str(i+1))].ravel())\n",
    "\n",
    "        train_mse_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        test_mse_dict[\"model_{}\".format(str(i+1))] = []\n",
    "\n",
    "    # mean_model as the model with the mean of the weights of all particle models\n",
    "    mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "    mean_model = init_model\n",
    "    mean_model.set_weights(mean_weights)\n",
    "\n",
    "    mean_model_train_mse = np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1])\n",
    "    mean_model_test_mse = np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1])\n",
    "\n",
    "    # loop over all epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # early stopping\n",
    "        if early_stopping is not None:\n",
    "            if epoch == 0:\n",
    "                train_mse_old = 0\n",
    "                test_mse_old = 0\n",
    "            else:\n",
    "                train_mse_new = mean_model_train_mse[epoch]\n",
    "                test_mse_new = mean_model_test_mse[epoch]\n",
    "                if np.absolute(test_mse_new - test_mse_old) <= early_stopping and np.absolute(train_mse_new - train_mse_old) <= early_stopping:\n",
    "                    print(\"STOP: Early Stopping after epoch {} because improvement in training MSE is only {} and in test mse only {}.\"\\\n",
    "                                                                         .format(epoch, train_mse_new - train_mse_old, test_mse_new - test_mse_old))\n",
    "                    break\n",
    "                test_mse_old = test_mse_new\n",
    "                                                                                    \n",
    "        # shuffle the data\n",
    "        if shuffle:\n",
    "            indices = y_train.sample(frac=1).index\n",
    "        else:\n",
    "            indices = y_train.index\n",
    "            \n",
    "        if disjoint_batch:\n",
    "            X_batches = [np.array(X_train)[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [y_train.iloc[indices].reset_index(drop = True)[int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [np.array(i) for i in y_batches]\n",
    "        else:\n",
    "            if last_batch_size != 0:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches-1)]\n",
    "                indices.append(np.random.choice(len(X_train), size = last_batch_size, replace = True))\n",
    "            else:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches)]\n",
    "            X_batches = [X_train[indices[i]] for i in range(len(indices))]\n",
    "            y_batches = [y_train[indices[i]] for i in range(len(indices))]\n",
    "        \n",
    "        # shuffling for batch particle connection\n",
    "        if batch_particle_connection and batch_particle_shuffle == \"permute\":\n",
    "            shuffled_indices = np.hstack(list(batch_particle_dict.values()))\n",
    "            np.random.shuffle(shuffled_indices)\n",
    "            batch_particle_values = list(batch_particle_dict.values())\n",
    "            for i in range(len(batch_particle_values)):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = shuffled_indices[i*len(batch_particle_values[i]):(i+1)*len(batch_particle_values[i])]\n",
    "        if batch_particle_connection and (batch_particle_shuffle == \"particle\" or batch_particle_shuffle == \"full\"):\n",
    "            batch_particle_dict = {}\n",
    "            batch_particle_indices = np.arange(particles) + 1\n",
    "            np.random.shuffle(batch_particle_indices)\n",
    "            if particles == num_batches:\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "            elif particles > num_batches:\n",
    "                base_batches = particles // num_batches\n",
    "                add_batches = particles % num_batches\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                    batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "                for i in range(add_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "            elif num_batches > particles:\n",
    "                num_reps = int(np.ceil(num_batches / particles))\n",
    "                particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "        if batch_particle_connection and (batch_particle_shuffle == \"batch\" or batch_particle_shuffle == \"full\"):\n",
    "            indices = np.arange(n)\n",
    "            np.random.shuffle(indices)\n",
    "            if disjoint_batch:\n",
    "                X_batches = [X_train[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "                y_batches = [y_train[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            else:\n",
    "                if last_batch_size != 0:\n",
    "                    indices = [np.random.choice(X_train.shape[0], size = batch_size, replace = True) for i in range(num_batches-1)]\n",
    "                    indices.append(np.random.choice(X_train.shape[0], size = last_batch_size, replace = True))\n",
    "                else:\n",
    "                    indices = [np.random.choice(X_train.shape[0], size = batch_size, replace = True) for i in range(num_batches)]\n",
    "                X_batches = [X_train[indices[i]] for i in range(len(indices))]\n",
    "                y_batches = [y_train[indices[i]] for i in range(len(indices))]\n",
    "               \n",
    "        # loop over all batches\n",
    "        for b in range(num_batches):\n",
    "            batch_particles = []\n",
    "            for i in range(particles):\n",
    "                if batch_particle_connection: \n",
    "                    if num_batches == particles or num_batches > particles:\n",
    "                        if batch_particle_dict[\"batch_{}\".format(str(b+1))] != i+1:\n",
    "                            continue\n",
    "                    else:\n",
    "                        if i+1 not in batch_particle_dict[\"batch_{}\".format(str(b+1))]:\n",
    "                            continue\n",
    "                if batch_particle_connection:\n",
    "                    batch_particles.append(i+1)\n",
    "\n",
    "                # set new weights for model\n",
    "                model_dict[\"model_{}\".format(str(i+1))].set_weights(weights_dict[\"model_{}\".format(str(i+1))])\n",
    "\n",
    "                # for every particle write the predictions on the training batches in a dictionary\n",
    "                weights_array_tik = np.array([])\n",
    "                for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                    weights_array_tik = np.append(weights_array_tik, weights_dict[\"model_{}\".format(str(i+1))][j].ravel())\n",
    "                y_pred_dict[\"model_{}\".format(str(i+1))] = regularize_pred(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                                .predict(X_batches[b]),\n",
    "                                                                           weights_array_tik)\n",
    "                \n",
    "                # for every particle write the Jacobian in a dictionary\n",
    "                jacobian_dict[\"model_{}\".format(str(i+1))] = 1/len(regularize_true(y_batches[b], weights_array_tik)) * (-2)*(regularize_true(y_batches[b], weights_array_tik) - y_pred_dict[\"model_{}\".format(str(i+1))].ravel())\n",
    "\n",
    "            if not batch_particle_connection:        \n",
    "                # compute the mean of the predictions\n",
    "                y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "                # compute the matrix D elementwise\n",
    "                d = np.zeros(shape = (particles, particles))\n",
    "                for k in range(particles):\n",
    "                    y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                    for j in range(particles):\n",
    "                        d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_dict[\"model_{}\".format(str(j+1))])\n",
    "                d = np.transpose(d)\n",
    "\n",
    "                # compute the scalar h_t\n",
    "                h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "                # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "                for i in range(particles):\n",
    "                    weights_array = np.array([])\n",
    "                    for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                        weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                    weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "                # matrix with particle parameters as row vectors\n",
    "                weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "                # compute the matrix with the updates for each particle\n",
    "                weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "                for i in range(particles):\n",
    "                    # write the updates back into the dictionary\n",
    "                    weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                    # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                    for l in range(len(shape_elements)-1):\n",
    "                        start = shape_elements[l]\n",
    "                        end = shape_elements[l+1]\n",
    "                        weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                \n",
    "        if batch_particle_connection:\n",
    "            # compute the mean of the predictions\n",
    "            y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "            # compute the matrix D elementwise\n",
    "            d = np.zeros(shape = (particles, particles))\n",
    "            for k in range(particles):\n",
    "                y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                for j in range(particles):\n",
    "                    d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_dict[\"model_{}\".format(str(j+1))])\n",
    "            d = np.transpose(d)\n",
    "\n",
    "            # compute the scalar h_t\n",
    "            h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "            # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "            for i in range(particles):\n",
    "                weights_array = np.array([])\n",
    "                for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                    weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "            # matrix with particle parameters as row vectors\n",
    "            weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "            # compute the matrix with the updates for each particle\n",
    "            weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "            for i in range(particles):\n",
    "                # write the updates back into the dictionary\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                for l in range(len(shape_elements)-1):\n",
    "                    start = shape_elements[l]\n",
    "                    end = shape_elements[l+1]\n",
    "                    weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                                                            \n",
    "        for i in range(particles):\n",
    "            # for every particle write the training MSE of the current iteration in a dictionary\n",
    "            train_mse_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_train, y_train, verbose = 0)[1])\n",
    "\n",
    "            # for every particle write the test MSE of the current iteration in a dictionary\n",
    "            test_mse_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_test, y_test, verbose = 0)[1])\n",
    "\n",
    "        # update the mean_model\n",
    "        mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "        mean_model.set_weights(mean_weights)\n",
    "\n",
    "        mean_model_train_mse = np.append(mean_model_train_mse, np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1]))\n",
    "        mean_model_test_mse = np.append(mean_model_test_mse, np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1])) \n",
    "\n",
    "        if verbose == 1:\n",
    "            print(\"Epoch {}. Training MSE: {}, Test MSE: {}.\".format(epoch+1,\n",
    "                                                                     np.round(mean_model_train_mse[-1], 3),\n",
    "                                                                     np.round(mean_model_test_mse[-1], 3)))\n",
    "            \n",
    "    mean_model.history.history = {\"mse\": mean_model_train_mse[1:],\n",
    "                                  \"val_mse\": mean_model_test_mse[1:]}\n",
    "        \n",
    "    if save_all:\n",
    "        param_dict = param_to_dict(X_train,\n",
    "                                   X_test,\n",
    "                                   y_train,\n",
    "                                   y_test,\n",
    "                                   layers,\n",
    "                                   neurons,\n",
    "                                   particles,\n",
    "                                   epochs,\n",
    "                                   batch_size,\n",
    "                                   h_0,\n",
    "                                   delta,\n",
    "                                   epsilon,\n",
    "                                   shuffle,\n",
    "                                   early_stopping\n",
    "                                   )\n",
    "        results_dict = results_to_dict(mean_model_train_mse,\n",
    "                                       mean_model_test_mse,\n",
    "                                       train_mse_dict,\n",
    "                                       test_mse_dict,\n",
    "                                       weights_dict,\n",
    "                                       y_pred_dict,\n",
    "                                       False\n",
    "                                       )\n",
    "\n",
    "        saving_dict = {}\n",
    "        saving_dict[\"parameters\"] = param_dict\n",
    "        saving_dict[\"results\"] = results_dict\n",
    "        saving_dict[\"analysis\"] = analysis_dict\n",
    "\n",
    "        save_objects(obj_dict = saving_dict,\n",
    "                     file = file_var)\n",
    "\n",
    "        nn_save(model = mean_model,\n",
    "                path_name = file_model)\n",
    " \n",
    "    return mean_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting_dict = {\"particles\": 100,\n",
    "                \"epochs\": 20,\n",
    "                \"batch_size\": len(X_train),    # len(X_train)\n",
    "                \"h_0\": 2,\n",
    "                \"epsilon\": 0.5,\n",
    "                \"shuffle\": True,\n",
    "                \"early_stopping\": None\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dict = {\"disjoint_batch\": True,\n",
    "                 \"batch_particle_connection\": {\"connect\": False,\n",
    "                                               \"shuffle\": \"particle\"},        # None, \"permute\", \"particle\", \"batch\", \"full\"\n",
    "                 \"tikhonov\": {\"regularize\": True,\n",
    "                              \"lambda\": 1}\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training MSE: 18.367, Test MSE: 18.323.\n",
      "Epoch 2. Training MSE: 9.559, Test MSE: 9.548.\n",
      "Epoch 3. Training MSE: 5.85, Test MSE: 5.859.\n",
      "Epoch 4. Training MSE: 3.75, Test MSE: 3.779.\n",
      "Epoch 5. Training MSE: 2.592, Test MSE: 2.617.\n",
      "Epoch 6. Training MSE: 1.704, Test MSE: 1.73.\n",
      "Epoch 7. Training MSE: 1.296, Test MSE: 1.318.\n",
      "Epoch 8. Training MSE: 1.199, Test MSE: 1.226.\n",
      "Epoch 9. Training MSE: 1.141, Test MSE: 1.162.\n",
      "Epoch 10. Training MSE: 1.115, Test MSE: 1.136.\n"
     ]
    }
   ],
   "source": [
    "reg_model = enkf_regressor_analysis(X_train,\n",
    "                                    X_test,\n",
    "                                    y_train,\n",
    "                                    y_test,\n",
    "                                    layers,\n",
    "                                    neurons,\n",
    "                                    setting_dict,\n",
    "                                    analysis_dict,\n",
    "                                    save_all = False,\n",
    "                                    file_var = \"../objects/wine/wine_enkf_E{}_B{}_P{}_H{}_Reg.pckl\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                    file_model = \"../models/wine/wine_enkf_E{}_B{}_P{}_H{}_Reg.h5\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_plot_mse(reg_model,\n",
    "            mse_mean = None,\n",
    "            start_epoch = 1,\n",
    "            save = None)#\"../img/wine/wine_enkf_E{}_B{}_P{}_H{}_Reg.png\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting_dict = {\"particles\": 181,\n",
    "                \"epochs\": 20,\n",
    "                \"batch_size\": 23,    # len(X_train)\n",
    "                \"h_0\": 2,\n",
    "                \"epsilon\": 0.5,\n",
    "                \"shuffle\": True,\n",
    "                \"early_stopping\": None\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dict = {\"disjoint_batch\": True,\n",
    "                 \"batch_particle_connection\": {\"connect\": True,\n",
    "                                               \"shuffle\": \"particle\"},        # None, \"permute\", \"particle\", \"batch\", \"full\"\n",
    "                 \"tikhonov\": {\"regularize\": False,\n",
    "                              \"lambda\": 1}\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpc_model = enkf_regressor_analysis(X_train,\n",
    "                                    X_test,\n",
    "                                    y_train,\n",
    "                                    y_test,\n",
    "                                    layers,\n",
    "                                    neurons,\n",
    "                                    setting_dict,\n",
    "                                    analysis_dict,\n",
    "                                    save_all = False,\n",
    "                                    file_var = \"../objects/wine/wine_enkf_E{}_B{}_P{}_H{}_BPC.pckl\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                    file_model = \"../models/wine/wine_enkf_E{}_B{}_P{}_H{}_BPC.h5\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_plot_mse(bpc_model,\n",
    "            mse_mean = None,\n",
    "            start_epoch = 1,\n",
    "            save = None)#\"../img/wine/wine_enkf_E{}_B{}_P{}_H{}_BPC.png\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting_dict = {\"particles\": 181,\n",
    "                \"epochs\": 20,\n",
    "                \"batch_size\": len(X_train),    # len(X_train)\n",
    "                \"h_0\": 2,\n",
    "                \"epsilon\": 0.5,\n",
    "                \"shuffle\": True,\n",
    "                \"early_stopping\": None\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dict = {\"disjoint_batch\": True,\n",
    "                 \"batch_particle_connection\": {\"connect\": True,\n",
    "                                               \"shuffle\": \"particle\"},        # None, \"permute\", \"particle\", \"batch\", \"full\"\n",
    "                 \"tikhonov\": {\"regularize\": False,\n",
    "                              \"lambda\": 1}\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eki_model = enkf_regressor_analysis(X_train,\n",
    "                                    X_test,\n",
    "                                    y_train,\n",
    "                                    y_test,\n",
    "                                    layers,\n",
    "                                    neurons,\n",
    "                                    setting_dict,\n",
    "                                    analysis_dict,\n",
    "                                    save_all = False,\n",
    "                                    file_var = \"../objects/wine/wine_enkf_E{}_B{}_P{}_H{}.pckl\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                    file_model = \"../models/wine/wine_enkf_E{}_B{}_P{}_H{}.h5\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_plot_mse(eki_model,\n",
    "            mse_mean = None,\n",
    "            start_epoch = 1,\n",
    "            save = None)#\"../img/wine/wine_enkf_E{}_B{}_P{}_H{}.png\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
