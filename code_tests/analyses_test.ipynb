{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../python/functions\")\n",
    "sys.path.insert(2, \"../python/architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../python/architecture\\reproducible.py:14: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_prep_functions import wine_prep\n",
    "from plotting_functions import nn_plot_mse\n",
    "from enkf_functions import enkf_regressor\n",
    "from saving_functions import load_objects, save_objects\n",
    "from model_functions import nn_load, nn_model_structure, nn_model_compile\n",
    "import reproducible\n",
    "import no_gpu\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = wine_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use samller dataset for increased speed\n",
    "X_train = X_train[:1000]\n",
    "X_test = X_test[:500]\n",
    "y_train = y_train[:1000]\n",
    "y_test = y_test[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 4\n",
    "neurons = [32, 32, 16, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enkf_regressor_analysis(X_train,\n",
    "                            X_test,\n",
    "                            y_train,\n",
    "                            y_test,\n",
    "                            layers,\n",
    "                            neurons,\n",
    "                            setting_dict,\n",
    "                            analysis_dict,\n",
    "                            save_all = False,\n",
    "                            file_var = \"file.pckl\",\n",
    "                            file_model = \"file.h5\",\n",
    "                            verbose = 0\n",
    "                            ):\n",
    "\n",
    "    \"\"\" Ensemble Kalman Filter algorithm analysis for regression problems.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train (np.ndarray): Training data X.\n",
    "    X_test (np.ndarray): Test data X.\n",
    "    y_train (pd.DataFrame): Training data y.\n",
    "    y_test (pd.DataFrame): Test data y.\n",
    "    layers (int): Number of layers.\n",
    "    neurons (list): Number of neurons in each layer.\n",
    "    setting_dict (dict): Dictionary containing\n",
    "        particles (int): Number of particles in the ensemble.\n",
    "        epochs (int): Number of epochs.\n",
    "        batch_size (None or int): Size of the batches. Must be between 0 and the number of observations in the training set.\n",
    "        h_0 (int or float): Starting step size.\n",
    "        delta (float): Constant for numerical stability in the jacobian.\n",
    "        epsilon (float): Constant for numerical stability in the step size.\n",
    "        randomization (bool): Whether or not to add noise to the particles and randomize them around their mean.\n",
    "        shuffle (bool): Whether or not to shuffle the data prior to each epoch.\n",
    "        early_stopping (bool): Whether or not to stop the calculation when the changes get small.\n",
    "        early_stopping_diff (bool): Minimum change before early stopping is applied.\n",
    "    analysis_dict (dict): Dictionary containing\n",
    "        disjoint_batch (bool): Whether or not to use disjoint batches. If False then each batch is sampled with replacement.\n",
    "        multiple_updates_same_batch (int or None): Number of consecutive updates on the same batch.\n",
    "        batch_particle_connection (dict): Dictionary containing\n",
    "            connect (bool): Whether or not to connect particles and batches.\n",
    "            shuffle (str or None): Whether or not and how to shuffle the connection. None = no shuffle. \"batch\" = shuffle the batch for fixed particle sets. \"full\" = shuffle the particle sets and their corresponding batch.\n",
    "            update_all (bool): Whether or not to update after all particles have seen some data.\n",
    "        batch_evaluation (dict): Dictionary containing\n",
    "            mean_model (bool): Whether or not evaluate the mean model after every batch on that batch. Warning: the computational costs are very high.\n",
    "            particles (bool): Whether or not evaluate every particle model after every batch on that batch. Warning: the computational costs are incredibly high.\n",
    "            file (str): Path and name of the file to save evaluations into \n",
    "        tikhonov (dict): Dictionary containing\n",
    "            regularize (bool): Whether or not to use Tikhonov regularization.\n",
    "            lambda (None or float): Lambda parameter in Tikhonov regularization.\n",
    "            reg_mse_stop (bool): Whether or not to stop when MSE + Tikhonov regularization starts to rise again.\n",
    "    save_all (bool): Whether or not to save all important variables and models.\n",
    "    file_var (str): Path and name of the file to save variables into.\n",
    "    file_model (str): Path and name of the file to save the final model into.\n",
    "    verbose (int): If 0, then don't print anything throughout the training process. If 1, then print training and test accuracy after each epoch.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    mean_model (tensorflow.python.keras.engine.sequential.Sequential): The final model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    particles = setting_dict[\"particles\"]\n",
    "    epochs = setting_dict[\"epochs\"]\n",
    "    batch_size = setting_dict[\"batch_size\"]\n",
    "    h_0 = setting_dict[\"h_0\"]\n",
    "    delta = None\n",
    "    epsilon = setting_dict[\"epsilon\"]\n",
    "    randomization = setting_dict[\"randomization\"]\n",
    "    shuffle = setting_dict[\"shuffle\"]\n",
    "    early_stopping = setting_dict[\"early_stopping\"]\n",
    "    early_stopping_diff = setting_dict[\"early_stopping_diff\"]\n",
    "    \n",
    "    disjoint_batch = analysis_dict[\"disjoint_batch\"]\n",
    "    batch_evaluation = analysis_dict[\"batch_evaluation\"]\n",
    "    batch_particle_connection = analysis_dict[\"batch_particle_connection\"][\"connect\"]\n",
    "    batch_particle_shuffle = analysis_dict[\"batch_particle_connection\"][\"shuffle\"]\n",
    "    update_all = analysis_dict[\"batch_particle_connection\"][\"update_all\"]\n",
    "    mult_updates_batch = analysis_dict[\"multiple_updates_same_batch\"]\n",
    "    tik_regularize = analysis_dict[\"tikhonov\"][\"regularize\"]\n",
    "    tik_lambda = analysis_dict[\"tikhonov\"][\"lambda\"]\n",
    "    reg_mse_stop = analysis_dict[\"tikhonov\"][\"reg_mse_stop\"]\n",
    "    \n",
    "    if tik_lambda is None:\n",
    "        tik_lambda = 0\n",
    "    \n",
    "    if mult_updates_batch is None:\n",
    "        mult_updates_batch = 1\n",
    "\n",
    "    if batch_size == None:\n",
    "        batch_size = len(X_train)\n",
    "\n",
    "    n_cols = X_train.shape[1]\n",
    "    \n",
    "    if disjoint_batch:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        batch_indices = np.cumsum([0] + list(np.ones(num_batches) * batch_size))\n",
    "        batch_indices[-1] = n\n",
    "    else:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        last_batch_size = n % batch_size\n",
    "        \n",
    "    if batch_particle_connection:\n",
    "        batch_particle_dict = {}\n",
    "        batch_particle_indices = np.arange(particles) + 1\n",
    "        np.random.shuffle(batch_particle_indices)\n",
    "        if particles == num_batches:\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "        elif particles > num_batches:\n",
    "            base_batches = particles // num_batches\n",
    "            add_batches = particles % num_batches\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "            for i in range(add_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "        elif num_batches > particles:\n",
    "            num_reps = int(np.ceil(num_batches / particles))\n",
    "            particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "    else:\n",
    "        batch_particle_dict = None\n",
    "\n",
    "    model_dict = {}\n",
    "    weights_dict = {}\n",
    "    y_pred_dict = {}\n",
    "    jacobian_dict = {}\n",
    "    weights_vector_dict = {}\n",
    "    train_mse_dict = {}\n",
    "    test_mse_dict = {}\n",
    "    \n",
    "    if batch_evaluation[\"mean_model\"]:\n",
    "        train_batch_mse_mean_dict = {}\n",
    "    if batch_evaluation[\"particles\"]:\n",
    "        train_batch_mse_particle_dict = {}\n",
    "\n",
    "    # init_model already has weights and biases following the Glorot distribution\n",
    "    # it can already be used to predict and evaluate, but it is very bad\n",
    "    # only used to determine shapes and shape_elements via its weights\n",
    "    init_model = nn_model_structure(layers = layers,\n",
    "                                    neurons = neurons,\n",
    "                                    n_cols = n_cols,\n",
    "                                    classification = False)\n",
    "    init_model = nn_model_compile(init_model,\n",
    "                                  optimizer = \"sgd\")\n",
    "    weights = init_model.get_weights()\n",
    "    # shape contains the shapes of the weight matrices and bias vectors as a list of arrays\n",
    "    shapes = [np.array(params.shape) for params in weights]\n",
    "    # shape_elements contains the indices of the weights as a vector and tells where to cut\n",
    "    shape_elements = np.cumsum([0] + [np.prod(shape) for shape in shapes])\n",
    "\n",
    "    for i in range(particles):\n",
    "        \n",
    "        # just an initial model with the correct structure regarding neurons, layers, activation functions, Glorot initialization\n",
    "        model = nn_model_structure(layers = layers,\n",
    "                                   neurons = neurons,\n",
    "                                   n_cols = n_cols,\n",
    "                                   classification = False)\n",
    "        model = nn_model_compile(model,\n",
    "                                 optimizer = \"sgd\")\n",
    "        # for every particle write the model in a dictionary\n",
    "        model_dict[\"model_{}\".format(str(i+1))] = model\n",
    "\n",
    "        # for every particles write the weights and biases in a dictionary\n",
    "        weights_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .get_weights()\n",
    "        \n",
    "        # for every particle write the predictions on the training batches in a dictionary\n",
    "        y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .predict(X_train[:batch_size,:])\n",
    "\n",
    "        # for every particle write the Jacobian in a dictionary\n",
    "        jacobian_dict[\"model_{}\".format(str(i+1))] = 1/len(y_train[:batch_size]) * (-2)*(y_train[:batch_size] - y_pred_dict[\"model_{}\".format(str(i+1))].ravel())\n",
    "\n",
    "        train_mse_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        test_mse_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        \n",
    "        if batch_evaluation[\"particles\"]:\n",
    "            train_batch_mse_particle_dict[\"particle_{}\".format(str(i+1))] = {}\n",
    "\n",
    "    # mean_model as the model with the mean of the weights of all particle models\n",
    "    mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "    mean_model = init_model\n",
    "    mean_model.set_weights(mean_weights)\n",
    "\n",
    "    mean_model_train_mse = np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1])\n",
    "    mean_model_test_mse = np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1])\n",
    "    if tik_regularize:\n",
    "        mean_weights_raveled = [arr.ravel() for arr in mean_weights]\n",
    "        mean_model_train_mse_reg = mean_model_train_mse + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2)\n",
    "        mean_model_test_mse_reg = mean_model_test_mse + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2)\n",
    "\n",
    "    # loop over all epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # early stopping\n",
    "        if early_stopping:\n",
    "            if epoch == 0:\n",
    "                train_mse_old = 0\n",
    "                test_mse_old = 0\n",
    "            else:\n",
    "                train_mse_new = mean_model_train_mse[epoch]\n",
    "                test_mse_new = mean_model_test_mse[epoch]\n",
    "                if np.absolute(test_mse_new - test_mse_old) <= early_stopping_diff and np.absolute(train_mse_new - train_mse_old) <= early_stopping_diff:\n",
    "                    print(\"STOP: Early Stopping after epoch {} because improvement in training mse is only {} and in test mse only {}.\"\\\n",
    "                                                                         .format(epoch, train_mse_new - train_mse_old, test_mse_new - test_mse_old))\n",
    "                    break\n",
    "                test_mse_old = test_mse_new\n",
    "                                                                            \n",
    "        # Tikhonov regularization stopping\n",
    "        if tik_regularize and reg_mse_stop:\n",
    "            if epoch >= 1:\n",
    "                if mean_model_train_mse_reg[epoch] > mean_model_train_mse_reg[epoch-1] and mean_model_test_mse_reg[epoch] > mean_model_test_mse_reg[epoch-1]:\n",
    "                    print(\"Training and test MSEs containing Tikhonov regularization start to rise. Algorithm is stopped after epoch {}.\".format(epoch))\n",
    "                    break\n",
    "                \n",
    "        # shuffle the data\n",
    "        if shuffle:\n",
    "            indices = y_train.sample(frac=1).index\n",
    "        else:\n",
    "            indices = y_train.index\n",
    "            \n",
    "        if disjoint_batch:\n",
    "            X_batches = [np.array(X_train)[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [y_train.iloc[indices].reset_index(drop = True)[int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [np.array(i) for i in y_batches]\n",
    "        else:\n",
    "            if last_batch_size != 0:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches-1)]\n",
    "                indices.append(np.random.choice(len(X_train), size = last_batch_size, replace = True))\n",
    "            else:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches)]\n",
    "            X_batches = [X_train[indices[i]] for i in range(len(indices))]\n",
    "            y_batches = [y_train[indices[i]] for i in range(len(indices))]\n",
    "            \n",
    "        if batch_particle_connection and batch_particle_shuffle == \"batch\":\n",
    "            shuffled_indices = np.hstack(list(batch_particle_dict.values()))\n",
    "            np.random.shuffle(shuffled_indices)\n",
    "            batch_particle_values = list(batch_particle_dict.values())\n",
    "            for i in range(len(batch_particle_values)):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = shuffled_indices[i*len(batch_particle_values[i]):(i+1)*len(batch_particle_values[i])]\n",
    "        elif batch_particle_connection and batch_particle_shuffle == \"full\":\n",
    "            batch_particle_dict = {}\n",
    "            batch_particle_indices = np.arange(particles) + 1\n",
    "            np.random.shuffle(batch_particle_indices)\n",
    "            if particles == num_batches:\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "            elif particles > num_batches:\n",
    "                base_batches = particles // num_batches\n",
    "                add_batches = particles % num_batches\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                    batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "                for i in range(add_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "            elif num_batches > particles:\n",
    "                num_reps = int(np.ceil(num_batches / particles))\n",
    "                particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "            \n",
    "        if batch_evaluation[\"mean_model\"]:\n",
    "            train_batch_mse_mean_dict[\"Epoch_{}\".format(str(epoch+1))] = {}\n",
    "        if batch_evaluation[\"particles\"]:\n",
    "            train_batch_mse_particle_dict[\"particle_{}\".format(str(i+1))][\"Epoch_{}\".format(str(epoch+1))] = {}\n",
    "                    \n",
    "        # loop over all batches\n",
    "        for b in range(num_batches):\n",
    "            for mult_updates in range(mult_updates_batch):\n",
    "                batch_particles = []\n",
    "                y_pred_batch_dict = {}\n",
    "                jacobian_batch_dict = {}\n",
    "                for i in range(particles):\n",
    "                    if batch_particle_connection: \n",
    "                        if num_batches == particles or num_batches > particles:\n",
    "                            if batch_particle_dict[\"batch_{}\".format(str(b+1))] != i+1:\n",
    "                                continue\n",
    "                        else:\n",
    "                            if i+1 not in batch_particle_dict[\"batch_{}\".format(str(b+1))]:\n",
    "                                continue\n",
    "                    if batch_particle_connection:\n",
    "                        batch_particles.append(i+1)\n",
    "                    \n",
    "                    # set new weights for model\n",
    "                    model_dict[\"model_{}\".format(str(i+1))].set_weights(weights_dict[\"model_{}\".format(str(i+1))])\n",
    "\n",
    "                    # for every particle write the predictions on the training batches in a dictionary\n",
    "                    y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                    .predict(X_batches[b])\n",
    "                    y_pred_batch_dict[\"model_{}\".format(i+1)] = y_pred_dict[\"model_{}\".format(i+1)]\n",
    "\n",
    "                    # for every particle write the Jacobian in a dictionary\n",
    "                    jacobian_dict[\"model_{}\".format(str(i+1))] = 1/len(y_batches[b]) * (-2)*(y_batches[b] - y_pred_dict[\"model_{}\".format(str(i+1))].ravel())\n",
    "                    jacobian_batch_dict[\"model_{}\".format(i+1)] = jacobian_dict[\"model_{}\".format(i+1)]\n",
    "\n",
    "                    if batch_evaluation[\"mean_model\"]:\n",
    "                        train_batch_mse_mean_dict[\"Epoch_{}\".format(str(epoch+1))][\"Batch_{}\".format(str(b+1))] = mean_model.evaluate(X_batches[b], y_batches[b], verbose = 0)[1]\n",
    "                    if batch_evaluation[\"particles\"]:\n",
    "                        train_batch_mse_particle_dict[\"particle_{}\".format(str(i+1))][\"Epoch_{}\".format(str(epoch+1))][\"Batch_{}\".format(str(b+1))] = model_dict[\"model_{}\".format(str(i+1))].evaluate(X_batches[b], y_batches[b], verbose = 0)[1]\n",
    "\n",
    "                        \n",
    "                if not batch_particle_connection:        \n",
    "                    # compute the mean of the predictions\n",
    "                    y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "                    # compute the matrix D elementwise\n",
    "                    d = np.zeros(shape = (particles, particles))\n",
    "                    for k in range(particles):\n",
    "                        y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                        for j in range(particles):\n",
    "                            d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_dict[\"model_{}\".format(str(j+1))])\n",
    "                    d = np.transpose(d)\n",
    "\n",
    "                    # compute the scalar h_t\n",
    "                    h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "                    # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "                    for i in range(particles):\n",
    "                        weights_array = np.array([])\n",
    "                        for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                            weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "                    # matrix with particle parameters as row vectors\n",
    "                    weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "                    # compute the matrix with the updates for each particle\n",
    "                    weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "                    for i in range(particles):\n",
    "                        # write the updates back into the dictionary\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                        # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                        for l in range(len(shape_elements)-1):\n",
    "                            start = shape_elements[l]\n",
    "                            end = shape_elements[l+1]\n",
    "                            weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                \n",
    "                elif batch_particle_connection and not update_all:\n",
    "                    # compute the mean of the predictions\n",
    "                    y_pred_mean = np.mean(list(y_pred_batch_dict.values()), axis = 0)\n",
    "                    \n",
    "                    # compute the matrix D elementwise\n",
    "                    d = np.zeros(shape = (len(batch_particles), len(batch_particles)))\n",
    "                    for k in range(len(batch_particles)):\n",
    "                        y_pred_centered = y_pred_batch_dict[\"model_{}\".format(batch_particles[k])] - y_pred_mean\n",
    "                        for j in range(len(batch_particles)):\n",
    "                            d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_batch_dict[\"model_{}\".format(batch_particles[j])])\n",
    "                    d = np.transpose(d)\n",
    "\n",
    "                    # compute the scalar h_t\n",
    "                    h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "                    # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "                    for i in range(particles):\n",
    "                        weights_array = np.array([])\n",
    "                        for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                            weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "                    # matrix with particle parameters as row vectors\n",
    "                    weights_vector_batch_dict = {}\n",
    "                    for i in range(len(batch_particles)):\n",
    "                        weights_vector_batch_dict[\"model_{}\".format(batch_particles[i])] = weights_vector_dict[\"model_{}\".format(batch_particles[i])]\n",
    "                        weights_all_ptcls = np.array(list(weights_vector_batch_dict.values()))\n",
    "\n",
    "                    # compute the matrix with the updates for each particle\n",
    "                    weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "                    for i in range(len(batch_particles)):\n",
    "                        # write the updates back into the dictionary\n",
    "                        weights_vector_dict[\"model_{}\".format(batch_particles[i])] = weights_all_ptcls[i]\n",
    "                        # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                        for l in range(len(shape_elements)-1):\n",
    "                            start = shape_elements[l]\n",
    "                            end = shape_elements[l+1]\n",
    "                            weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "\n",
    "        if batch_particle_connection and update_all:\n",
    "            # compute the mean of the predictions\n",
    "            y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "            # compute the matrix D elementwise\n",
    "            d = np.zeros(shape = (particles, particles))\n",
    "            for k in range(particles):\n",
    "                y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                for j in range(particles):\n",
    "                    d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_dict[\"model_{}\".format(str(j+1))])\n",
    "            d = np.transpose(d)\n",
    "\n",
    "            # compute the scalar h_t\n",
    "            h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "            # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "            for i in range(particles):\n",
    "                weights_array = np.array([])\n",
    "                for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                    weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "            # matrix with particle parameters as row vectors\n",
    "            weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "            # compute the matrix with the updates for each particle\n",
    "            weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "            for i in range(particles):\n",
    "                # write the updates back into the dictionary\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                for l in range(len(shape_elements)-1):\n",
    "                    start = shape_elements[l]\n",
    "                    end = shape_elements[l+1]\n",
    "                    weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                                                            \n",
    "        for i in range(particles):\n",
    "            # for every particle write the training mse of the current iteration in a dictionary\n",
    "            train_mse_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_train, y_train, verbose = 0)[1])\n",
    "\n",
    "            # for every particle write the test mse of the current iteration in a dictionary\n",
    "            test_mse_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_test, y_test, verbose = 0)[1])\n",
    "\n",
    "        # update the mean_model\n",
    "        mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "        mean_model.set_weights(mean_weights)\n",
    "\n",
    "        mean_model_train_mse = np.append(mean_model_train_mse, np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1]))\n",
    "        mean_model_test_mse = np.append(mean_model_test_mse, np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1]))\n",
    "        if tik_regularize and reg_mse_stop:\n",
    "            mean_weights_raveled = [arr.ravel() for arr in mean_weights]\n",
    "            mean_model_train_mse_reg = np.append(mean_model_train_mse_reg, mean_model_train_mse[-1] + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2))\n",
    "            mean_model_test_mse_reg = np.append(mean_model_test_mse_reg, mean_model_test_mse[-1] + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2))\n",
    "        \n",
    "\n",
    "        if verbose == 1:\n",
    "            print(\"Epoch {}. Training MSE: {}, Test MSE: {}.\".format(epoch+1,\n",
    "                                                                     np.round(mean_model_train_mse[-1], 3),\n",
    "                                                                     np.round(mean_model_test_mse[-1], 3)))\n",
    "            \n",
    "    mean_model.history.history = {\"mse\": mean_model_train_mse[1:],\n",
    "                                  \"val_mse\": mean_model_test_mse[1:]}\n",
    "        \n",
    "    if save_all:\n",
    "        param_dict = param_to_dict(X_train,\n",
    "                                   X_test,\n",
    "                                   y_train,\n",
    "                                   y_test,\n",
    "                                   layers,\n",
    "                                   neurons,\n",
    "                                   particles,\n",
    "                                   epochs,\n",
    "                                   batch_size,\n",
    "                                   h_0,\n",
    "                                   delta,\n",
    "                                   epsilon,\n",
    "                                   randomization,\n",
    "                                   shuffle,\n",
    "                                   early_stopping,\n",
    "                                   early_stopping_diff\n",
    "                                   )\n",
    "        results_dict = results_to_dict(mean_model_train_mse,\n",
    "                                       mean_model_test_mse,\n",
    "                                       train_mse_dict,\n",
    "                                       test_mse_dict,\n",
    "                                       weights_dict,\n",
    "                                       y_pred_dict,\n",
    "                                       False\n",
    "                                       )\n",
    "\n",
    "        saving_dict = {}\n",
    "        saving_dict[\"parameters\"] = param_dict\n",
    "        saving_dict[\"results\"] = results_dict\n",
    "        saving_dict[\"analysis\"] = analysis_dict\n",
    "\n",
    "        save_objects(obj_dict = saving_dict,\n",
    "                     file = file_var)\n",
    "\n",
    "        nn_save(model = mean_model,\n",
    "                path_name = file_model)\n",
    "        \n",
    "        if batch_evaluation[\"mean_model\"]:\n",
    "            full_dict = {}\n",
    "            full_dict[\"mean_model\"] = train_batch_mse_mean_dict\n",
    "            if batch_evaluation[\"particles\"]:\n",
    "                full_dict[\"particles\"] = train_batch_mse_particle_dict\n",
    "            save_objects(obj_dict = full_dict,\n",
    "                         file = batch_evaluation[\"file\"])\n",
    "        elif batch_evaluation[\"particles\"]:\n",
    "            full_dict = {}\n",
    "            full_dict[\"particles\"] = train_batch_mse_particle_dict\n",
    "            if batch_evaluation[\"mean_model\"]:\n",
    "                full_dict[\"mean_model\"] = train_batch_mse_mean_dict\n",
    "            save_objects(obj_dict = full_dict,\n",
    "                         file = batch_evaluation[\"file\"])\n",
    " \n",
    "    return mean_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting_dict = {\"particles\": 20,\n",
    "                \"epochs\": 10,\n",
    "                \"batch_size\": 100,    # len(X_train)\n",
    "                \"h_0\": 2,\n",
    "                \"epsilon\": 0.5,\n",
    "                \"randomization\": False,\n",
    "                \"shuffle\": True,\n",
    "                \"early_stopping\": False,\n",
    "                \"early_stopping_diff\": 0.001\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dict = {\"disjoint_batch\": True,\n",
    "                 \"multiple_updates_same_batch\": None,\n",
    "                 \"batch_particle_connection\": {\"connect\": True,\n",
    "                                               \"shuffle\": \"full\",        # None, \"batch\", \"full\"\n",
    "                                               \"update_all\": True},\n",
    "                 \"batch_evaluation\": {\"mean_model\": False,\n",
    "                                      \"particles\": False,\n",
    "                                      \"file\": \"../objects/wine/batch_mse.pckl\"},\n",
    "                 \"tikhonov\": {\"regularize\": False,\n",
    "                              \"lambda\": None,\n",
    "                              \"reg_mse_stop\": False}\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training MSE: 11.288, Test MSE: 11.264.\n",
      "Epoch 2. Training MSE: 4.675, Test MSE: 4.665.\n",
      "Epoch 3. Training MSE: 2.615, Test MSE: 2.625.\n",
      "Epoch 4. Training MSE: 2.076, Test MSE: 2.095.\n",
      "Epoch 5. Training MSE: 1.323, Test MSE: 1.366.\n",
      "Epoch 6. Training MSE: 1.134, Test MSE: 1.186.\n",
      "Epoch 7. Training MSE: 0.974, Test MSE: 1.028.\n",
      "Epoch 8. Training MSE: 0.874, Test MSE: 0.953.\n",
      "Epoch 9. Training MSE: 0.847, Test MSE: 0.934.\n",
      "Epoch 10. Training MSE: 0.832, Test MSE: 0.92.\n"
     ]
    }
   ],
   "source": [
    "mean_model = enkf_regressor_analysis(X_train,\n",
    "                                     X_test,\n",
    "                                     y_train,\n",
    "                                     y_test,\n",
    "                                     layers,\n",
    "                                     neurons,\n",
    "                                     setting_dict,\n",
    "                                     analysis_dict,\n",
    "                                     save_all = False,\n",
    "                                     file_var = \"../objects/wine/wine_enkf_E{}_B{}_P{}_H{}.pckl\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                     file_model = \"../models/wine/wine_enkf_E{}_B{}_P{}_H{}.h5\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                     verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation time: 0.34036461512247723 minutes.\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "print(\"Calculation time: {} minutes.\".format((end_time - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_plot_mse(mean_model,\n",
    "            mse_mean = None,\n",
    "            start_epoch = 1,\n",
    "            savefig = False,\n",
    "            file = \"../img/wine/wine_enkf_E{}_B{}_P{}_H{}.png\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
