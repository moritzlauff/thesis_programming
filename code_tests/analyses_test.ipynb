{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../python/functions\")\n",
    "sys.path.insert(2, \"../python/architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../python/architecture\\reproducible.py:14: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_prep_functions import wine_prep\n",
    "from plotting_functions import nn_plot_mse\n",
    "from enkf_functions import enkf_regressor\n",
    "from saving_functions import load_objects, save_objects\n",
    "from model_functions import nn_load, nn_model_structure, nn_model_compile\n",
    "import reproducible\n",
    "import no_gpu\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = wine_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use samller dataset for increased speed\n",
    "X_train = X_train[:1000]\n",
    "X_test = X_test[:500]\n",
    "y_train = y_train[:1000]\n",
    "y_test = y_test[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 4\n",
    "neurons = [32, 32, 16, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enkf_regressor_analysis(X_train,\n",
    "                            X_test,\n",
    "                            y_train,\n",
    "                            y_test,\n",
    "                            layers,\n",
    "                            neurons,\n",
    "                            setting_dict,\n",
    "                            analysis_dict,\n",
    "                            save_all = False,\n",
    "                            file_var = \"file.pckl\",\n",
    "                            file_model = \"file.h5\",\n",
    "                            verbose = 0\n",
    "                            ):\n",
    "\n",
    "    \"\"\" Ensemble Kalman Filter algorithm analysis for regression problems.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train (np.ndarray): Training data X.\n",
    "    X_test (np.ndarray): Test data X.\n",
    "    y_train (pd.DataFrame): Training data y.\n",
    "    y_test (pd.DataFrame): Test data y.\n",
    "    layers (int): Number of layers.\n",
    "    neurons (list): Number of neurons in each layer.\n",
    "    setting_dict (dict): Dictionary containing\n",
    "        particles (int): Number of particles in the ensemble.\n",
    "        epochs (int): Number of epochs.\n",
    "        batch_size (None or int): Size of the batches. Must be between 0 and the number of observations in the training set.\n",
    "        h_0 (int or float): Starting step size.\n",
    "        delta (float): Constant for numerical stability in the jacobian.\n",
    "        epsilon (float): Constant for numerical stability in the step size.\n",
    "        randomization (bool): Whether or not to add noise to the particles and randomize them around their mean.\n",
    "        shuffle (bool): Whether or not to shuffle the data prior to each epoch.\n",
    "        early_stopping (bool): Whether or not to stop the calculation when the changes get small.\n",
    "        early_stopping_diff (bool): Minimum change before early stopping is applied.\n",
    "    analysis_dict (dict): Dictionary containing\n",
    "        disjoint_batch (bool): Whether or not to use disjoint batches. If False then each batch is sampled with replacement.\n",
    "        multiple_updates_same_batch (int or None): Number of consecutive updates on the same batch.\n",
    "        batch_particle_connection (dict): Dictionary containing\n",
    "            connect (bool): Whether or not to connect particles and batches.\n",
    "            shuffle (str or None): Whether or not and how to shuffle the connection. None = no shuffle. \"batch\" = shuffle the batch for fixed particle sets. \"full\" = shuffle the particle sets and their corresponding batch.\n",
    "            update_all (bool): Whether or not to update after all particles have seen some data.\n",
    "        batch_evaluation (dict): Dictionary containing\n",
    "            mean_model (bool): Whether or not evaluate the mean model after every batch on that batch. Warning: the computational costs are very high.\n",
    "            particles (bool): Whether or not evaluate every particle model after every batch on that batch. Warning: the computational costs are incredibly high.\n",
    "            file (str): Path and name of the file to save evaluations into \n",
    "        tikhonov (dict): Dictionary containing\n",
    "            regularize (bool): Whether or not to use Tikhonov regularization.\n",
    "            lambda (None or float): Lambda parameter in Tikhonov regularization.\n",
    "            reg_mse_stop (bool): Whether or not to stop when MSE + Tikhonov regularization starts to rise again.\n",
    "    save_all (bool): Whether or not to save all important variables and models.\n",
    "    file_var (str): Path and name of the file to save variables into.\n",
    "    file_model (str): Path and name of the file to save the final model into.\n",
    "    verbose (int): If 0, then don't print anything throughout the training process. If 1, then print training and test accuracy after each epoch.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    mean_model (tensorflow.python.keras.engine.sequential.Sequential): The final model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    particles = setting_dict[\"particles\"]\n",
    "    epochs = setting_dict[\"epochs\"]\n",
    "    batch_size = setting_dict[\"batch_size\"]\n",
    "    h_0 = setting_dict[\"h_0\"]\n",
    "    delta = None\n",
    "    epsilon = setting_dict[\"epsilon\"]\n",
    "    randomization = setting_dict[\"randomization\"]\n",
    "    shuffle = setting_dict[\"shuffle\"]\n",
    "    early_stopping = setting_dict[\"early_stopping\"]\n",
    "    early_stopping_diff = setting_dict[\"early_stopping_diff\"]\n",
    "    \n",
    "    disjoint_batch = analysis_dict[\"disjoint_batch\"]\n",
    "    batch_evaluation = analysis_dict[\"batch_evaluation\"]\n",
    "    batch_particle_connection = analysis_dict[\"batch_particle_connection\"][\"connect\"]\n",
    "    batch_particle_shuffle = analysis_dict[\"batch_particle_connection\"][\"shuffle\"]\n",
    "    update_all = analysis_dict[\"batch_particle_connection\"][\"update_all\"]\n",
    "    mult_updates_batch = analysis_dict[\"multiple_updates_same_batch\"]\n",
    "    tik_regularize = analysis_dict[\"tikhonov\"][\"regularize\"]\n",
    "    tik_lambda = analysis_dict[\"tikhonov\"][\"lambda\"]\n",
    "    reg_mse_stop = analysis_dict[\"tikhonov\"][\"reg_mse_stop\"]\n",
    "    \n",
    "    if tik_lambda is None:\n",
    "        tik_lambda = 0\n",
    "    \n",
    "    if mult_updates_batch is None:\n",
    "        mult_updates_batch = 1\n",
    "\n",
    "    if batch_size == None:\n",
    "        batch_size = len(X_train)\n",
    "\n",
    "    n_cols = X_train.shape[1]\n",
    "    \n",
    "    if disjoint_batch:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        batch_indices = np.cumsum([0] + list(np.ones(num_batches) * batch_size))\n",
    "        batch_indices[-1] = n\n",
    "    else:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        last_batch_size = n % batch_size\n",
    "        \n",
    "    if batch_particle_connection:\n",
    "        batch_particle_dict = {}\n",
    "        batch_particle_indices = np.arange(particles) + 1\n",
    "        np.random.shuffle(batch_particle_indices)\n",
    "        if particles == num_batches:\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "        elif particles > num_batches:\n",
    "            base_batches = particles // num_batches\n",
    "            add_batches = particles % num_batches\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "            for i in range(add_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "        elif num_batches > particles:\n",
    "            num_reps = int(np.ceil(num_batches / particles))\n",
    "            particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "    else:\n",
    "        batch_particle_dict = None\n",
    "\n",
    "    model_dict = {}\n",
    "    weights_dict = {}\n",
    "    y_pred_dict = {}\n",
    "    jacobian_dict = {}\n",
    "    weights_vector_dict = {}\n",
    "    train_mse_dict = {}\n",
    "    test_mse_dict = {}\n",
    "    \n",
    "    if batch_evaluation[\"mean_model\"]:\n",
    "        train_batch_mse_mean_dict = {}\n",
    "    if batch_evaluation[\"particles\"]:\n",
    "        train_batch_mse_particle_dict = {}\n",
    "\n",
    "    # init_model already has weights and biases following the Glorot distribution\n",
    "    # it can already be used to predict and evaluate, but it is very bad\n",
    "    # only used to determine shapes and shape_elements via its weights\n",
    "    init_model = nn_model_structure(layers = layers,\n",
    "                                    neurons = neurons,\n",
    "                                    n_cols = n_cols,\n",
    "                                    classification = False)\n",
    "    init_model = nn_model_compile(init_model,\n",
    "                                  optimizer = \"sgd\")\n",
    "    weights = init_model.get_weights()\n",
    "    # shape contains the shapes of the weight matrices and bias vectors as a list of arrays\n",
    "    shapes = [np.array(params.shape) for params in weights]\n",
    "    # shape_elements contains the indices of the weights as a vector and tells where to cut\n",
    "    shape_elements = np.cumsum([0] + [np.prod(shape) for shape in shapes])\n",
    "\n",
    "    for i in range(particles):\n",
    "        \n",
    "        # just an initial model with the correct structure regarding neurons, layers, activation functions, Glorot initialization\n",
    "        model = nn_model_structure(layers = layers,\n",
    "                                   neurons = neurons,\n",
    "                                   n_cols = n_cols,\n",
    "                                   classification = False)\n",
    "        model = nn_model_compile(model,\n",
    "                                 optimizer = \"sgd\")\n",
    "        # for every particle write the model in a dictionary\n",
    "        model_dict[\"model_{}\".format(str(i+1))] = model\n",
    "\n",
    "        # for every particles write the weights and biases in a dictionary\n",
    "        weights_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .get_weights()\n",
    "        \n",
    "        # for every particle write the predictions on the training batches in a dictionary\n",
    "        y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .predict(X_train[:batch_size,:])\n",
    "\n",
    "        # for every particle write the Jacobian in a dictionary\n",
    "        jacobian_dict[\"model_{}\".format(str(i+1))] = 1/len(y_train[:batch_size]) * (-2)*(y_train[:batch_size] - y_pred_dict[\"model_{}\".format(str(i+1))].ravel())\n",
    "\n",
    "        train_mse_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        test_mse_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        \n",
    "        if batch_evaluation[\"particles\"]:\n",
    "            train_batch_mse_particle_dict[\"particle_{}\".format(str(i+1))] = {}\n",
    "\n",
    "    # mean_model as the model with the mean of the weights of all particle models\n",
    "    mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "    mean_model = init_model\n",
    "    mean_model.set_weights(mean_weights)\n",
    "\n",
    "    mean_model_train_mse = np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1])\n",
    "    mean_model_test_mse = np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1])\n",
    "    if tik_regularize:\n",
    "        mean_weights_raveled = [arr.ravel() for arr in mean_weights]\n",
    "        mean_model_train_mse_reg = mean_model_train_mse + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2)\n",
    "        mean_model_test_mse_reg = mean_model_test_mse + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2)\n",
    "\n",
    "    # loop over all epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # early stopping\n",
    "        if early_stopping:\n",
    "            if epoch == 0:\n",
    "                train_mse_old = 0\n",
    "                test_mse_old = 0\n",
    "            else:\n",
    "                train_mse_new = mean_model_train_mse[epoch]\n",
    "                test_mse_new = mean_model_test_mse[epoch]\n",
    "                if np.absolute(test_mse_new - test_mse_old) <= early_stopping_diff and np.absolute(train_mse_new - train_mse_old) <= early_stopping_diff:\n",
    "                    print(\"STOP: Early Stopping after epoch {} because improvement in training MSE is only {} and in test mse only {}.\"\\\n",
    "                                                                         .format(epoch, train_mse_new - train_mse_old, test_mse_new - test_mse_old))\n",
    "                    break\n",
    "                test_mse_old = test_mse_new\n",
    "                                                                            \n",
    "        # Tikhonov regularization stopping\n",
    "        if tik_regularize and reg_mse_stop:\n",
    "            if epoch >= 1:\n",
    "                if mean_model_train_mse_reg[epoch] > mean_model_train_mse_reg[epoch-1] and mean_model_test_mse_reg[epoch] > mean_model_test_mse_reg[epoch-1]:\n",
    "                    print(\"Training and test MSEs containing Tikhonov regularization start to rise. Algorithm is stopped after epoch {}.\".format(epoch))\n",
    "                    break\n",
    "                \n",
    "        # shuffle the data\n",
    "        if shuffle:\n",
    "            indices = y_train.sample(frac=1).index\n",
    "        else:\n",
    "            indices = y_train.index\n",
    "            \n",
    "        if disjoint_batch:\n",
    "            X_batches = [np.array(X_train)[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [y_train.iloc[indices].reset_index(drop = True)[int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [np.array(i) for i in y_batches]\n",
    "        else:\n",
    "            if last_batch_size != 0:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches-1)]\n",
    "                indices.append(np.random.choice(len(X_train), size = last_batch_size, replace = True))\n",
    "            else:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches)]\n",
    "            X_batches = [X_train[indices[i]] for i in range(len(indices))]\n",
    "            y_batches = [y_train[indices[i]] for i in range(len(indices))]\n",
    "            \n",
    "        if batch_particle_connection and batch_particle_shuffle == \"batch\":\n",
    "            shuffled_indices = np.hstack(list(batch_particle_dict.values()))\n",
    "            np.random.shuffle(shuffled_indices)\n",
    "            batch_particle_values = list(batch_particle_dict.values())\n",
    "            for i in range(len(batch_particle_values)):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = shuffled_indices[i*len(batch_particle_values[i]):(i+1)*len(batch_particle_values[i])]\n",
    "        elif batch_particle_connection and batch_particle_shuffle == \"full\":\n",
    "            batch_particle_dict = {}\n",
    "            batch_particle_indices = np.arange(particles) + 1\n",
    "            np.random.shuffle(batch_particle_indices)\n",
    "            if particles == num_batches:\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "            elif particles > num_batches:\n",
    "                base_batches = particles // num_batches\n",
    "                add_batches = particles % num_batches\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                    batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "                for i in range(add_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "            elif num_batches > particles:\n",
    "                num_reps = int(np.ceil(num_batches / particles))\n",
    "                particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "            \n",
    "        if batch_evaluation[\"mean_model\"]:\n",
    "            train_batch_mse_mean_dict[\"Epoch_{}\".format(str(epoch+1))] = {}\n",
    "        if batch_evaluation[\"particles\"]:\n",
    "            train_batch_mse_particle_dict[\"particle_{}\".format(str(i+1))][\"Epoch_{}\".format(str(epoch+1))] = {}\n",
    "                    \n",
    "        # loop over all batches\n",
    "        for b in range(num_batches):\n",
    "            for mult_updates in range(mult_updates_batch):\n",
    "                batch_particles = []\n",
    "                y_pred_batch_dict = {}\n",
    "                jacobian_batch_dict = {}\n",
    "                for i in range(particles):\n",
    "                    if batch_particle_connection: \n",
    "                        if num_batches == particles or num_batches > particles:\n",
    "                            if batch_particle_dict[\"batch_{}\".format(str(b+1))] != i+1:\n",
    "                                continue\n",
    "                        else:\n",
    "                            if i+1 not in batch_particle_dict[\"batch_{}\".format(str(b+1))]:\n",
    "                                continue\n",
    "                    if batch_particle_connection:\n",
    "                        batch_particles.append(i+1)\n",
    "                    \n",
    "                    # set new weights for model\n",
    "                    model_dict[\"model_{}\".format(str(i+1))].set_weights(weights_dict[\"model_{}\".format(str(i+1))])\n",
    "\n",
    "                    # for every particle write the predictions on the training batches in a dictionary\n",
    "                    y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                    .predict(X_batches[b])\n",
    "                    y_pred_batch_dict[\"model_{}\".format(i+1)] = y_pred_dict[\"model_{}\".format(i+1)]\n",
    "\n",
    "                    # for every particle write the Jacobian in a dictionary\n",
    "                    jacobian_dict[\"model_{}\".format(str(i+1))] = 1/len(y_batches[b]) * (-2)*(y_batches[b] - y_pred_dict[\"model_{}\".format(str(i+1))].ravel())\n",
    "                    jacobian_batch_dict[\"model_{}\".format(i+1)] = jacobian_dict[\"model_{}\".format(i+1)]\n",
    "\n",
    "                    if batch_evaluation[\"mean_model\"]:\n",
    "                        train_batch_mse_mean_dict[\"Epoch_{}\".format(str(epoch+1))][\"Batch_{}\".format(str(b+1))] = mean_model.evaluate(X_batches[b], y_batches[b], verbose = 0)[1]\n",
    "                    if batch_evaluation[\"particles\"]:\n",
    "                        train_batch_mse_particle_dict[\"particle_{}\".format(str(i+1))][\"Epoch_{}\".format(str(epoch+1))][\"Batch_{}\".format(str(b+1))] = model_dict[\"model_{}\".format(str(i+1))].evaluate(X_batches[b], y_batches[b], verbose = 0)[1]\n",
    "\n",
    "                        \n",
    "                if not batch_particle_connection:        \n",
    "                    # compute the mean of the predictions\n",
    "                    y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "                    # compute the matrix D elementwise\n",
    "                    d = np.zeros(shape = (particles, particles))\n",
    "                    for k in range(particles):\n",
    "                        y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                        for j in range(particles):\n",
    "                            d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_dict[\"model_{}\".format(str(j+1))])\n",
    "                    d = np.transpose(d)\n",
    "\n",
    "                    # compute the scalar h_t\n",
    "                    h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "                    # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "                    for i in range(particles):\n",
    "                        weights_array = np.array([])\n",
    "                        for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                            weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "                    # matrix with particle parameters as row vectors\n",
    "                    weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "                    # compute the matrix with the updates for each particle\n",
    "                    weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "                    for i in range(particles):\n",
    "                        # write the updates back into the dictionary\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                        # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                        for l in range(len(shape_elements)-1):\n",
    "                            start = shape_elements[l]\n",
    "                            end = shape_elements[l+1]\n",
    "                            weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                \n",
    "                elif batch_particle_connection and not update_all:\n",
    "                    # compute the mean of the predictions\n",
    "                    y_pred_mean = np.mean(list(y_pred_batch_dict.values()), axis = 0)\n",
    "                    \n",
    "                    # compute the matrix D elementwise\n",
    "                    d = np.zeros(shape = (len(batch_particles), len(batch_particles)))\n",
    "                    for k in range(len(batch_particles)):\n",
    "                        y_pred_centered = y_pred_batch_dict[\"model_{}\".format(batch_particles[k])] - y_pred_mean\n",
    "                        for j in range(len(batch_particles)):\n",
    "                            d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_batch_dict[\"model_{}\".format(batch_particles[j])])\n",
    "                    d = np.transpose(d)\n",
    "\n",
    "                    # compute the scalar h_t\n",
    "                    h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "                    # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "                    for i in range(particles):\n",
    "                        weights_array = np.array([])\n",
    "                        for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                            weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "                    # matrix with particle parameters as row vectors\n",
    "                    weights_vector_batch_dict = {}\n",
    "                    for i in range(len(batch_particles)):\n",
    "                        weights_vector_batch_dict[\"model_{}\".format(batch_particles[i])] = weights_vector_dict[\"model_{}\".format(batch_particles[i])]\n",
    "                        weights_all_ptcls = np.array(list(weights_vector_batch_dict.values()))\n",
    "\n",
    "                    # compute the matrix with the updates for each particle\n",
    "                    weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "                    for i in range(len(batch_particles)):\n",
    "                        # write the updates back into the dictionary\n",
    "                        weights_vector_dict[\"model_{}\".format(batch_particles[i])] = weights_all_ptcls[i]\n",
    "                        # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                        for l in range(len(shape_elements)-1):\n",
    "                            start = shape_elements[l]\n",
    "                            end = shape_elements[l+1]\n",
    "                            weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "\n",
    "        if batch_particle_connection and update_all:\n",
    "            # compute the mean of the predictions\n",
    "            y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "            # compute the matrix D elementwise\n",
    "            d = np.zeros(shape = (particles, particles))\n",
    "            for k in range(particles):\n",
    "                y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                for j in range(particles):\n",
    "                    d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_dict[\"model_{}\".format(str(j+1))])\n",
    "            d = np.transpose(d)\n",
    "\n",
    "            # compute the scalar h_t\n",
    "            h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "            # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "            for i in range(particles):\n",
    "                weights_array = np.array([])\n",
    "                for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                    weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "            # matrix with particle parameters as row vectors\n",
    "            weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "            # compute the matrix with the updates for each particle\n",
    "            weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "            for i in range(particles):\n",
    "                # write the updates back into the dictionary\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                for l in range(len(shape_elements)-1):\n",
    "                    start = shape_elements[l]\n",
    "                    end = shape_elements[l+1]\n",
    "                    weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                                                            \n",
    "        for i in range(particles):\n",
    "            # for every particle write the training MSE of the current iteration in a dictionary\n",
    "            train_mse_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_train, y_train, verbose = 0)[1])\n",
    "\n",
    "            # for every particle write the test MSE of the current iteration in a dictionary\n",
    "            test_mse_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_test, y_test, verbose = 0)[1])\n",
    "\n",
    "        # update the mean_model\n",
    "        mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "        mean_model.set_weights(mean_weights)\n",
    "\n",
    "        mean_model_train_mse = np.append(mean_model_train_mse, np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1]))\n",
    "        mean_model_test_mse = np.append(mean_model_test_mse, np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1]))\n",
    "        if tik_regularize and reg_mse_stop:\n",
    "            mean_weights_raveled = [arr.ravel() for arr in mean_weights]\n",
    "            mean_model_train_mse_reg = np.append(mean_model_train_mse_reg, mean_model_train_mse[-1] + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2))\n",
    "            mean_model_test_mse_reg = np.append(mean_model_test_mse_reg, mean_model_test_mse[-1] + tik_lambda * np.sum(np.hstack(mean_weights_raveled)**2))\n",
    "        \n",
    "\n",
    "        if verbose == 1:\n",
    "            print(\"Epoch {}. Training MSE: {}, Test MSE: {}.\".format(epoch+1,\n",
    "                                                                     np.round(mean_model_train_mse[-1], 3),\n",
    "                                                                     np.round(mean_model_test_mse[-1], 3)))\n",
    "            \n",
    "    mean_model.history.history = {\"mse\": mean_model_train_mse[1:],\n",
    "                                  \"val_mse\": mean_model_test_mse[1:]}\n",
    "        \n",
    "    if save_all:\n",
    "        param_dict = param_to_dict(X_train,\n",
    "                                   X_test,\n",
    "                                   y_train,\n",
    "                                   y_test,\n",
    "                                   layers,\n",
    "                                   neurons,\n",
    "                                   particles,\n",
    "                                   epochs,\n",
    "                                   batch_size,\n",
    "                                   h_0,\n",
    "                                   delta,\n",
    "                                   epsilon,\n",
    "                                   randomization,\n",
    "                                   shuffle,\n",
    "                                   early_stopping,\n",
    "                                   early_stopping_diff\n",
    "                                   )\n",
    "        results_dict = results_to_dict(mean_model_train_mse,\n",
    "                                       mean_model_test_mse,\n",
    "                                       train_mse_dict,\n",
    "                                       test_mse_dict,\n",
    "                                       weights_dict,\n",
    "                                       y_pred_dict,\n",
    "                                       False\n",
    "                                       )\n",
    "\n",
    "        saving_dict = {}\n",
    "        saving_dict[\"parameters\"] = param_dict\n",
    "        saving_dict[\"results\"] = results_dict\n",
    "        saving_dict[\"analysis\"] = analysis_dict\n",
    "\n",
    "        save_objects(obj_dict = saving_dict,\n",
    "                     file = file_var)\n",
    "\n",
    "        nn_save(model = mean_model,\n",
    "                path_name = file_model)\n",
    "        \n",
    "        if batch_evaluation[\"mean_model\"]:\n",
    "            full_dict = {}\n",
    "            full_dict[\"mean_model\"] = train_batch_mse_mean_dict\n",
    "            if batch_evaluation[\"particles\"]:\n",
    "                full_dict[\"particles\"] = train_batch_mse_particle_dict\n",
    "            save_objects(obj_dict = full_dict,\n",
    "                         file = batch_evaluation[\"file\"])\n",
    "        elif batch_evaluation[\"particles\"]:\n",
    "            full_dict = {}\n",
    "            full_dict[\"particles\"] = train_batch_mse_particle_dict\n",
    "            if batch_evaluation[\"mean_model\"]:\n",
    "                full_dict[\"mean_model\"] = train_batch_mse_mean_dict\n",
    "            save_objects(obj_dict = full_dict,\n",
    "                         file = batch_evaluation[\"file\"])\n",
    " \n",
    "    return mean_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enkf_regressor_analysis(X_train,\n",
    "                            X_test,\n",
    "                            y_train,\n",
    "                            y_test,\n",
    "                            layers,\n",
    "                            neurons,\n",
    "                            setting_dict,\n",
    "                            analysis_dict,\n",
    "                            save_all = False,\n",
    "                            file_var = \"file.pckl\",\n",
    "                            file_model = \"file.h5\",\n",
    "                            verbose = 0\n",
    "                            ):\n",
    "\n",
    "    \"\"\" Ensemble Kalman Filter algorithm analysis for regression problems.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train (np.ndarray): Training data X.\n",
    "    X_test (np.ndarray): Test data X.\n",
    "    y_train (pd.DataFrame): Training data y.\n",
    "    y_test (pd.DataFrame): Test data y.\n",
    "    layers (int): Number of layers.\n",
    "    neurons (list): Number of neurons in each layer.\n",
    "    setting_dict (dict): Dictionary containing\n",
    "        particles (int): Number of particles in the ensemble.\n",
    "        epochs (int): Number of epochs.\n",
    "        batch_size (None or int): Size of the batches. Must be between 0 and the number of observations in the training set.\n",
    "        h_0 (int or float): Starting step size.\n",
    "        delta (float): Constant for numerical stability in the jacobian.\n",
    "        epsilon (float): Constant for numerical stability in the step size.\n",
    "        randomization (bool): Whether or not to add noise to the particles and randomize them around their mean. Remark: Not implemented.\n",
    "        shuffle (bool): Whether or not to shuffle the data prior to each epoch.\n",
    "        early_stopping (bool): Whether or not to stop the calculation when the changes get small.\n",
    "        early_stopping_diff (bool): Minimum change before early stopping is applied.\n",
    "    analysis_dict (dict): Dictionary containing\n",
    "        disjoint_batch (bool): Whether or not to use disjoint batches. If False then each batch is sampled with replacement.\n",
    "        batch_particle_connection (dict): Dictionary containing\n",
    "            connect (bool): Whether or not to connect particles and batches.\n",
    "            shuffle (str or None): Whether or not and how to shuffle the connection. None = no shuffle. \"permute\" = change the allocation of the existing batches and particle sets. \"particle\" = shuffle the particle sets for fixed batches. \"batch\" = shuffle the batch for fixed particle sets. \"full\" = shuffle the particle sets and their corresponding batch.\n",
    "    save_all (bool): Whether or not to save all important variables and models.\n",
    "    file_var (str): Path and name of the file to save variables into.\n",
    "    file_model (str): Path and name of the file to save the final model into.\n",
    "    verbose (int): If 0, then don't print anything throughout the training process. If 1, then print training and test accuracy after each epoch.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    mean_model (tensorflow.python.keras.engine.sequential.Sequential): The final model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    particles = setting_dict[\"particles\"]\n",
    "    epochs = setting_dict[\"epochs\"]\n",
    "    batch_size = setting_dict[\"batch_size\"]\n",
    "    h_0 = setting_dict[\"h_0\"]\n",
    "    delta = None\n",
    "    epsilon = setting_dict[\"epsilon\"]\n",
    "    randomization = setting_dict[\"randomization\"]\n",
    "    shuffle = setting_dict[\"shuffle\"]\n",
    "    early_stopping = setting_dict[\"early_stopping\"]\n",
    "    early_stopping_diff = setting_dict[\"early_stopping_diff\"]\n",
    "    \n",
    "    disjoint_batch = analysis_dict[\"disjoint_batch\"]\n",
    "    batch_particle_connection = analysis_dict[\"batch_particle_connection\"][\"connect\"]\n",
    "    batch_particle_shuffle = analysis_dict[\"batch_particle_connection\"][\"shuffle\"]\n",
    "    \n",
    "    if batch_size == None:\n",
    "        batch_size = len(X_train)\n",
    "\n",
    "    n_cols = X_train.shape[1]\n",
    "    \n",
    "    if disjoint_batch:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        batch_indices = np.cumsum([0] + list(np.ones(num_batches) * batch_size))\n",
    "        batch_indices[-1] = n\n",
    "    else:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        last_batch_size = n % batch_size\n",
    "        \n",
    "    if batch_particle_connection:\n",
    "        batch_particle_dict = {}\n",
    "        batch_particle_indices = np.arange(particles) + 1\n",
    "        np.random.shuffle(batch_particle_indices)\n",
    "        if particles == num_batches:\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "        elif particles > num_batches:\n",
    "            base_batches = particles // num_batches\n",
    "            add_batches = particles % num_batches\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "            for i in range(add_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "        elif num_batches > particles:\n",
    "            num_reps = int(np.ceil(num_batches / particles))\n",
    "            particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "    else:\n",
    "        batch_particle_dict = None\n",
    "\n",
    "    model_dict = {}\n",
    "    weights_dict = {}\n",
    "    y_pred_dict = {}\n",
    "    jacobian_dict = {}\n",
    "    weights_vector_dict = {}\n",
    "    train_mse_dict = {}\n",
    "    test_mse_dict = {}\n",
    "\n",
    "    # init_model already has weights and biases following the Glorot distribution\n",
    "    # it can already be used to predict and evaluate, but it is very bad\n",
    "    # only used to determine shapes and shape_elements via its weights\n",
    "    init_model = nn_model_structure(layers = layers,\n",
    "                                    neurons = neurons,\n",
    "                                    n_cols = n_cols,\n",
    "                                    classification = False)\n",
    "    init_model = nn_model_compile(init_model,\n",
    "                                  optimizer = \"sgd\")\n",
    "    weights = init_model.get_weights()\n",
    "    # shape contains the shapes of the weight matrices and bias vectors as a list of arrays\n",
    "    shapes = [np.array(params.shape) for params in weights]\n",
    "    # shape_elements contains the indices of the weights as a vector and tells where to cut\n",
    "    shape_elements = np.cumsum([0] + [np.prod(shape) for shape in shapes])\n",
    "\n",
    "    for i in range(particles):\n",
    "        \n",
    "        # just an initial model with the correct structure regarding neurons, layers, activation functions, Glorot initialization\n",
    "        model = nn_model_structure(layers = layers,\n",
    "                                   neurons = neurons,\n",
    "                                   n_cols = n_cols,\n",
    "                                   classification = False)\n",
    "        model = nn_model_compile(model,\n",
    "                                 optimizer = \"sgd\")\n",
    "        # for every particle write the model in a dictionary\n",
    "        model_dict[\"model_{}\".format(str(i+1))] = model\n",
    "\n",
    "        # for every particles write the weights and biases in a dictionary\n",
    "        weights_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .get_weights()\n",
    "        \n",
    "        # for every particle write the predictions on the training batches in a dictionary\n",
    "        y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .predict(X_train[:batch_size,:])\n",
    "\n",
    "        # for every particle write the Jacobian in a dictionary\n",
    "        jacobian_dict[\"model_{}\".format(str(i+1))] = 1/len(y_train[:batch_size]) * (-2)*(y_train[:batch_size] - y_pred_dict[\"model_{}\".format(str(i+1))].ravel())\n",
    "\n",
    "        train_mse_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        test_mse_dict[\"model_{}\".format(str(i+1))] = []\n",
    "\n",
    "    # mean_model as the model with the mean of the weights of all particle models\n",
    "    mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "    mean_model = init_model\n",
    "    mean_model.set_weights(mean_weights)\n",
    "\n",
    "    mean_model_train_mse = np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1])\n",
    "    mean_model_test_mse = np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1])\n",
    "\n",
    "    # loop over all epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # early stopping\n",
    "        if early_stopping:\n",
    "            if epoch == 0:\n",
    "                train_mse_old = 0\n",
    "                test_mse_old = 0\n",
    "            else:\n",
    "                train_mse_new = mean_model_train_mse[epoch]\n",
    "                test_mse_new = mean_model_test_mse[epoch]\n",
    "                if np.absolute(test_mse_new - test_mse_old) <= early_stopping_diff and np.absolute(train_mse_new - train_mse_old) <= early_stopping_diff:\n",
    "                    print(\"STOP: Early Stopping after epoch {} because improvement in training MSE is only {} and in test mse only {}.\"\\\n",
    "                                                                         .format(epoch, train_mse_new - train_mse_old, test_mse_new - test_mse_old))\n",
    "                    break\n",
    "                test_mse_old = test_mse_new\n",
    "                                                                                    \n",
    "        # shuffle the data\n",
    "        if shuffle:\n",
    "            indices = y_train.sample(frac=1).index\n",
    "        else:\n",
    "            indices = y_train.index\n",
    "            \n",
    "        if disjoint_batch:\n",
    "            X_batches = [np.array(X_train)[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [y_train.iloc[indices].reset_index(drop = True)[int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [np.array(i) for i in y_batches]\n",
    "        else:\n",
    "            if last_batch_size != 0:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches-1)]\n",
    "                indices.append(np.random.choice(len(X_train), size = last_batch_size, replace = True))\n",
    "            else:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches)]\n",
    "            X_batches = [X_train[indices[i]] for i in range(len(indices))]\n",
    "            y_batches = [y_train[indices[i]] for i in range(len(indices))]\n",
    "            \n",
    "        if batch_particle_connection and batch_particle_shuffle == \"permute\":\n",
    "            shuffled_indices = np.hstack(list(batch_particle_dict.values()))\n",
    "            np.random.shuffle(shuffled_indices)\n",
    "            batch_particle_values = list(batch_particle_dict.values())\n",
    "            for i in range(len(batch_particle_values)):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = shuffled_indices[i*len(batch_particle_values[i]):(i+1)*len(batch_particle_values[i])]\n",
    "        if batch_particle_connection and (batch_particle_shuffle == \"particle\" or batch_particle_shuffle == \"full\"):\n",
    "            batch_particle_dict = {}\n",
    "            batch_particle_indices = np.arange(particles) + 1\n",
    "            np.random.shuffle(batch_particle_indices)\n",
    "            if particles == num_batches:\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "            elif particles > num_batches:\n",
    "                base_batches = particles // num_batches\n",
    "                add_batches = particles % num_batches\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                    batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "                for i in range(add_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "            elif num_batches > particles:\n",
    "                num_reps = int(np.ceil(num_batches / particles))\n",
    "                particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "        if batch_particle_connection and (batch_particle_shuffle == \"batch\" or batch_particle_shuffle == \"full\"):\n",
    "            indices = np.arange(n)\n",
    "            np.random.shuffle(indices)\n",
    "            if disjoint_batch:\n",
    "                X_batches = [X_train[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "                y_batches = [y_train[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            else:\n",
    "                if last_batch_size != 0:\n",
    "                    indices = [np.random.choice(X_train.shape[0], size = batch_size, replace = True) for i in range(num_batches-1)]\n",
    "                    indices.append(np.random.choice(X_train.shape[0], size = last_batch_size, replace = True))\n",
    "                else:\n",
    "                    indices = [np.random.choice(X_train.shape[0], size = batch_size, replace = True) for i in range(num_batches)]\n",
    "                X_batches = [X_train[indices[i]] for i in range(len(indices))]\n",
    "                y_batches = [y_train[indices[i]] for i in range(len(indices))]\n",
    "               \n",
    "        # loop over all batches\n",
    "        for b in range(num_batches):\n",
    "            batch_particles = []\n",
    "            for i in range(particles):\n",
    "                if batch_particle_connection: \n",
    "                    if num_batches == particles or num_batches > particles:\n",
    "                        if batch_particle_dict[\"batch_{}\".format(str(b+1))] != i+1:\n",
    "                            continue\n",
    "                    else:\n",
    "                        if i+1 not in batch_particle_dict[\"batch_{}\".format(str(b+1))]:\n",
    "                            continue\n",
    "                if batch_particle_connection:\n",
    "                    batch_particles.append(i+1)\n",
    "\n",
    "                # set new weights for model\n",
    "                model_dict[\"model_{}\".format(str(i+1))].set_weights(weights_dict[\"model_{}\".format(str(i+1))])\n",
    "\n",
    "                # for every particle write the predictions on the training batches in a dictionary\n",
    "                y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                .predict(X_batches[b])\n",
    "                \n",
    "                # for every particle write the Jacobian in a dictionary\n",
    "                jacobian_dict[\"model_{}\".format(str(i+1))] = 1/len(y_batches[b]) * (-2)*(y_batches[b] - y_pred_dict[\"model_{}\".format(str(i+1))].ravel())\n",
    "\n",
    "            if not batch_particle_connection:        \n",
    "                # compute the mean of the predictions\n",
    "                y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "                # compute the matrix D elementwise\n",
    "                d = np.zeros(shape = (particles, particles))\n",
    "                for k in range(particles):\n",
    "                    y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                    for j in range(particles):\n",
    "                        d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_dict[\"model_{}\".format(str(j+1))])\n",
    "                d = np.transpose(d)\n",
    "\n",
    "                # compute the scalar h_t\n",
    "                h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "                # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "                for i in range(particles):\n",
    "                    weights_array = np.array([])\n",
    "                    for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                        weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                    weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "                # matrix with particle parameters as row vectors\n",
    "                weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "                # compute the matrix with the updates for each particle\n",
    "                weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "                for i in range(particles):\n",
    "                    # write the updates back into the dictionary\n",
    "                    weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                    # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                    for l in range(len(shape_elements)-1):\n",
    "                        start = shape_elements[l]\n",
    "                        end = shape_elements[l+1]\n",
    "                        weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                \n",
    "        if batch_particle_connection:\n",
    "            # compute the mean of the predictions\n",
    "            y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "            # compute the matrix D elementwise\n",
    "            d = np.zeros(shape = (particles, particles))\n",
    "            for k in range(particles):\n",
    "                y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                for j in range(particles):\n",
    "                    d[k][j] = np.dot(y_pred_centered.ravel(), jacobian_dict[\"model_{}\".format(str(j+1))])\n",
    "            d = np.transpose(d)\n",
    "\n",
    "            # compute the scalar h_t\n",
    "            h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "            # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "            for i in range(particles):\n",
    "                weights_array = np.array([])\n",
    "                for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                    weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "            # matrix with particle parameters as row vectors\n",
    "            weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "            # compute the matrix with the updates for each particle\n",
    "            weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "            for i in range(particles):\n",
    "                # write the updates back into the dictionary\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                for l in range(len(shape_elements)-1):\n",
    "                    start = shape_elements[l]\n",
    "                    end = shape_elements[l+1]\n",
    "                    weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                                                            \n",
    "        for i in range(particles):\n",
    "            # for every particle write the training MSE of the current iteration in a dictionary\n",
    "            train_mse_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_train, y_train, verbose = 0)[1])\n",
    "\n",
    "            # for every particle write the test MSE of the current iteration in a dictionary\n",
    "            test_mse_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_test, y_test, verbose = 0)[1])\n",
    "\n",
    "        # update the mean_model\n",
    "        mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "        mean_model.set_weights(mean_weights)\n",
    "\n",
    "        mean_model_train_mse = np.append(mean_model_train_mse, np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1]))\n",
    "        mean_model_test_mse = np.append(mean_model_test_mse, np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1])) \n",
    "\n",
    "        if verbose == 1:\n",
    "            print(\"Epoch {}. Training MSE: {}, Test MSE: {}.\".format(epoch+1,\n",
    "                                                                     np.round(mean_model_train_mse[-1], 3),\n",
    "                                                                     np.round(mean_model_test_mse[-1], 3)))\n",
    "            \n",
    "    mean_model.history.history = {\"mse\": mean_model_train_mse[1:],\n",
    "                                  \"val_mse\": mean_model_test_mse[1:]}\n",
    "        \n",
    "    if save_all:\n",
    "        param_dict = param_to_dict(X_train,\n",
    "                                   X_test,\n",
    "                                   y_train,\n",
    "                                   y_test,\n",
    "                                   layers,\n",
    "                                   neurons,\n",
    "                                   particles,\n",
    "                                   epochs,\n",
    "                                   batch_size,\n",
    "                                   h_0,\n",
    "                                   delta,\n",
    "                                   epsilon,\n",
    "                                   randomization,\n",
    "                                   shuffle,\n",
    "                                   early_stopping,\n",
    "                                   early_stopping_diff\n",
    "                                   )\n",
    "        results_dict = results_to_dict(mean_model_train_mse,\n",
    "                                       mean_model_test_mse,\n",
    "                                       train_mse_dict,\n",
    "                                       test_mse_dict,\n",
    "                                       weights_dict,\n",
    "                                       y_pred_dict,\n",
    "                                       False\n",
    "                                       )\n",
    "\n",
    "        saving_dict = {}\n",
    "        saving_dict[\"parameters\"] = param_dict\n",
    "        saving_dict[\"results\"] = results_dict\n",
    "        saving_dict[\"analysis\"] = analysis_dict\n",
    "\n",
    "        save_objects(obj_dict = saving_dict,\n",
    "                     file = file_var)\n",
    "\n",
    "        nn_save(model = mean_model,\n",
    "                path_name = file_model)\n",
    " \n",
    "    return mean_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting_dict = {\"particles\": 20,\n",
    "                \"epochs\": 10,\n",
    "                \"batch_size\": 100,    # len(X_train)\n",
    "                \"h_0\": 2,\n",
    "                \"epsilon\": 0.5,\n",
    "                \"randomization\": False,\n",
    "                \"shuffle\": True,\n",
    "                \"early_stopping\": False,\n",
    "                \"early_stopping_diff\": 0.001\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dict = {\"disjoint_batch\": True,\n",
    "                 \"batch_particle_connection\": {\"connect\": True,\n",
    "                                               \"shuffle\": \"particle\"}        # None, \"permute\", \"particle\", \"batch\", \"full\"\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training MSE: 11.288, Test MSE: 11.264.\n",
      "Epoch 2. Training MSE: 4.675, Test MSE: 4.665.\n",
      "Epoch 3. Training MSE: 2.615, Test MSE: 2.625.\n",
      "Epoch 4. Training MSE: 2.076, Test MSE: 2.095.\n",
      "Epoch 5. Training MSE: 1.323, Test MSE: 1.366.\n",
      "Epoch 6. Training MSE: 1.134, Test MSE: 1.186.\n",
      "Epoch 7. Training MSE: 0.974, Test MSE: 1.028.\n",
      "Epoch 8. Training MSE: 0.874, Test MSE: 0.953.\n",
      "Epoch 9. Training MSE: 0.847, Test MSE: 0.934.\n",
      "Epoch 10. Training MSE: 0.832, Test MSE: 0.92.\n"
     ]
    }
   ],
   "source": [
    "mean_model = enkf_regressor_analysis(X_train,\n",
    "                                     X_test,\n",
    "                                     y_train,\n",
    "                                     y_test,\n",
    "                                     layers,\n",
    "                                     neurons,\n",
    "                                     setting_dict,\n",
    "                                     analysis_dict,\n",
    "                                     save_all = False,\n",
    "                                     file_var = \"../objects/wine/wine_enkf_E{}_B{}_P{}_H{}.pckl\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                     file_model = \"../models/wine/wine_enkf_E{}_B{}_P{}_H{}.h5\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                     verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation time: 1.0021738529205322 minutes.\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "print(\"Calculation time: {} minutes.\".format((end_time - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_plot_mse(mean_model,\n",
    "            mse_mean = None,\n",
    "            start_epoch = 1,\n",
    "            savefig = False,\n",
    "            file = \"../img/wine/wine_enkf_E{}_B{}_P{}_H{}.png\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
