{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../python/architecture\\reproducible.py:14: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../python/functions\")\n",
    "sys.path.insert(2, \"../python/architecture\")\n",
    "from new_Adam import Adam_test\n",
    "import reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "class CustomModel(tensorflow.python.keras.Model):\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "        \n",
    "        y_pred = tf.Variable(self(x, training=True))  # Forward pass\n",
    "        with tf.GradientTape(watch_accessed_variables = False) as tape: # watch_acc... macht hier auch keinen Unterschied\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            tape.watch(y_pred)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, y_pred)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, y_pred))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\morit\\anaconda3\\envs\\master_thesis\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 19us/sample - loss: 0.5963 - mae: 0.6554\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 23us/sample - loss: 0.2495 - mae: 0.4048\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 20us/sample - loss: 0.2038 - mae: 0.3633\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 28us/sample - loss: 0.1998 - mae: 0.3588\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 22us/sample - loss: 0.1961 - mae: 0.3556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20cb434f888>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Construct and compile an instance of CustomModel\n",
    "inputs = tensorflow.python.keras.Input(shape=(32,))\n",
    "outputs = tensorflow.python.keras.layers.Dense(1)(inputs)\n",
    "model = CustomModel(inputs, outputs)\n",
    "model.compile(optimizer=Adam_test(), loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# Just use `fit` as usual\n",
    "x = np.random.random((1000, 32))\n",
    "y = np.random.random((1000, 1))\n",
    "model.fit(x, y, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die resultierenden Werte für den Loss und den MAE sind exakt identisch zu der Variante wie sie im Internet steht und wo statt $y_{pred}$ stets $\\textit{self.trainable_variables}$ verwendet wird. Auch jeder andere beliebige Wert für $y_{pred}$ ändert absolut nichts, egal ob $y_{pred}$ innerhalb oder außerhalb des $with$-Ausdrucks steht.\n",
    "#### => Irgendwo anders werden die normalen Parameter weiterhin benutzt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
