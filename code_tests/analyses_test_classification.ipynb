{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../python/functions\")\n",
    "sys.path.insert(2, \"../python/architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../python/architecture\\reproducible.py:14: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_prep_functions import mnist_prep\n",
    "from plotting_functions import nn_plot_acc\n",
    "from enkf_functions import enkf_classifier\n",
    "from saving_functions import load_objects, save_objects\n",
    "from model_functions import nn_load, nn_model_structure, nn_model_compile\n",
    "import reproducible\n",
    "import no_gpu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = mnist_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use samller dataset for increased speed\n",
    "X_train = X_train[:1000, :]\n",
    "X_test = X_test[:500, :]\n",
    "y_train = y_train[:1000]\n",
    "y_test = y_test[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 5\n",
    "neurons = [128, 128, 64, 32, 10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def enkf_classifier_analysis(X_train,\n",
    "                             X_test,\n",
    "                             y_train,\n",
    "                             y_test,\n",
    "                             layers,\n",
    "                             neurons,\n",
    "                             setting_dict,\n",
    "                             analysis_dict,\n",
    "                             save_all = False,\n",
    "                             file_var = \"file.pckl\",\n",
    "                             file_model = \"file.h5\",\n",
    "                             verbose = 0\n",
    "                             ):\n",
    "\n",
    "    \"\"\" Ensemble Kalman Filter algorithm analysis for classification problems.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train (np.ndarray): Training data X.\n",
    "    X_test (np.ndarray): Test data X.\n",
    "    y_train (pd.DataFrame): Training data y.\n",
    "    y_test (pd.DataFrame): Test data y.\n",
    "    layers (int): Number of layers.\n",
    "    neurons (list): Number of neurons in each layer.\n",
    "    setting_dict (dict): Dictionary containing\n",
    "        particles (int): Number of particles in the ensemble.\n",
    "        epochs (int): Number of epochs.\n",
    "        batch_size (None or int): Size of the batches. Must be between 0 and the number of observations in the training set.\n",
    "        h_0 (int or float): Starting step size.\n",
    "        delta (float): Constant for numerical stability in the jacobian.\n",
    "        epsilon (float): Constant for numerical stability in the step size.\n",
    "        randomization (bool): Whether or not to add noise to the particles and randomize them around their mean.\n",
    "        shuffle (bool): Whether or not to shuffle the data prior to each epoch.\n",
    "        early_stopping (bool): Whether or not to stop the calculation when the changes get small.\n",
    "        early_stopping_diff (bool): Minimum change before early stopping is applied.\n",
    "    analysis_dict (dict): Dictionary containing\n",
    "        disjoint_batch (bool): Whether or not to use disjoint batches. If False then each batch is sampled with replacement.\n",
    "        multiple_updates_same_batch (int or None): Number of consecutive updates on the same batch.\n",
    "        batch_particle_connection (dict): Dictionary containing\n",
    "            connect (bool): Whether or not to connect particles and batches.\n",
    "            shuffle (str or None): Whether or not and how to shuffle the connection. None = no shuffle. \"batch\" = shuffle the batch for fixed particle sets. \"full\" = shuffle the particle sets and their corresponding batch.\n",
    "            update_all (bool): Whether or not to update after all particles have seen some data.\n",
    "        batch_evaluation (dict): Dictionary containing\n",
    "            mean_model (bool): Whether or not evaluate the mean model after every batch on that batch. Warning: the computational costs are very high.\n",
    "            particles (bool): Whether or not evaluate every particle model after every batch on that batch. Warning: the computational costs are incredibly high.\n",
    "            file (str): Path and name of the file to save evaluations into \n",
    "    save_all (bool): Whether or not to save all important variables and models.\n",
    "    file_var (str): Path and name of the file to save variables into.\n",
    "    file_model (str): Path and name of the file to save the final model into.\n",
    "    verbose (int): If 0, then don't print anything throughout the training process. If 1, then print training and test accuracy after each epoch.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    mean_model (tensorflow.python.keras.engine.sequential.Sequential): The final model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    particles = setting_dict[\"particles\"]\n",
    "    epochs = setting_dict[\"epochs\"]\n",
    "    batch_size = setting_dict[\"batch_size\"]\n",
    "    h_0 = setting_dict[\"h_0\"]\n",
    "    delta = setting_dict[\"delta\"]\n",
    "    epsilon = setting_dict[\"epsilon\"]\n",
    "    randomization = setting_dict[\"randomization\"]\n",
    "    shuffle = setting_dict[\"shuffle\"]\n",
    "    early_stopping = setting_dict[\"early_stopping\"]\n",
    "    early_stopping_diff = setting_dict[\"early_stopping_diff\"]\n",
    "    \n",
    "    disjoint_batch = analysis_dict[\"disjoint_batch\"]\n",
    "    batch_evaluation = analysis_dict[\"batch_evaluation\"]\n",
    "    batch_particle_connection = analysis_dict[\"batch_particle_connection\"][\"connect\"]\n",
    "    batch_particle_shuffle = analysis_dict[\"batch_particle_connection\"][\"shuffle\"]\n",
    "    update_all = analysis_dict[\"batch_particle_connection\"][\"update_all\"]\n",
    "    mult_updates_batch = analysis_dict[\"multiple_updates_same_batch\"]\n",
    "    \n",
    "    if mult_updates_batch is None:\n",
    "        mult_updates_batch = 1\n",
    "\n",
    "    if batch_size == None:\n",
    "        batch_size = len(X_train)\n",
    "\n",
    "    n_cols = X_train.shape[1]\n",
    "    \n",
    "    if disjoint_batch:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        batch_indices = np.cumsum([0] + list(np.ones(num_batches) * batch_size))\n",
    "        batch_indices[-1] = n\n",
    "    else:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        last_batch_size = n % batch_size\n",
    "        \n",
    "    if batch_particle_connection:\n",
    "        batch_particle_dict = {}\n",
    "        batch_particle_indices = np.arange(particles) + 1\n",
    "        np.random.shuffle(batch_particle_indices)\n",
    "        if particles == num_batches:\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "        elif particles > num_batches:\n",
    "            base_batches = particles // num_batches\n",
    "            add_batches = particles % num_batches\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "            for i in range(add_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "        elif num_batches > particles:\n",
    "            num_reps = int(np.ceil(num_batches / particles))\n",
    "            particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "    else:\n",
    "        batch_particle_dict = None\n",
    "\n",
    "    model_dict = {}\n",
    "    weights_dict = {}\n",
    "    y_pred_dict = {}\n",
    "    jacobian_dict = {}\n",
    "    weights_vector_dict = {}\n",
    "    train_acc_dict = {}\n",
    "    test_acc_dict = {}\n",
    "    \n",
    "    if batch_evaluation[\"mean_model\"]:\n",
    "        train_batch_acc_mean_dict = {}\n",
    "    if batch_evaluation[\"particles\"]:\n",
    "        train_batch_acc_particle_dict = {}\n",
    "\n",
    "    # init_model already has weights and biases following the Glorot distribution\n",
    "    # it can already be used to predict and evaluate, but it is very bad\n",
    "    # only used to determine shapes and shape_elements via its weights\n",
    "    init_model = nn_model_structure(layers = layers,\n",
    "                                    neurons = neurons,\n",
    "                                    n_cols = n_cols,\n",
    "                                    classification = True)\n",
    "    init_model = nn_model_compile(init_model,\n",
    "                                  optimizer = \"sgd\")\n",
    "    weights = init_model.get_weights()\n",
    "    # shape contains the shapes of the weight matrices and bias vectors as a list of arrays\n",
    "    shapes = [np.array(params.shape) for params in weights]\n",
    "    # shape_elements contains the indices of the weights as a vector and tells where to cut\n",
    "    shape_elements = np.cumsum([0] + [np.prod(shape) for shape in shapes])\n",
    "\n",
    "    for i in range(particles):\n",
    "        \n",
    "        # just an initial model with the correct structure regarding neurons, layers, activation functions, Glorot initialization\n",
    "        model = nn_model_structure(layers = layers,\n",
    "                                   neurons = neurons,\n",
    "                                   n_cols = n_cols,\n",
    "                                   classification = True)\n",
    "        model = nn_model_compile(model,\n",
    "                                 optimizer = \"sgd\")\n",
    "        # for every particle write the model in a dictionary\n",
    "        model_dict[\"model_{}\".format(str(i+1))] = model\n",
    "\n",
    "        # for every particles write the weights and biases in a dictionary\n",
    "        weights_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .get_weights()\n",
    "        \n",
    "        # for every particle write the predictions on the training batches in a dictionary\n",
    "        y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .predict(X_train)\n",
    "\n",
    "        # for every particle write the Jacobian in a dictionary\n",
    "        jacobian_dict[\"model_{}\".format(str(i+1))] = (-1) * np.multiply(np.array(y_train),\n",
    "                                                                                 np.array(1 / (y_pred_dict[\"model_{}\".format(str(i+1))] + delta)))\n",
    "\n",
    "        train_acc_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        test_acc_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        \n",
    "        if batch_evaluation[\"particles\"]:\n",
    "            train_batch_ac_particle_dict[\"particle_{}\".format(str(i+1))] = {}\n",
    "\n",
    "    # mean_model as the model with the mean of the weights of all particle models\n",
    "    mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "    mean_model = init_model\n",
    "    mean_model.set_weights(mean_weights)\n",
    "\n",
    "    mean_model_train_acc = np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1])\n",
    "    mean_model_test_acc = np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1])\n",
    "\n",
    "    # loop over all epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # early stopping\n",
    "        if early_stopping:\n",
    "            if epoch == 0:\n",
    "                train_acc_old = 0\n",
    "                test_acc_old = 0\n",
    "            else:\n",
    "                train_acc_new = mean_model_train_acc[epoch]\n",
    "                test_acc_new = mean_model_test_acc[epoch]\n",
    "                if np.absolute(test_acc_new - test_acc_old) <= early_stopping_diff and np.absolute(train_acc_new - train_acc_old) <= early_stopping_diff:\n",
    "                    print(\"STOP: Early Stopping after epoch {} because improvement in training accuracy is only {} and in test acc only {}.\"\\\n",
    "                                                                         .format(epoch, train_acc_new - train_acc_old, test_acc_new - test_acc_old))\n",
    "                    break\n",
    "                test_mse_old = test_mse_new\n",
    "                                                                            \n",
    "        # shuffle the data\n",
    "        if shuffle:\n",
    "            indices = y_train.sample(frac=1).index\n",
    "        else:\n",
    "            indices = y_train.index\n",
    "            \n",
    "        if disjoint_batch:\n",
    "            X_batches = [np.array(X_train)[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [y_train.iloc[indices].reset_index(drop = True)[int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [np.array(i) for i in y_batches]\n",
    "        else:\n",
    "            if last_batch_size != 0:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches-1)]\n",
    "                indices.append(np.random.choice(len(X_train), size = last_batch_size, replace = True))\n",
    "            else:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches)]\n",
    "            X_batches = [X_train[indices[i]] for i in range(len(indices))]\n",
    "            y_batches = [y_train[indices[i]] for i in range(len(indices))]\n",
    "            \n",
    "        if batch_particle_connection and batch_particle_shuffle == \"batch\":\n",
    "            shuffled_indices = np.hstack(list(batch_particle_dict.values()))\n",
    "            np.random.shuffle(shuffled_indices)\n",
    "            batch_particle_values = list(batch_particle_dict.values())\n",
    "            for i in range(len(batch_particle_values)):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = shuffled_indices[i*len(batch_particle_values[i]):(i+1)*len(batch_particle_values[i])]\n",
    "        elif batch_particle_connection and batch_particle_shuffle == \"full\":\n",
    "            batch_particle_dict = {}\n",
    "            batch_particle_indices = np.arange(particles) + 1\n",
    "            np.random.shuffle(batch_particle_indices)\n",
    "            if particles == num_batches:\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "            elif particles > num_batches:\n",
    "                base_batches = particles // num_batches\n",
    "                add_batches = particles % num_batches\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                    batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "                for i in range(add_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "            elif num_batches > particles:\n",
    "                num_reps = int(np.ceil(num_batches / particles))\n",
    "                particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "            \n",
    "        if batch_evaluation[\"mean_model\"]:\n",
    "            train_batch_acc_mean_dict[\"Epoch_{}\".format(str(epoch+1))] = {}\n",
    "        if batch_evaluation[\"particles\"]:\n",
    "            train_batch_acc_particle_dict[\"particle_{}\".format(str(i+1))][\"Epoch_{}\".format(str(epoch+1))] = {}\n",
    "                    \n",
    "        # loop over all batches\n",
    "        for b in range(num_batches):\n",
    "            for mult_updates in range(mult_updates_batch):\n",
    "                batch_particles = []\n",
    "                y_pred_batch_dict = {}\n",
    "                jacobian_batch_dict = {}\n",
    "                for i in range(particles):\n",
    "                    if batch_particle_connection: \n",
    "                        if num_batches == particles or num_batches > particles:\n",
    "                            if batch_particle_dict[\"batch_{}\".format(str(b+1))] != i+1:\n",
    "                                continue\n",
    "                        else:\n",
    "                            if i+1 not in batch_particle_dict[\"batch_{}\".format(str(b+1))]:\n",
    "                                continue\n",
    "                    if batch_particle_connection:\n",
    "                        batch_particles.append(i+1)\n",
    "                    \n",
    "                    # set new weights for model\n",
    "                    model_dict[\"model_{}\".format(str(i+1))].set_weights(weights_dict[\"model_{}\".format(str(i+1))])\n",
    "\n",
    "                    # for every particle write the predictions on the training batches in a dictionary\n",
    "                    y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                    .predict(X_batches[b])\n",
    "                    y_pred_batch_dict[\"model_{}\".format(i+1)] = y_pred_dict[\"model_{}\".format(i+1)]\n",
    "\n",
    "                    # for every particle write the Jacobian in a dictionary\n",
    "                    jacobian_dict[\"model_{}\".format(str(i+1))] = (-1) * np.multiply(np.array(y_batches[b]),\n",
    "                                                                                np.array(1 / (y_pred_dict[\"model_{}\".format(str(i+1))] + delta)))\n",
    "                    jacobian_batch_dict[\"model_{}\".format(i+1)] = jacobian_dict[\"model_{}\".format(i+1)]\n",
    "\n",
    "                    if batch_evaluation[\"mean_model\"]:\n",
    "                        train_batch_acc_mean_dict[\"Epoch_{}\".format(str(epoch+1))][\"Batch_{}\".format(str(b+1))] = mean_model.evaluate(X_batches[b], y_batches[b], verbose = 0)[1]\n",
    "                    if batch_evaluation[\"particles\"]:\n",
    "                        train_batch_acc_particle_dict[\"particle_{}\".format(str(i+1))][\"Epoch_{}\".format(str(epoch+1))][\"Batch_{}\".format(str(b+1))] = model_dict[\"model_{}\".format(str(i+1))].evaluate(X_batches[b], y_batches[b], verbose = 0)[1]\n",
    "\n",
    "                        \n",
    "                if not batch_particle_connection:        \n",
    "                    # compute the mean of the predictions\n",
    "                    y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "                    # compute the matrix D elementwise\n",
    "                    d = np.zeros(shape = (particles, particles))\n",
    "                    for k in range(particles):\n",
    "                        y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                        for j in range(particles):\n",
    "                            d[k][j] = np.sum(np.multiply(y_pred_centered, jacobian_dict[\"model_{}\".format(str(j+1))]))\n",
    "                    d = np.transpose(d)\n",
    "\n",
    "                    # compute the scalar h_t\n",
    "                    h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "                    # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "                    for i in range(particles):\n",
    "                        weights_array = np.array([])\n",
    "                        for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                            weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "                    # matrix with particle parameters as row vectors\n",
    "                    weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "                    # compute the matrix with the updates for each particle\n",
    "                    weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "                    for i in range(particles):\n",
    "                        # write the updates back into the dictionary\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                        # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                        for l in range(len(shape_elements)-1):\n",
    "                            start = shape_elements[l]\n",
    "                            end = shape_elements[l+1]\n",
    "                            weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                \n",
    "                elif batch_particle_connection and not update_all:\n",
    "                    # compute the mean of the predictions\n",
    "                    y_pred_mean = np.mean(list(y_pred_batch_dict.values()), axis = 0)\n",
    "                    \n",
    "                    # compute the matrix D elementwise\n",
    "                    d = np.zeros(shape = (len(batch_particles), len(batch_particles)))\n",
    "                    for k in range(len(batch_particles)):\n",
    "                        y_pred_centered = y_pred_batch_dict[\"model_{}\".format(batch_particles[k])] - y_pred_mean\n",
    "                        for j in range(len(batch_particles)):\n",
    "                            d[k][j] = np.sum(np.multiply(y_pred_centered, jacobian_dict[\"model_{}\".format(batch_particles[j])]))\n",
    "                    d = np.transpose(d)\n",
    "\n",
    "                    # compute the scalar h_t\n",
    "                    h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "                    # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "                    for i in range(particles):\n",
    "                        weights_array = np.array([])\n",
    "                        for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                            weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "                    # matrix with particle parameters as row vectors\n",
    "                    weights_vector_batch_dict = {}\n",
    "                    for i in range(len(batch_particles)):\n",
    "                        weights_vector_batch_dict[\"model_{}\".format(batch_particles[i])] = weights_vector_dict[\"model_{}\".format(batch_particles[i])]\n",
    "                        weights_all_ptcls = np.array(list(weights_vector_batch_dict.values()))\n",
    "\n",
    "                    # compute the matrix with the updates for each particle\n",
    "                    weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "                    for i in range(len(batch_particles)):\n",
    "                        # write the updates back into the dictionary\n",
    "                        weights_vector_dict[\"model_{}\".format(batch_particles[i])] = weights_all_ptcls[i]\n",
    "                        # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                        for l in range(len(shape_elements)-1):\n",
    "                            start = shape_elements[l]\n",
    "                            end = shape_elements[l+1]\n",
    "                            weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "\n",
    "        if batch_particle_connection and update_all:\n",
    "            # compute the mean of the predictions\n",
    "            y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "            # compute the matrix D elementwise\n",
    "            d = np.zeros(shape = (particles, particles))\n",
    "            for k in range(particles):\n",
    "                y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                for j in range(particles):\n",
    "                    d[k][j] = np.sum(np.multiply(y_pred_centered, jacobian_dict[\"model_{}\".format(str(j+1))]))\n",
    "            d = np.transpose(d)\n",
    "\n",
    "            # compute the scalar h_t\n",
    "            h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "            # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "            for i in range(particles):\n",
    "                weights_array = np.array([])\n",
    "                for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                    weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "            # matrix with particle parameters as row vectors\n",
    "            weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "            # compute the matrix with the updates for each particle\n",
    "            weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "            for i in range(particles):\n",
    "                # write the updates back into the dictionary\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                for l in range(len(shape_elements)-1):\n",
    "                    start = shape_elements[l]\n",
    "                    end = shape_elements[l+1]\n",
    "                    weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                                                            \n",
    "        for i in range(particles):\n",
    "            # for every particle write the training accuracy of the current iteration in a dictionary\n",
    "            train_acc_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_train, y_train, verbose = 0)[1])\n",
    "\n",
    "            # for every particle write the test accuracy of the current iteration in a dictionary\n",
    "            test_acc_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_test, y_test, verbose = 0)[1])\n",
    "\n",
    "        # update the mean_model\n",
    "        mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "        mean_model.set_weights(mean_weights)\n",
    "\n",
    "        mean_model_train_acc = np.append(mean_model_train_acc, np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1]))\n",
    "        mean_model_test_acc = np.append(mean_model_test_acc, np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1]))\n",
    "        \n",
    "        if verbose == 1:\n",
    "            print(\"Epoch {}. Training Accuracy: {}, Test Accuracy: {}.\".format(epoch+1,\n",
    "                                                                               np.round(mean_model_train_acc[-1], 3),\n",
    "                                                                               np.round(mean_model_test_acc[-1], 3)))\n",
    "            \n",
    "    mean_model.history.history = {\"accuracy\": mean_model_train_acc[1:],\n",
    "                                  \"val_accuracy\": mean_model_test_acc[1:]}\n",
    "        \n",
    "    if save_all:\n",
    "        param_dict = param_to_dict(X_train,\n",
    "                                   X_test,\n",
    "                                   y_train,\n",
    "                                   y_test,\n",
    "                                   layers,\n",
    "                                   neurons,\n",
    "                                   particles,\n",
    "                                   epochs,\n",
    "                                   batch_size,\n",
    "                                   h_0,\n",
    "                                   delta,\n",
    "                                   epsilon,\n",
    "                                   randomization,\n",
    "                                   shuffle,\n",
    "                                   early_stopping,\n",
    "                                   early_stopping_diff\n",
    "                                   )\n",
    "        results_dict = results_to_dict(mean_model_train_mse,\n",
    "                                       mean_model_test_mse,\n",
    "                                       train_mse_dict,\n",
    "                                       test_mse_dict,\n",
    "                                       weights_dict,\n",
    "                                       y_pred_dict,\n",
    "                                       True\n",
    "                                       )\n",
    "\n",
    "        saving_dict = {}\n",
    "        saving_dict[\"parameters\"] = param_dict\n",
    "        saving_dict[\"results\"] = results_dict\n",
    "        saving_dict[\"analysis\"] = analysis_dict\n",
    "\n",
    "        save_objects(obj_dict = saving_dict,\n",
    "                     file = file_var)\n",
    "\n",
    "        nn_save(model = mean_model,\n",
    "                path_name = file_model)\n",
    "        \n",
    "        if batch_evaluation[\"mean_model\"]:\n",
    "            full_dict = {}\n",
    "            full_dict[\"mean_model\"] = train_batch_acc_mean_dict\n",
    "            if batch_evaluation[\"particles\"]:\n",
    "                full_dict[\"particles\"] = train_batch_acc_particle_dict\n",
    "            save_objects(obj_dict = full_dict,\n",
    "                         file = batch_evaluation[\"file\"])\n",
    "        elif batch_evaluation[\"particles\"]:\n",
    "            full_dict = {}\n",
    "            full_dict[\"particles\"] = train_batch_acc_particle_dict\n",
    "            if batch_evaluation[\"mean_model\"]:\n",
    "                full_dict[\"mean_model\"] = train_batch_acc_mean_dict\n",
    "            save_objects(obj_dict = full_dict,\n",
    "                         file = batch_evaluation[\"file\"])\n",
    " \n",
    "    return mean_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enkf_classifier_analysis(X_train,\n",
    "                             X_test,\n",
    "                             y_train,\n",
    "                             y_test,\n",
    "                             layers,\n",
    "                             neurons,\n",
    "                             setting_dict,\n",
    "                             analysis_dict,\n",
    "                             save_all = False,\n",
    "                             file_var = \"file.pckl\",\n",
    "                             file_model = \"file.h5\",\n",
    "                             verbose = 0\n",
    "                             ):\n",
    "\n",
    "    \"\"\" Ensemble Kalman Filter algorithm analysis for classification problems.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train (np.ndarray): Training data X.\n",
    "    X_test (np.ndarray): Test data X.\n",
    "    y_train (pd.DataFrame): Training data y.\n",
    "    y_test (pd.DataFrame): Test data y.\n",
    "    layers (int): Number of layers.\n",
    "    neurons (list): Number of neurons in each layer.\n",
    "    setting_dict (dict): Dictionary containing\n",
    "        particles (int): Number of particles in the ensemble.\n",
    "        epochs (int): Number of epochs.\n",
    "        batch_size (None or int): Size of the batches. Must be between 0 and the number of observations in the training set.\n",
    "        h_0 (int or float): Starting step size.\n",
    "        delta (float): Constant for numerical stability in the jacobian.\n",
    "        epsilon (float): Constant for numerical stability in the step size.\n",
    "        shuffle (bool): Whether or not to shuffle the data prior to each epoch.\n",
    "        early_stopping (float or None): Minimum change before early stopping is applied.\n",
    "    analysis_dict (dict): Dictionary containing\n",
    "        disjoint_batch (bool): Whether or not to use disjoint batches. If False then each batch is sampled with replacement.\n",
    "        batch_particle_connection (dict): Dictionary containing\n",
    "            connect (bool): Whether or not to connect particles and batches.\n",
    "            shuffle (str or None): Whether or not and how to shuffle the connection. None = no shuffle. \"permute\" = change the allocation of the existing batches and particle sets. \"particle\" = shuffle the particle sets for fixed batches. \"batch\" = shuffle the batch for fixed particle sets. \"full\" = shuffle the particle sets and their corresponding batch.\n",
    "    save_all (bool): Whether or not to save all important variables and models.\n",
    "    file_var (str): Path and name of the file to save variables into.\n",
    "    file_model (str): Path and name of the file to save the final model into.\n",
    "    verbose (int): If 0, then don't print anything throughout the training process. If 1, then print training and test accuracy after each epoch.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    mean_model (tensorflow.python.keras.engine.sequential.Sequential): The final model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    particles = setting_dict[\"particles\"]\n",
    "    epochs = setting_dict[\"epochs\"]\n",
    "    batch_size = setting_dict[\"batch_size\"]\n",
    "    h_0 = setting_dict[\"h_0\"]\n",
    "    delta = setting_dict[\"delta\"]\n",
    "    epsilon = setting_dict[\"epsilon\"]\n",
    "    shuffle = setting_dict[\"shuffle\"]\n",
    "    early_stopping = setting_dict[\"early_stopping\"]\n",
    "    \n",
    "    disjoint_batch = analysis_dict[\"disjoint_batch\"]\n",
    "    batch_particle_connection = analysis_dict[\"batch_particle_connection\"][\"connect\"]\n",
    "    batch_particle_shuffle = analysis_dict[\"batch_particle_connection\"][\"shuffle\"]\n",
    "\n",
    "    if batch_size == None:\n",
    "        batch_size = len(X_train)\n",
    "\n",
    "    n_cols = X_train.shape[1]\n",
    "    \n",
    "    if disjoint_batch:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        batch_indices = np.cumsum([0] + list(np.ones(num_batches) * batch_size))\n",
    "        batch_indices[-1] = n\n",
    "    else:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        last_batch_size = n % batch_size\n",
    "        \n",
    "    if batch_particle_connection:\n",
    "        batch_particle_dict = {}\n",
    "        batch_particle_indices = np.arange(particles) + 1\n",
    "        np.random.shuffle(batch_particle_indices)\n",
    "        if particles == num_batches:\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "        elif particles > num_batches:\n",
    "            base_batches = particles // num_batches\n",
    "            add_batches = particles % num_batches\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "            for i in range(add_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "        elif num_batches > particles:\n",
    "            num_reps = int(np.ceil(num_batches / particles))\n",
    "            particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "    else:\n",
    "        batch_particle_dict = None\n",
    "\n",
    "    model_dict = {}\n",
    "    weights_dict = {}\n",
    "    y_pred_dict = {}\n",
    "    jacobian_dict = {}\n",
    "    weights_vector_dict = {}\n",
    "    train_acc_dict = {}\n",
    "    test_acc_dict = {}\n",
    "    \n",
    "    # init_model already has weights and biases following the Glorot distribution\n",
    "    # it can already be used to predict and evaluate, but it is very bad\n",
    "    # only used to determine shapes and shape_elements via its weights\n",
    "    init_model = nn_model_structure(layers = layers,\n",
    "                                    neurons = neurons,\n",
    "                                    n_cols = n_cols,\n",
    "                                    classification = True)\n",
    "    init_model = nn_model_compile(init_model,\n",
    "                                  optimizer = \"sgd\")\n",
    "    weights = init_model.get_weights()\n",
    "    # shape contains the shapes of the weight matrices and bias vectors as a list of arrays\n",
    "    shapes = [np.array(params.shape) for params in weights]\n",
    "    # shape_elements contains the indices of the weights as a vector and tells where to cut\n",
    "    shape_elements = np.cumsum([0] + [np.prod(shape) for shape in shapes])\n",
    "\n",
    "    for i in range(particles):\n",
    "        \n",
    "        # just an initial model with the correct structure regarding neurons, layers, activation functions, Glorot initialization\n",
    "        model = nn_model_structure(layers = layers,\n",
    "                                   neurons = neurons,\n",
    "                                   n_cols = n_cols,\n",
    "                                   classification = True)\n",
    "        model = nn_model_compile(model,\n",
    "                                 optimizer = \"sgd\")\n",
    "        # for every particle write the model in a dictionary\n",
    "        model_dict[\"model_{}\".format(str(i+1))] = model\n",
    "\n",
    "        # for every particles write the weights and biases in a dictionary\n",
    "        weights_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .get_weights()\n",
    "        \n",
    "        # for every particle write the predictions on the training batches in a dictionary\n",
    "        y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .predict(X_train)\n",
    "\n",
    "        # for every particle write the Jacobian in a dictionary\n",
    "        jacobian_dict[\"model_{}\".format(str(i+1))] = (-1) * np.multiply(np.array(y_train),\n",
    "                                                                                 np.array(1 / (y_pred_dict[\"model_{}\".format(str(i+1))] + delta)))\n",
    "\n",
    "        train_acc_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        test_acc_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        \n",
    "    # mean_model as the model with the mean of the weights of all particle models\n",
    "    mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "    mean_model = init_model\n",
    "    mean_model.set_weights(mean_weights)\n",
    "\n",
    "    mean_model_train_acc = np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1])\n",
    "    mean_model_test_acc = np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1])\n",
    "\n",
    "    # loop over all epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # early stopping\n",
    "        if early_stopping is not None:\n",
    "            if epoch == 0:\n",
    "                train_acc_old = 0\n",
    "                test_acc_old = 0\n",
    "            else:\n",
    "                train_acc_new = mean_model_train_acc[epoch]\n",
    "                test_acc_new = mean_model_test_acc[epoch]\n",
    "                if np.absolute(test_acc_new - test_acc_old) <= early_stopping and np.absolute(train_acc_new - train_acc_old) <= early_stopping:\n",
    "                    print(\"STOP: Early Stopping after epoch {} because improvement in training accuracy is only {} and in test acc only {}.\"\\\n",
    "                                                                         .format(epoch, train_acc_new - train_acc_old, test_acc_new - test_acc_old))\n",
    "                    break\n",
    "                test_mse_old = test_mse_new\n",
    "                                                                            \n",
    "        # shuffle the data\n",
    "        if shuffle:\n",
    "            indices = y_train.sample(frac=1).index\n",
    "        else:\n",
    "            indices = y_train.index\n",
    "            \n",
    "        if disjoint_batch:\n",
    "            X_batches = [np.array(X_train)[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [y_train.iloc[indices].reset_index(drop = True)[int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [np.array(i) for i in y_batches]\n",
    "        else:\n",
    "            if last_batch_size != 0:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches-1)]\n",
    "                indices.append(np.random.choice(len(X_train), size = last_batch_size, replace = True))\n",
    "            else:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches)]\n",
    "            X_batches = [X_train[indices[i]] for i in range(len(indices))]\n",
    "            y_batches = [y_train[indices[i]] for i in range(len(indices))]\n",
    "         \n",
    "        # shuffling for batch particle connection\n",
    "        if batch_particle_connection and batch_particle_shuffle == \"permute\":\n",
    "            shuffled_indices = np.hstack(list(batch_particle_dict.values()))\n",
    "            np.random.shuffle(shuffled_indices)\n",
    "            batch_particle_values = list(batch_particle_dict.values())\n",
    "            for i in range(len(batch_particle_values)):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = shuffled_indices[i*len(batch_particle_values[i]):(i+1)*len(batch_particle_values[i])]\n",
    "        if batch_particle_connection and (batch_particle_shuffle == \"particle\" or batch_particle_shuffle == \"full\"):\n",
    "            batch_particle_dict = {}\n",
    "            batch_particle_indices = np.arange(particles) + 1\n",
    "            np.random.shuffle(batch_particle_indices)\n",
    "            if particles == num_batches:\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "            elif particles > num_batches:\n",
    "                base_batches = particles // num_batches\n",
    "                add_batches = particles % num_batches\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                    batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "                for i in range(add_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "            elif num_batches > particles:\n",
    "                num_reps = int(np.ceil(num_batches / particles))\n",
    "                particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "        if batch_particle_connection and (batch_particle_shuffle == \"batch\" or batch_particle_shuffle == \"full\"):\n",
    "            indices = np.arange(n)\n",
    "            np.random.shuffle(indices)\n",
    "            if disjoint_batch:\n",
    "                X_batches = [X_train[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "                y_batches = [y_train[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            else:\n",
    "                if last_batch_size != 0:\n",
    "                    indices = [np.random.choice(X_train.shape[0], size = batch_size, replace = True) for i in range(num_batches-1)]\n",
    "                    indices.append(np.random.choice(X_train.shape[0], size = last_batch_size, replace = True))\n",
    "                else:\n",
    "                    indices = [np.random.choice(X_train.shape[0], size = batch_size, replace = True) for i in range(num_batches)]\n",
    "                X_batches = [X_train[indices[i]] for i in range(len(indices))]\n",
    "                y_batches = [y_train[indices[i]] for i in range(len(indices))]\n",
    "                             \n",
    "        # loop over all batches\n",
    "        for b in range(num_batches):\n",
    "            batch_particles = []\n",
    "            for i in range(particles):\n",
    "                if batch_particle_connection: \n",
    "                    if num_batches == particles or num_batches > particles:\n",
    "                        if batch_particle_dict[\"batch_{}\".format(str(b+1))] != i+1:\n",
    "                            continue\n",
    "                    else:\n",
    "                        if i+1 not in batch_particle_dict[\"batch_{}\".format(str(b+1))]:\n",
    "                            continue\n",
    "                if batch_particle_connection:\n",
    "                    batch_particles.append(i+1)\n",
    "\n",
    "                # set new weights for model\n",
    "                model_dict[\"model_{}\".format(str(i+1))].set_weights(weights_dict[\"model_{}\".format(str(i+1))])\n",
    "\n",
    "                # for every particle write the predictions on the training batches in a dictionary\n",
    "                y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                .predict(X_batches[b])\n",
    "                \n",
    "                # for every particle write the Jacobian in a dictionary\n",
    "                jacobian_dict[\"model_{}\".format(str(i+1))] = (-1) * np.multiply(np.array(y_batches[b]),\n",
    "                                                                            np.array(1 / (y_pred_dict[\"model_{}\".format(str(i+1))] + delta)))\n",
    "\n",
    "            if not batch_particle_connection:        \n",
    "                # compute the mean of the predictions\n",
    "                y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "                # compute the matrix D elementwise\n",
    "                d = np.zeros(shape = (particles, particles))\n",
    "                for k in range(particles):\n",
    "                    y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                    for j in range(particles):\n",
    "                        d[k][j] = np.sum(np.multiply(y_pred_centered, jacobian_dict[\"model_{}\".format(str(j+1))]))\n",
    "                d = np.transpose(d)\n",
    "\n",
    "                # compute the scalar h_t\n",
    "                h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "                # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "                for i in range(particles):\n",
    "                    weights_array = np.array([])\n",
    "                    for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                        weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                    weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "                # matrix with particle parameters as row vectors\n",
    "                weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "                # compute the matrix with the updates for each particle\n",
    "                weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "                for i in range(particles):\n",
    "                    # write the updates back into the dictionary\n",
    "                    weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                    # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                    for l in range(len(shape_elements)-1):\n",
    "                        start = shape_elements[l]\n",
    "                        end = shape_elements[l+1]\n",
    "                        weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "\n",
    "        if batch_particle_connection:\n",
    "            # compute the mean of the predictions\n",
    "            y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "            # compute the matrix D elementwise\n",
    "            d = np.zeros(shape = (particles, particles))\n",
    "            for k in range(particles):\n",
    "                y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                for j in range(particles):\n",
    "                    d[k][j] = np.sum(np.multiply(y_pred_centered, jacobian_dict[\"model_{}\".format(str(j+1))]))\n",
    "            d = np.transpose(d)\n",
    "\n",
    "            # compute the scalar h_t\n",
    "            h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "            # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "            for i in range(particles):\n",
    "                weights_array = np.array([])\n",
    "                for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                    weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "            # matrix with particle parameters as row vectors\n",
    "            weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "            # compute the matrix with the updates for each particle\n",
    "            weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "            for i in range(particles):\n",
    "                # write the updates back into the dictionary\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                for l in range(len(shape_elements)-1):\n",
    "                    start = shape_elements[l]\n",
    "                    end = shape_elements[l+1]\n",
    "                    weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                                                            \n",
    "        for i in range(particles):\n",
    "            # for every particle write the training accuracy of the current iteration in a dictionary\n",
    "            train_acc_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_train, y_train, verbose = 0)[1])\n",
    "\n",
    "            # for every particle write the test accuracy of the current iteration in a dictionary\n",
    "            test_acc_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_test, y_test, verbose = 0)[1])\n",
    "\n",
    "        # update the mean_model\n",
    "        mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "        mean_model.set_weights(mean_weights)\n",
    "\n",
    "        mean_model_train_acc = np.append(mean_model_train_acc, np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1]))\n",
    "        mean_model_test_acc = np.append(mean_model_test_acc, np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1]))\n",
    "        \n",
    "        if verbose == 1:\n",
    "            print(\"Epoch {}. Training Accuracy: {}, Test Accuracy: {}.\".format(epoch+1,\n",
    "                                                                               np.round(mean_model_train_acc[-1], 3),\n",
    "                                                                               np.round(mean_model_test_acc[-1], 3)))\n",
    "            \n",
    "    mean_model.history.history = {\"accuracy\": mean_model_train_acc[1:],\n",
    "                                  \"val_accuracy\": mean_model_test_acc[1:]}\n",
    "        \n",
    "    if save_all:\n",
    "        param_dict = param_to_dict(X_train,\n",
    "                                   X_test,\n",
    "                                   y_train,\n",
    "                                   y_test,\n",
    "                                   layers,\n",
    "                                   neurons,\n",
    "                                   particles,\n",
    "                                   epochs,\n",
    "                                   batch_size,\n",
    "                                   h_0,\n",
    "                                   delta,\n",
    "                                   epsilon,\n",
    "                                   shuffle,\n",
    "                                   early_stopping\n",
    "                                   )\n",
    "        results_dict = results_to_dict(mean_model_train_mse,\n",
    "                                       mean_model_test_mse,\n",
    "                                       train_mse_dict,\n",
    "                                       test_mse_dict,\n",
    "                                       weights_dict,\n",
    "                                       y_pred_dict,\n",
    "                                       True\n",
    "                                       )\n",
    "\n",
    "        saving_dict = {}\n",
    "        saving_dict[\"parameters\"] = param_dict\n",
    "        saving_dict[\"results\"] = results_dict\n",
    "        saving_dict[\"analysis\"] = analysis_dict\n",
    "\n",
    "        save_objects(obj_dict = saving_dict,\n",
    "                     file = file_var)\n",
    "\n",
    "        nn_save(model = mean_model,\n",
    "                path_name = file_model)\n",
    "        \n",
    "    return mean_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting_dict = {\"particles\": 50,\n",
    "                \"epochs\": 10,\n",
    "                \"batch_size\": 100,    # len(X_train)\n",
    "                \"h_0\": 2,\n",
    "                \"delta\": 0.005,\n",
    "                \"epsilon\": 0.5,\n",
    "                \"shuffle\": True,\n",
    "                \"early_stopping\": None\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dict = {\"disjoint_batch\": True,\n",
    "                 \"batch_particle_connection\": {\"connect\": True,\n",
    "                                               \"shuffle\": \"particle\"}        # None, \"permute\", \"particle\", \"batch\", \"full\"\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training Accuracy: 0.099, Test Accuracy: 0.11.\n",
      "Epoch 2. Training Accuracy: 0.099, Test Accuracy: 0.11.\n",
      "Epoch 3. Training Accuracy: 0.117, Test Accuracy: 0.098.\n",
      "Epoch 4. Training Accuracy: 0.117, Test Accuracy: 0.098.\n",
      "Epoch 5. Training Accuracy: 0.117, Test Accuracy: 0.098.\n",
      "Epoch 6. Training Accuracy: 0.117, Test Accuracy: 0.098.\n",
      "Epoch 7. Training Accuracy: 0.117, Test Accuracy: 0.098.\n",
      "Epoch 8. Training Accuracy: 0.117, Test Accuracy: 0.098.\n",
      "Epoch 9. Training Accuracy: 0.117, Test Accuracy: 0.098.\n",
      "Epoch 10. Training Accuracy: 0.117, Test Accuracy: 0.098.\n"
     ]
    }
   ],
   "source": [
    "mean_model = enkf_classifier_analysis(X_train,\n",
    "                                      X_test,\n",
    "                                      y_train,\n",
    "                                      y_test,\n",
    "                                      layers,\n",
    "                                      neurons,\n",
    "                                      setting_dict,\n",
    "                                      analysis_dict,\n",
    "                                      save_all = False,\n",
    "                                      file_var = \"../objects/mnist/mnist_enkf_E{}_B{}_P{}_H{}.pckl\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                      file_model = \"../models/mnist/mnist_enkf_E{}_B{}_P{}_H{}.h5\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                      verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation time: 2.7766504526138305 minutes.\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "print(\"Calculation time: {} minutes.\".format((end_time - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_plot_acc(mean_model,\n",
    "            mean_comparison = None,\n",
    "            start_epoch = 1,\n",
    "            save = None)#\"../img/mnist/mnist_enkf_E{}_B{}_P{}_H{}.png\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
