{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../python/functions\")\n",
    "sys.path.insert(2, \"../python/architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../python/architecture\\reproducible.py:14: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_prep_functions import mnist_prep\n",
    "from plotting_functions import nn_plot_acc\n",
    "from enkf_functions import enkf_classifier\n",
    "from saving_functions import load_objects, save_objects\n",
    "from model_functions import nn_load, nn_model_structure, nn_model_compile\n",
    "import reproducible\n",
    "import no_gpu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = mnist_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use samller dataset for increased speed\n",
    "X_train = X_train[:1000, :]\n",
    "X_test = X_test[:500, :]\n",
    "y_train = y_train[:1000]\n",
    "y_test = y_test[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 5\n",
    "neurons = [128, 128, 64, 32, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enkf_classifier_analysis(X_train,\n",
    "                             X_test,\n",
    "                             y_train,\n",
    "                             y_test,\n",
    "                             layers,\n",
    "                             neurons,\n",
    "                             setting_dict,\n",
    "                             analysis_dict,\n",
    "                             save_all = False,\n",
    "                             file_var = \"file.pckl\",\n",
    "                             file_model = \"file.h5\",\n",
    "                             verbose = 0\n",
    "                             ):\n",
    "\n",
    "    \"\"\" Ensemble Kalman Filter algorithm analysis for classification problems.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train (np.ndarray): Training data X.\n",
    "    X_test (np.ndarray): Test data X.\n",
    "    y_train (pd.DataFrame): Training data y.\n",
    "    y_test (pd.DataFrame): Test data y.\n",
    "    layers (int): Number of layers.\n",
    "    neurons (list): Number of neurons in each layer.\n",
    "    setting_dict (dict): Dictionary containing\n",
    "        particles (int): Number of particles in the ensemble.\n",
    "        epochs (int): Number of epochs.\n",
    "        batch_size (None or int): Size of the batches. Must be between 0 and the number of observations in the training set.\n",
    "        h_0 (int or float): Starting step size.\n",
    "        delta (float): Constant for numerical stability in the jacobian.\n",
    "        epsilon (float): Constant for numerical stability in the step size.\n",
    "        randomization (bool): Whether or not to add noise to the particles and randomize them around their mean.\n",
    "        shuffle (bool): Whether or not to shuffle the data prior to each epoch.\n",
    "        early_stopping (bool): Whether or not to stop the calculation when the changes get small.\n",
    "        early_stopping_diff (bool): Minimum change before early stopping is applied.\n",
    "    analysis_dict (dict): Dictionary containing\n",
    "        disjoint_batch (bool): Whether or not to use disjoint batches. If False then each batch is sampled with replacement.\n",
    "        multiple_updates_same_batch (int or None): Number of consecutive updates on the same batch.\n",
    "        batch_particle_connection (dict): Dictionary containing\n",
    "            connect (bool): Whether or not to connect particles and batches.\n",
    "            shuffle (str or None): Whether or not and how to shuffle the connection. None = no shuffle. \"batch\" = shuffle the batch for fixed particle sets. \"full\" = shuffle the particle sets and their corresponding batch.\n",
    "            update_all (bool): Whether or not to update after all particles have seen some data.\n",
    "        batch_evaluation (dict): Dictionary containing\n",
    "            mean_model (bool): Whether or not evaluate the mean model after every batch on that batch. Warning: the computational costs are very high.\n",
    "            particles (bool): Whether or not evaluate every particle model after every batch on that batch. Warning: the computational costs are incredibly high.\n",
    "            file (str): Path and name of the file to save evaluations into \n",
    "    save_all (bool): Whether or not to save all important variables and models.\n",
    "    file_var (str): Path and name of the file to save variables into.\n",
    "    file_model (str): Path and name of the file to save the final model into.\n",
    "    verbose (int): If 0, then don't print anything throughout the training process. If 1, then print training and test accuracy after each epoch.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    mean_model (tensorflow.python.keras.engine.sequential.Sequential): The final model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    particles = setting_dict[\"particles\"]\n",
    "    epochs = setting_dict[\"epochs\"]\n",
    "    batch_size = setting_dict[\"batch_size\"]\n",
    "    h_0 = setting_dict[\"h_0\"]\n",
    "    delta = setting_dict[\"delta\"]\n",
    "    epsilon = setting_dict[\"epsilon\"]\n",
    "    randomization = setting_dict[\"randomization\"]\n",
    "    shuffle = setting_dict[\"shuffle\"]\n",
    "    early_stopping = setting_dict[\"early_stopping\"]\n",
    "    early_stopping_diff = setting_dict[\"early_stopping_diff\"]\n",
    "    \n",
    "    disjoint_batch = analysis_dict[\"disjoint_batch\"]\n",
    "    batch_evaluation = analysis_dict[\"batch_evaluation\"]\n",
    "    batch_particle_connection = analysis_dict[\"batch_particle_connection\"][\"connect\"]\n",
    "    batch_particle_shuffle = analysis_dict[\"batch_particle_connection\"][\"shuffle\"]\n",
    "    update_all = analysis_dict[\"batch_particle_connection\"][\"update_all\"]\n",
    "    mult_updates_batch = analysis_dict[\"multiple_updates_same_batch\"]\n",
    "    \n",
    "    if mult_updates_batch is None:\n",
    "        mult_updates_batch = 1\n",
    "\n",
    "    if batch_size == None:\n",
    "        batch_size = len(X_train)\n",
    "\n",
    "    n_cols = X_train.shape[1]\n",
    "    \n",
    "    if disjoint_batch:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        batch_indices = np.cumsum([0] + list(np.ones(num_batches) * batch_size))\n",
    "        batch_indices[-1] = n\n",
    "    else:\n",
    "        n = len(X_train)\n",
    "        num_batches = int(np.ceil(n / batch_size))\n",
    "        last_batch_size = n % batch_size\n",
    "        \n",
    "    if batch_particle_connection:\n",
    "        batch_particle_dict = {}\n",
    "        batch_particle_indices = np.arange(particles) + 1\n",
    "        np.random.shuffle(batch_particle_indices)\n",
    "        if particles == num_batches:\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "        elif particles > num_batches:\n",
    "            base_batches = particles // num_batches\n",
    "            add_batches = particles % num_batches\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "            for i in range(add_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "        elif num_batches > particles:\n",
    "            num_reps = int(np.ceil(num_batches / particles))\n",
    "            particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "            for i in range(num_batches):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "    else:\n",
    "        batch_particle_dict = None\n",
    "\n",
    "    model_dict = {}\n",
    "    weights_dict = {}\n",
    "    y_pred_dict = {}\n",
    "    jacobian_dict = {}\n",
    "    weights_vector_dict = {}\n",
    "    train_acc_dict = {}\n",
    "    test_acc_dict = {}\n",
    "    \n",
    "    if batch_evaluation[\"mean_model\"]:\n",
    "        train_batch_acc_mean_dict = {}\n",
    "    if batch_evaluation[\"particles\"]:\n",
    "        train_batch_acc_particle_dict = {}\n",
    "\n",
    "    # init_model already has weights and biases following the Glorot distribution\n",
    "    # it can already be used to predict and evaluate, but it is very bad\n",
    "    # only used to determine shapes and shape_elements via its weights\n",
    "    init_model = nn_model_structure(layers = layers,\n",
    "                                    neurons = neurons,\n",
    "                                    n_cols = n_cols,\n",
    "                                    classification = True)\n",
    "    init_model = nn_model_compile(init_model,\n",
    "                                  optimizer = \"sgd\")\n",
    "    weights = init_model.get_weights()\n",
    "    # shape contains the shapes of the weight matrices and bias vectors as a list of arrays\n",
    "    shapes = [np.array(params.shape) for params in weights]\n",
    "    # shape_elements contains the indices of the weights as a vector and tells where to cut\n",
    "    shape_elements = np.cumsum([0] + [np.prod(shape) for shape in shapes])\n",
    "\n",
    "    for i in range(particles):\n",
    "        \n",
    "        # just an initial model with the correct structure regarding neurons, layers, activation functions, Glorot initialization\n",
    "        model = nn_model_structure(layers = layers,\n",
    "                                   neurons = neurons,\n",
    "                                   n_cols = n_cols,\n",
    "                                   classification = True)\n",
    "        model = nn_model_compile(model,\n",
    "                                 optimizer = \"sgd\")\n",
    "        # for every particle write the model in a dictionary\n",
    "        model_dict[\"model_{}\".format(str(i+1))] = model\n",
    "\n",
    "        # for every particles write the weights and biases in a dictionary\n",
    "        weights_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .get_weights()\n",
    "        \n",
    "        # for every particle write the predictions on the training batches in a dictionary\n",
    "        y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                        .predict(X_train)\n",
    "\n",
    "        # for every particle write the Jacobian in a dictionary\n",
    "        jacobian_dict[\"model_{}\".format(str(i+1))] = (-1) * np.multiply(np.array(y_train),\n",
    "                                                                                 np.array(1 / (y_pred_dict[\"model_{}\".format(str(i+1))] + delta)))\n",
    "\n",
    "        train_acc_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        test_acc_dict[\"model_{}\".format(str(i+1))] = []\n",
    "        \n",
    "        if batch_evaluation[\"particles\"]:\n",
    "            train_batch_ac_particle_dict[\"particle_{}\".format(str(i+1))] = {}\n",
    "\n",
    "    # mean_model as the model with the mean of the weights of all particle models\n",
    "    mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "    mean_model = init_model\n",
    "    mean_model.set_weights(mean_weights)\n",
    "\n",
    "    mean_model_train_acc = np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1])\n",
    "    mean_model_test_acc = np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1])\n",
    "\n",
    "    # loop over all epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # early stopping\n",
    "        if early_stopping:\n",
    "            if epoch == 0:\n",
    "                train_acc_old = 0\n",
    "                test_acc_old = 0\n",
    "            else:\n",
    "                train_acc_new = mean_model_train_acc[epoch]\n",
    "                test_acc_new = mean_model_test_acc[epoch]\n",
    "                if np.absolute(test_acc_new - test_acc_old) <= early_stopping_diff and np.absolute(train_acc_new - train_acc_old) <= early_stopping_diff:\n",
    "                    print(\"STOP: Early Stopping after epoch {} because improvement in training accuracy is only {} and in test acc only {}.\"\\\n",
    "                                                                         .format(epoch, train_acc_new - train_acc_old, test_acc_new - test_acc_old))\n",
    "                    break\n",
    "                test_mse_old = test_mse_new\n",
    "                                                                            \n",
    "        # shuffle the data\n",
    "        if shuffle:\n",
    "            indices = y_train.sample(frac=1).index\n",
    "        else:\n",
    "            indices = y_train.index\n",
    "            \n",
    "        if disjoint_batch:\n",
    "            X_batches = [np.array(X_train)[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [y_train.iloc[indices].reset_index(drop = True)[int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "            y_batches = [np.array(i) for i in y_batches]\n",
    "        else:\n",
    "            if last_batch_size != 0:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches-1)]\n",
    "                indices.append(np.random.choice(len(X_train), size = last_batch_size, replace = True))\n",
    "            else:\n",
    "                indices = [np.random.choice(len(X_train), size = batch_size, replace = True) for i in range(num_batches)]\n",
    "            X_batches = [X_train[indices[i]] for i in range(len(indices))]\n",
    "            y_batches = [y_train[indices[i]] for i in range(len(indices))]\n",
    "            \n",
    "        if batch_particle_connection and batch_particle_shuffle == \"batch\":\n",
    "            shuffled_indices = np.hstack(list(batch_particle_dict.values()))\n",
    "            np.random.shuffle(shuffled_indices)\n",
    "            batch_particle_values = list(batch_particle_dict.values())\n",
    "            for i in range(len(batch_particle_values)):\n",
    "                batch_particle_dict[\"batch_{}\".format(str(i+1))] = shuffled_indices[i*len(batch_particle_values[i]):(i+1)*len(batch_particle_values[i])]\n",
    "        elif batch_particle_connection and batch_particle_shuffle == \"full\":\n",
    "            batch_particle_dict = {}\n",
    "            batch_particle_indices = np.arange(particles) + 1\n",
    "            np.random.shuffle(batch_particle_indices)\n",
    "            if particles == num_batches:\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[i]\n",
    "            elif particles > num_batches:\n",
    "                base_batches = particles // num_batches\n",
    "                add_batches = particles % num_batches\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = batch_particle_indices[:base_batches]\n",
    "                    batch_particle_indices = batch_particle_indices[base_batches:]\n",
    "                for i in range(add_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = np.concatenate([batch_particle_dict[\"batch_{}\".format(str(i+1))], np.array([batch_particle_indices[i]])])\n",
    "            elif num_batches > particles:\n",
    "                num_reps = int(np.ceil(num_batches / particles))\n",
    "                particles_repeated = np.tile(batch_particle_indices, num_reps)\n",
    "                for i in range(num_batches):\n",
    "                    batch_particle_dict[\"batch_{}\".format(str(i+1))] = particles_repeated[i]\n",
    "            \n",
    "        if batch_evaluation[\"mean_model\"]:\n",
    "            train_batch_acc_mean_dict[\"Epoch_{}\".format(str(epoch+1))] = {}\n",
    "        if batch_evaluation[\"particles\"]:\n",
    "            train_batch_acc_particle_dict[\"particle_{}\".format(str(i+1))][\"Epoch_{}\".format(str(epoch+1))] = {}\n",
    "                    \n",
    "        # loop over all batches\n",
    "        for b in range(num_batches):\n",
    "            for mult_updates in range(mult_updates_batch):\n",
    "                batch_particles = []\n",
    "                y_pred_batch_dict = {}\n",
    "                jacobian_batch_dict = {}\n",
    "                for i in range(particles):\n",
    "                    if batch_particle_connection: \n",
    "                        if num_batches == particles or num_batches > particles:\n",
    "                            if batch_particle_dict[\"batch_{}\".format(str(b+1))] != i+1:\n",
    "                                continue\n",
    "                        else:\n",
    "                            if i+1 not in batch_particle_dict[\"batch_{}\".format(str(b+1))]:\n",
    "                                continue\n",
    "                    if batch_particle_connection:\n",
    "                        batch_particles.append(i+1)\n",
    "                    \n",
    "                    # set new weights for model\n",
    "                    model_dict[\"model_{}\".format(str(i+1))].set_weights(weights_dict[\"model_{}\".format(str(i+1))])\n",
    "\n",
    "                    # for every particle write the predictions on the training batches in a dictionary\n",
    "                    y_pred_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                    .predict(X_batches[b])\n",
    "                    y_pred_batch_dict[\"model_{}\".format(i+1)] = y_pred_dict[\"model_{}\".format(i+1)]\n",
    "\n",
    "                    # for every particle write the Jacobian in a dictionary\n",
    "                    jacobian_dict[\"model_{}\".format(str(i+1))] = (-1) * np.multiply(np.array(y_batches[b]),\n",
    "                                                                                np.array(1 / (y_pred_dict[\"model_{}\".format(str(i+1))] + delta)))\n",
    "                    jacobian_batch_dict[\"model_{}\".format(i+1)] = jacobian_dict[\"model_{}\".format(i+1)]\n",
    "\n",
    "                    if batch_evaluation[\"mean_model\"]:\n",
    "                        train_batch_acc_mean_dict[\"Epoch_{}\".format(str(epoch+1))][\"Batch_{}\".format(str(b+1))] = mean_model.evaluate(X_batches[b], y_batches[b], verbose = 0)[1]\n",
    "                    if batch_evaluation[\"particles\"]:\n",
    "                        train_batch_acc_particle_dict[\"particle_{}\".format(str(i+1))][\"Epoch_{}\".format(str(epoch+1))][\"Batch_{}\".format(str(b+1))] = model_dict[\"model_{}\".format(str(i+1))].evaluate(X_batches[b], y_batches[b], verbose = 0)[1]\n",
    "\n",
    "                        \n",
    "                if not batch_particle_connection:        \n",
    "                    # compute the mean of the predictions\n",
    "                    y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "                    # compute the matrix D elementwise\n",
    "                    d = np.zeros(shape = (particles, particles))\n",
    "                    for k in range(particles):\n",
    "                        y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                        for j in range(particles):\n",
    "                            d[k][j] = np.sum(np.multiply(y_pred_centered, jacobian_dict[\"model_{}\".format(str(j+1))]))\n",
    "                    d = np.transpose(d)\n",
    "\n",
    "                    # compute the scalar h_t\n",
    "                    h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "                    # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "                    for i in range(particles):\n",
    "                        weights_array = np.array([])\n",
    "                        for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                            weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "                    # matrix with particle parameters as row vectors\n",
    "                    weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "                    # compute the matrix with the updates for each particle\n",
    "                    weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "                    for i in range(particles):\n",
    "                        # write the updates back into the dictionary\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                        # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                        for l in range(len(shape_elements)-1):\n",
    "                            start = shape_elements[l]\n",
    "                            end = shape_elements[l+1]\n",
    "                            weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                \n",
    "                elif batch_particle_connection and not update_all:\n",
    "                    # compute the mean of the predictions\n",
    "                    y_pred_mean = np.mean(list(y_pred_batch_dict.values()), axis = 0)\n",
    "                    \n",
    "                    # compute the matrix D elementwise\n",
    "                    d = np.zeros(shape = (len(batch_particles), len(batch_particles)))\n",
    "                    for k in range(len(batch_particles)):\n",
    "                        y_pred_centered = y_pred_batch_dict[\"model_{}\".format(batch_particles[k])] - y_pred_mean\n",
    "                        for j in range(len(batch_particles)):\n",
    "                            d[k][j] = np.sum(np.multiply(y_pred_centered, jacobian_dict[\"model_{}\".format(batch_particles[j])]))\n",
    "                    d = np.transpose(d)\n",
    "\n",
    "                    # compute the scalar h_t\n",
    "                    h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "                    # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "                    for i in range(particles):\n",
    "                        weights_array = np.array([])\n",
    "                        for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                            weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                        weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "                    # matrix with particle parameters as row vectors\n",
    "                    weights_vector_batch_dict = {}\n",
    "                    for i in range(len(batch_particles)):\n",
    "                        weights_vector_batch_dict[\"model_{}\".format(batch_particles[i])] = weights_vector_dict[\"model_{}\".format(batch_particles[i])]\n",
    "                        weights_all_ptcls = np.array(list(weights_vector_batch_dict.values()))\n",
    "\n",
    "                    # compute the matrix with the updates for each particle\n",
    "                    weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "                    for i in range(len(batch_particles)):\n",
    "                        # write the updates back into the dictionary\n",
    "                        weights_vector_dict[\"model_{}\".format(batch_particles[i])] = weights_all_ptcls[i]\n",
    "                        # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                        for l in range(len(shape_elements)-1):\n",
    "                            start = shape_elements[l]\n",
    "                            end = shape_elements[l+1]\n",
    "                            weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "\n",
    "        if batch_particle_connection and update_all:\n",
    "            # compute the mean of the predictions\n",
    "            y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "\n",
    "            # compute the matrix D elementwise\n",
    "            d = np.zeros(shape = (particles, particles))\n",
    "            for k in range(particles):\n",
    "                y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "                for j in range(particles):\n",
    "                    d[k][j] = np.sum(np.multiply(y_pred_centered, jacobian_dict[\"model_{}\".format(str(j+1))]))\n",
    "            d = np.transpose(d)\n",
    "\n",
    "            # compute the scalar h_t\n",
    "            h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "\n",
    "            # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "            for i in range(particles):\n",
    "                weights_array = np.array([])\n",
    "                for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                    weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "\n",
    "            # matrix with particle parameters as row vectors\n",
    "            weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "            # compute the matrix with the updates for each particle\n",
    "            weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "            for i in range(particles):\n",
    "                # write the updates back into the dictionary\n",
    "                weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "                # reshape the updates, so that they are of the original matrx and vector shape\n",
    "                for l in range(len(shape_elements)-1):\n",
    "                    start = shape_elements[l]\n",
    "                    end = shape_elements[l+1]\n",
    "                    weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "                                                            \n",
    "        for i in range(particles):\n",
    "            # for every particle write the training accuracy of the current iteration in a dictionary\n",
    "            train_acc_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_train, y_train, verbose = 0)[1])\n",
    "\n",
    "            # for every particle write the test accuracy of the current iteration in a dictionary\n",
    "            test_acc_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                      .evaluate(X_test, y_test, verbose = 0)[1])\n",
    "\n",
    "        # update the mean_model\n",
    "        mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "        mean_model.set_weights(mean_weights)\n",
    "\n",
    "        mean_model_train_acc = np.append(mean_model_train_acc, np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1]))\n",
    "        mean_model_test_acc = np.append(mean_model_test_acc, np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1]))\n",
    "        \n",
    "        if verbose == 1:\n",
    "            print(\"Epoch {}. Training Accuracy: {}, Test Accuracy: {}.\".format(epoch+1,\n",
    "                                                                               np.round(mean_model_train_acc[-1], 3),\n",
    "                                                                               np.round(mean_model_test_acc[-1], 3)))\n",
    "            \n",
    "    mean_model.history.history = {\"accuracy\": mean_model_train_acc[1:],\n",
    "                                  \"val_accuracy\": mean_model_test_acc[1:]}\n",
    "        \n",
    "    if save_all:\n",
    "        param_dict = param_to_dict(X_train,\n",
    "                                   X_test,\n",
    "                                   y_train,\n",
    "                                   y_test,\n",
    "                                   layers,\n",
    "                                   neurons,\n",
    "                                   particles,\n",
    "                                   epochs,\n",
    "                                   batch_size,\n",
    "                                   h_0,\n",
    "                                   delta,\n",
    "                                   epsilon,\n",
    "                                   randomization,\n",
    "                                   shuffle,\n",
    "                                   early_stopping,\n",
    "                                   early_stopping_diff\n",
    "                                   )\n",
    "        results_dict = results_to_dict(mean_model_train_mse,\n",
    "                                       mean_model_test_mse,\n",
    "                                       train_mse_dict,\n",
    "                                       test_mse_dict,\n",
    "                                       weights_dict,\n",
    "                                       y_pred_dict,\n",
    "                                       True\n",
    "                                       )\n",
    "\n",
    "        saving_dict = {}\n",
    "        saving_dict[\"parameters\"] = param_dict\n",
    "        saving_dict[\"results\"] = results_dict\n",
    "        saving_dict[\"analysis\"] = analysis_dict\n",
    "\n",
    "        save_objects(obj_dict = saving_dict,\n",
    "                     file = file_var)\n",
    "\n",
    "        nn_save(model = mean_model,\n",
    "                path_name = file_model)\n",
    "        \n",
    "        if batch_evaluation[\"mean_model\"]:\n",
    "            full_dict = {}\n",
    "            full_dict[\"mean_model\"] = train_batch_acc_mean_dict\n",
    "            if batch_evaluation[\"particles\"]:\n",
    "                full_dict[\"particles\"] = train_batch_acc_particle_dict\n",
    "            save_objects(obj_dict = full_dict,\n",
    "                         file = batch_evaluation[\"file\"])\n",
    "        elif batch_evaluation[\"particles\"]:\n",
    "            full_dict = {}\n",
    "            full_dict[\"particles\"] = train_batch_acc_particle_dict\n",
    "            if batch_evaluation[\"mean_model\"]:\n",
    "                full_dict[\"mean_model\"] = train_batch_acc_mean_dict\n",
    "            save_objects(obj_dict = full_dict,\n",
    "                         file = batch_evaluation[\"file\"])\n",
    " \n",
    "    return mean_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting_dict = {\"particles\": 50,\n",
    "                \"epochs\": 10,\n",
    "                \"batch_size\": 100,    # len(X_train)\n",
    "                \"h_0\": 2,\n",
    "                \"delta\": 0.005,\n",
    "                \"epsilon\": 0.5,\n",
    "                \"randomization\": False,\n",
    "                \"shuffle\": True,\n",
    "                \"early_stopping\": False,\n",
    "                \"early_stopping_diff\": 0.001\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dict = {\"disjoint_batch\": True,\n",
    "                 \"multiple_updates_same_batch\": None,\n",
    "                 \"batch_particle_connection\": {\"connect\": False,\n",
    "                                               \"shuffle\": \"full\",        # None, \"batch\", \"full\"\n",
    "                                               \"update_all\": False},\n",
    "                 \"batch_evaluation\": {\"mean_model\": False,\n",
    "                                      \"particles\": False,\n",
    "                                      \"file\": \"../objects/mnist/batch_acc.pckl\"}\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training Accuracy: 0.147, Test Accuracy: 0.112.\n",
      "Epoch 2. Training Accuracy: 0.209, Test Accuracy: 0.134.\n",
      "Epoch 3. Training Accuracy: 0.258, Test Accuracy: 0.234.\n",
      "Epoch 4. Training Accuracy: 0.275, Test Accuracy: 0.226.\n",
      "Epoch 5. Training Accuracy: 0.287, Test Accuracy: 0.242.\n",
      "Epoch 6. Training Accuracy: 0.292, Test Accuracy: 0.236.\n",
      "Epoch 7. Training Accuracy: 0.297, Test Accuracy: 0.24.\n",
      "Epoch 8. Training Accuracy: 0.299, Test Accuracy: 0.232.\n",
      "Epoch 9. Training Accuracy: 0.295, Test Accuracy: 0.226.\n",
      "Epoch 10. Training Accuracy: 0.298, Test Accuracy: 0.226.\n"
     ]
    }
   ],
   "source": [
    "mean_model = enkf_classifier_analysis(X_train,\n",
    "                                      X_test,\n",
    "                                      y_train,\n",
    "                                      y_test,\n",
    "                                      layers,\n",
    "                                      neurons,\n",
    "                                      setting_dict,\n",
    "                                      analysis_dict,\n",
    "                                      save_all = False,\n",
    "                                      file_var = \"../objects/mnist/mnist_enkf_E{}_B{}_P{}_H{}.pckl\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                      file_model = \"../models/mnist/mnist_enkf_E{}_B{}_P{}_H{}.h5\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]),\n",
    "                                      verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation time: 3.58960311015447 minutes.\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "print(\"Calculation time: {} minutes.\".format((end_time - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_plot_acc(mean_model,\n",
    "            mean_comparison = None,\n",
    "            start_epoch = 1,\n",
    "            savefig = False,\n",
    "            file = \"../img/mnist/mnist_enkf_E{}_B{}_P{}_H{}.png\".format(setting_dict[\"epochs\"], setting_dict[\"batch_size\"], setting_dict[\"particles\"], setting_dict[\"h_0\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
