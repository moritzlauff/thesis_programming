{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../python/functions\")\n",
    "sys.path.insert(2, \"../python/architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../python/architecture\\reproducible.py:14: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_prep_functions import mnist_prep\n",
    "from model_functions import *\n",
    "from plotting_functions import *\n",
    "import no_gpu\n",
    "import reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import six\n",
    "from six.moves import zip  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from tensorflow.python.distribute import distribution_strategy_context\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.optimizer_v2 import adadelta as adadelta_v2\n",
    "from tensorflow.python.keras.optimizer_v2 import adagrad as adagrad_v2\n",
    "from tensorflow.python.keras.optimizer_v2 import adam as adam_v2\n",
    "from tensorflow.python.keras.optimizer_v2 import adamax as adamax_v2\n",
    "from tensorflow.python.keras.optimizer_v2 import ftrl\n",
    "from tensorflow.python.keras.optimizer_v2 import gradient_descent as gradient_descent_v2\n",
    "from tensorflow.python.keras.optimizer_v2 import nadam as nadam_v2\n",
    "from tensorflow.python.keras.optimizer_v2 import optimizer_v2\n",
    "from tensorflow.python.keras.optimizer_v2 import rmsprop as rmsprop_v2\n",
    "from tensorflow.python.keras.utils.generic_utils import deserialize_keras_object\n",
    "from tensorflow.python.keras.utils.generic_utils import serialize_keras_object\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.training import optimizer as tf_optimizer_module\n",
    "from tensorflow.python.training import training_util\n",
    "from tensorflow.python.training.tracking import base as trackable\n",
    "from tensorflow.python.util.tf_export import keras_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.optimizers import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD_test(Optimizer):\n",
    "  \"\"\"Stochastic gradient descent optimizer.\n",
    "\n",
    "  Includes support for momentum,\n",
    "  learning rate decay, and Nesterov momentum.\n",
    "\n",
    "  Arguments:\n",
    "      lr: float >= 0. Learning rate.\n",
    "      momentum: float >= 0. Parameter that accelerates SGD in the relevant\n",
    "        direction and dampens oscillations.\n",
    "      decay: float >= 0. Learning rate decay over each update.\n",
    "      nesterov: boolean. Whether to apply Nesterov momentum.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False, **kwargs):\n",
    "    super(SGD_test, self).__init__(**kwargs)\n",
    "    with K.name_scope(self.__class__.__name__):\n",
    "      self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "      self.lr = K.variable(lr, name='lr')\n",
    "      self.momentum = K.variable(momentum, name='momentum')\n",
    "      self.decay = K.variable(decay, name='decay')\n",
    "    self.initial_decay = decay\n",
    "    self.nesterov = nesterov\n",
    "\n",
    "  def _create_all_weights(self, params):\n",
    "    shapes = [K.int_shape(p) for p in params]\n",
    "    moments = [K.zeros(shape) for shape in shapes]\n",
    "    self.weights = [self.iterations] + moments\n",
    "    return moments\n",
    "\n",
    "  def get_updates(self, loss, params):\n",
    "    grads = self.get_gradients(loss, params)\n",
    "    self.updates = [state_ops.assign_add(self.iterations, 1)]\n",
    "\n",
    "    lr = self.lr\n",
    "    if self.initial_decay > 0:\n",
    "      lr = lr * (  # pylint: disable=g-no-augmented-assignment\n",
    "          1. /\n",
    "          (1. +\n",
    "           self.decay * math_ops.cast(self.iterations, K.dtype(self.decay))))\n",
    "    # momentum\n",
    "    moments = self._create_all_weights(params)\n",
    "    for p, g, m in zip(params, grads, moments):\n",
    "      v = self.momentum * m - lr * g  # velocity\n",
    "      self.updates.append(state_ops.assign(m, v))\n",
    "\n",
    "      if self.nesterov:\n",
    "        new_p = p + self.momentum * v - lr * g\n",
    "      else:\n",
    "        new_p = p + v\n",
    "\n",
    "      # Apply constraints.\n",
    "      if getattr(p, 'constraint', None) is not None:\n",
    "        new_p = p.constraint(new_p)\n",
    "\n",
    "      self.updates.append(state_ops.assign(p, new_p))\n",
    "    return self.updates\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        'lr': float(K.get_value(self.lr)),\n",
    "        'momentum': float(K.get_value(self.momentum)),\n",
    "        'decay': float(K.get_value(self.decay)),\n",
    "        'nesterov': self.nesterov\n",
    "    }\n",
    "    base_config = super(SGD, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam_test(Optimizer):\n",
    "  \"\"\"Adam optimizer.\n",
    "\n",
    "  Default parameters follow those provided in the original paper.\n",
    "\n",
    "  Arguments:\n",
    "      lr: float >= 0. Learning rate.\n",
    "      beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "      beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "      decay: float >= 0. Learning rate decay over each update.\n",
    "      amsgrad: boolean. Whether to apply the AMSGrad variant of this algorithm\n",
    "        from the paper \"On the Convergence of Adam and Beyond\".\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               lr=0.001,\n",
    "               beta_1=0.9,\n",
    "               beta_2=0.999,\n",
    "               epsilon=None,\n",
    "               decay=0.,\n",
    "               amsgrad=False,\n",
    "               **kwargs):\n",
    "    super(Adam_test, self).__init__(**kwargs)\n",
    "    with K.name_scope(self.__class__.__name__):\n",
    "      self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "      self.lr = K.variable(lr, name='lr')\n",
    "      self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "      self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "      self.decay = K.variable(decay, name='decay')\n",
    "    if epsilon is None:\n",
    "      epsilon = K.epsilon()\n",
    "    self.epsilon = epsilon\n",
    "    self.initial_decay = decay\n",
    "    self.amsgrad = amsgrad\n",
    "\n",
    "  def _create_all_weights(self, params):\n",
    "    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "    vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "    if self.amsgrad:\n",
    "      vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "    else:\n",
    "      vhats = [K.zeros(1) for _ in params]\n",
    "    self.weights = [self.iterations] + ms + vs + vhats\n",
    "    return ms, vs, vhats\n",
    "\n",
    "  def get_updates(self, loss, params):\n",
    "    grads = self.get_gradients(loss, params)\n",
    "    self.updates = []\n",
    "\n",
    "    lr = self.lr\n",
    "    if self.initial_decay > 0:\n",
    "      lr = lr * (  # pylint: disable=g-no-augmented-assignment\n",
    "          1. /\n",
    "          (1. +\n",
    "           self.decay * math_ops.cast(self.iterations, K.dtype(self.decay))))\n",
    "\n",
    "    with ops.control_dependencies([state_ops.assign_add(self.iterations, 1)]):\n",
    "      t = math_ops.cast(self.iterations, K.floatx())\n",
    "    lr_t = lr * (\n",
    "        K.sqrt(1. - math_ops.pow(self.beta_2, t)) /\n",
    "        (1. - math_ops.pow(self.beta_1, t)))\n",
    "\n",
    "    ms, vs, vhats = self._create_all_weights(params)\n",
    "    for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "      m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "      v_t = (self.beta_2 * v) + (1. - self.beta_2) * math_ops.square(g)\n",
    "      if self.amsgrad:\n",
    "        vhat_t = math_ops.maximum(vhat, v_t)\n",
    "        p_t = p - lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n",
    "        self.updates.append(state_ops.assign(vhat, vhat_t))\n",
    "      else:\n",
    "        p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "      self.updates.append(state_ops.assign(m, m_t))\n",
    "      self.updates.append(state_ops.assign(v, v_t))\n",
    "      new_p = p_t\n",
    "\n",
    "      # Apply constraints.\n",
    "      if getattr(p, 'constraint', None) is not None:\n",
    "        new_p = p.constraint(new_p)\n",
    "\n",
    "      self.updates.append(state_ops.assign(p, new_p))\n",
    "    return self.updates\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        'lr': float(K.get_value(self.lr)),\n",
    "        'beta_1': float(K.get_value(self.beta_1)),\n",
    "        'beta_2': float(K.get_value(self.beta_2)),\n",
    "        'decay': float(K.get_value(self.decay)),\n",
    "        'epsilon': self.epsilon,\n",
    "        'amsgrad': self.amsgrad\n",
    "    }\n",
    "    base_config = super(Adam, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\morit\\anaconda3\\envs\\master_thesis\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.SGD_test at 0x13859531a88>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(SGD_test(), Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "isinstance(SGD_test(), tensorflow.python.keras.optimizers.Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "isinstance(SGD_test(), keras.optimizers.Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_data = pd.read_csv(\"../data/MNIST/mnist_train.csv\").sample(frac = 1)\n",
    "test_data = pd.read_csv(\"../data/MNIST/mnist_test.csv\").sample(frac = 1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = mnist_prep()\n",
    "# use samller dataset for increased speed\n",
    "X_train_small = X_train[:1000, :]\n",
    "X_val_small = X_val[:500, :]\n",
    "y_train_small = y_train[:1000]\n",
    "y_val_small = y_val[:500]\n",
    "\n",
    "n_cols = X_train_small.shape[1]\n",
    "\n",
    "scaler_train = StandardScaler()\n",
    "X_train_scaled = scaler_train.fit_transform(X_train)\n",
    "\n",
    "scaler_val = StandardScaler()\n",
    "X_val_scaled = scaler_val.fit_transform(X_val)\n",
    "\n",
    "y_train_onehot = pd.get_dummies(y_train)\n",
    "y_val_onehot = pd.get_dummies(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation = \"relu\", input_shape = (X_train_scaled.shape[1],)))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = Adam_test(), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_small = X_train[:1000, :]\n",
    "X_val_small = X_val[:500, :]\n",
    "y_train_small = y_train[:1000]\n",
    "y_val_small = y_val[:500]\n",
    "\n",
    "n_cols = X_train_small.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 500 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 73us/sample - loss: 2.1791 - accuracy: 0.2860 - val_loss: 1.8170 - val_accuracy: 0.5060\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 25us/sample - loss: 1.4368 - accuracy: 0.6520 - val_loss: 1.2284 - val_accuracy: 0.6820\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 25us/sample - loss: 0.8300 - accuracy: 0.8120 - val_loss: 0.8157 - val_accuracy: 0.7820\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 26us/sample - loss: 0.4437 - accuracy: 0.8950 - val_loss: 0.6405 - val_accuracy: 0.7980\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 25us/sample - loss: 0.2582 - accuracy: 0.9360 - val_loss: 0.5871 - val_accuracy: 0.8240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1381e4374c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_small, y_train_small, batch_size = 100, epochs = 5, validation_data = (X_val_small, y_val_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOB(Optimizer):\n",
    "    \"\"\"Coin Betting Optimizer from the paper:\n",
    "        https://arxiv.org/pdf/1705.07795.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=100, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize COCOB Optimizer\n",
    "        Args:\n",
    "            alpha: Refer to paper.\n",
    "        \"\"\"\n",
    "        super(COCOB, self).__init__(**kwargs)\n",
    "        self._alpha = alpha\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "    \n",
    "    def get_updates(self, params, loss, contraints=None):\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        shapes = [K.int_shape(p) for p in params]\n",
    "        L = [K.variable(np.full(fill_value=1e-8, shape=shape)) for shape in shapes]\n",
    "        reward = [K.zeros(shape) for shape in shapes]\n",
    "        tilde_w = [K.zeros(shape) for shape in shapes]\n",
    "        gradients_sum = [K.zeros(shape) for shape in shapes]\n",
    "        gradients_norm_sum = [K.zeros(shape) for shape in shapes]\n",
    "    \n",
    "        for p, g, li, ri, twi, gsi, gns in zip(params, grads, L, reward, tilde_w,gradients_sum, gradients_norm_sum):\n",
    "            grad_sum_update = gsi + g\n",
    "            grad_norm_sum_update = gns + K.abs(g)\n",
    "            l_update = K.maximum(li, K.abs(g))\n",
    "            reward_update = K.maximum(ri - g * twi, 0)\n",
    "            new_w = - grad_sum_update / (l_update * (K.maximum(grad_norm_sum_update + l_update, self._alpha * l_update))) * (reward_update + l_update)\n",
    "            param_update = p - twi + new_w\n",
    "            tilde_w_update = new_w            \n",
    "            self.updates.append(K.update(gsi, grad_sum_update))\n",
    "            self.updates.append(K.update(gns, grad_norm_sum_update))\n",
    "            self.updates.append(K.update(li, l_update))\n",
    "            self.updates.append(K.update(ri, reward_update))\n",
    "            self.updates.append(K.update(p, param_update))\n",
    "            self.updates.append(K.update(twi, tilde_w_update))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):        \n",
    "        config = {'alpha': float(K.get_value(self._alpha)) }\n",
    "        base_config = super(COCOB, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.COCOB at 0x1381dd29ac8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COCOB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(COCOB(), Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation = \"relu\", input_shape = (X_train_scaled.shape[1],)))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "model.compile(optimizer = COCOB(), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 500 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 61us/sample - loss: 1.1999 - accuracy: 0.6260 - val_loss: 0.7101 - val_accuracy: 0.7840\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 27us/sample - loss: 0.3049 - accuracy: 0.9140 - val_loss: 0.5743 - val_accuracy: 0.8220\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 25us/sample - loss: 0.1078 - accuracy: 0.9790 - val_loss: 0.5814 - val_accuracy: 0.8480\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 27us/sample - loss: 0.0392 - accuracy: 0.9930 - val_loss: 0.5563 - val_accuracy: 0.8600\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 27us/sample - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.5658 - val_accuracy: 0.8600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1381dd41a08>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_small, y_train_small, batch_size = 100, epochs = 5, validation_data = (X_val_small, y_val_small))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
