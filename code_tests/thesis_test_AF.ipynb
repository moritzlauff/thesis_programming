{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../python/functions\")\n",
    "sys.path.insert(2, \"../python/architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../python/architecture/reproducible.py:14: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_prep_functions import mnist_prep\n",
    "from model_functions import *\n",
    "from plotting_functions import *\n",
    "import no_gpu\n",
    "import reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import six\n",
    "from six.moves import zip  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from tensorflow.python.distribute import distribution_strategy_context\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.optimizer_v2 import adadelta as adadelta_v2\n",
    "from tensorflow.python.keras.optimizer_v2 import adagrad as adagrad_v2\n",
    "from tensorflow.python.keras.optimizer_v2 import adam as adam_v2\n",
    "from tensorflow.python.keras.optimizer_v2 import adamax as adamax_v2\n",
    "from tensorflow.python.keras.optimizer_v2 import ftrl\n",
    "from tensorflow.python.keras.optimizer_v2 import gradient_descent as gradient_descent_v2\n",
    "from tensorflow.python.keras.optimizer_v2 import nadam as nadam_v2\n",
    "from tensorflow.python.keras.optimizer_v2 import optimizer_v2\n",
    "from tensorflow.python.keras.optimizer_v2 import rmsprop as rmsprop_v2\n",
    "from tensorflow.python.keras.utils.generic_utils import deserialize_keras_object\n",
    "from tensorflow.python.keras.utils.generic_utils import serialize_keras_object\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.training import optimizer as tf_optimizer_module\n",
    "from tensorflow.python.training import training_util\n",
    "from tensorflow.python.training.tracking import base as trackable\n",
    "from tensorflow.python.util.tf_export import keras_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.optimizers import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD_test(Optimizer):\n",
    "  \"\"\"Stochastic gradient descent optimizer.\n",
    "\n",
    "  Includes support for momentum,\n",
    "  learning rate decay, and Nesterov momentum.\n",
    "\n",
    "  Arguments:\n",
    "      lr: float >= 0. Learning rate.\n",
    "      momentum: float >= 0. Parameter that accelerates SGD in the relevant\n",
    "        direction and dampens oscillations.\n",
    "      decay: float >= 0. Learning rate decay over each update.\n",
    "      nesterov: boolean. Whether to apply Nesterov momentum.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False, **kwargs):\n",
    "    super(SGD_test, self).__init__(**kwargs)\n",
    "    with K.name_scope(self.__class__.__name__):\n",
    "      self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "      self.lr = K.variable(lr, name='lr')\n",
    "      self.momentum = K.variable(momentum, name='momentum')\n",
    "      self.decay = K.variable(decay, name='decay')\n",
    "    self.initial_decay = decay\n",
    "    self.nesterov = nesterov\n",
    "\n",
    "  def _create_all_weights(self, params):\n",
    "    shapes = [K.int_shape(p) for p in params]\n",
    "    moments = [K.zeros(shape) for shape in shapes]\n",
    "    self.weights = [self.iterations] + moments\n",
    "    return moments\n",
    "\n",
    "  def get_updates(self, loss, params):\n",
    "    grads = self.get_gradients(loss, params)\n",
    "    self.updates = [state_ops.assign_add(self.iterations, 1)]\n",
    "\n",
    "    lr = self.lr\n",
    "    if self.initial_decay > 0:\n",
    "      lr = lr * (  # pylint: disable=g-no-augmented-assignment\n",
    "          1. /\n",
    "          (1. +\n",
    "           self.decay * math_ops.cast(self.iterations, K.dtype(self.decay))))\n",
    "    # momentum\n",
    "    moments = self._create_all_weights(params)\n",
    "    for p, g, m in zip(params, grads, moments):\n",
    "      v = self.momentum * m - lr * g  # velocity\n",
    "      self.updates.append(state_ops.assign(m, v))\n",
    "\n",
    "      if self.nesterov:\n",
    "        new_p = p + self.momentum * v - lr * g\n",
    "      else:\n",
    "        new_p = p + v\n",
    "\n",
    "      # Apply constraints.\n",
    "      if getattr(p, 'constraint', None) is not None:\n",
    "        new_p = p.constraint(new_p)\n",
    "\n",
    "      self.updates.append(state_ops.assign(p, new_p))\n",
    "    return self.updates\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        'lr': float(K.get_value(self.lr)),\n",
    "        'momentum': float(K.get_value(self.momentum)),\n",
    "        'decay': float(K.get_value(self.decay)),\n",
    "        'nesterov': self.nesterov\n",
    "    }\n",
    "    base_config = super(SGD, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.SGD_test at 0x7fc01438c6d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(SGD_test(), Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "isinstance(SGD_test(), tensorflow.python.keras.optimizers.Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "isinstance(SGD_test(), keras.optimizers.Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_data = pd.read_csv(\"../data/MNIST/mnist_train.csv\").sample(frac = 1)\n",
    "test_data = pd.read_csv(\"../data/MNIST/mnist_test.csv\").sample(frac = 1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = mnist_prep()\n",
    "# use samller dataset for increased speed\n",
    "X_train_small = X_train[:1000, :]\n",
    "X_val_small = X_val[:500, :]\n",
    "y_train_small = y_train[:1000]\n",
    "y_val_small = y_val[:500]\n",
    "\n",
    "n_cols = X_train_small.shape[1]\n",
    "\n",
    "scaler_train = StandardScaler()\n",
    "X_train_scaled = scaler_train.fit_transform(X_train)\n",
    "\n",
    "scaler_val = StandardScaler()\n",
    "X_val_scaled = scaler_val.fit_transform(X_val)\n",
    "\n",
    "y_train_onehot = pd.get_dummies(y_train)\n",
    "y_val_onehot = pd.get_dummies(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation = \"relu\", input_shape = (X_train_scaled.shape[1],)))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(optimizer = SGD_test(), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_small = X_train[:1000, :]\n",
    "X_val_small = X_val[:500, :]\n",
    "y_train_small = y_train[:1000]\n",
    "y_val_small = y_val[:500]\n",
    "\n",
    "n_cols = X_train_small.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 500 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 80us/sample - loss: 2.4525 - accuracy: 0.1220 - val_loss: 2.2775 - val_accuracy: 0.1540\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 51us/sample - loss: 2.2278 - accuracy: 0.1850 - val_loss: 2.1344 - val_accuracy: 0.2640\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 57us/sample - loss: 2.0747 - accuracy: 0.2840 - val_loss: 2.0264 - val_accuracy: 0.3380\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 49us/sample - loss: 1.9510 - accuracy: 0.3700 - val_loss: 1.9343 - val_accuracy: 0.3940\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 51us/sample - loss: 1.8414 - accuracy: 0.4450 - val_loss: 1.8506 - val_accuracy: 0.4360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbf63b32790>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_small, y_train_small, batch_size = 100, epochs = 5, validation_data = (X_val_small, y_val_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOB(Optimizer):\n",
    "    \"\"\"Coin Betting Optimizer from the paper:\n",
    "        https://arxiv.org/pdf/1705.07795.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=100, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize COCOB Optimizer\n",
    "        Args:\n",
    "            alpha: Refer to paper.\n",
    "        \"\"\"\n",
    "        super(COCOB, self).__init__(**kwargs)\n",
    "        self._alpha = alpha\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "    \n",
    "    def get_updates(self, params, loss, contraints=None):\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        shapes = [K.int_shape(p) for p in params]\n",
    "        L = [K.variable(np.full(fill_value=1e-8, shape=shape)) for shape in shapes]\n",
    "        reward = [K.zeros(shape) for shape in shapes]\n",
    "        tilde_w = [K.zeros(shape) for shape in shapes]\n",
    "        gradients_sum = [K.zeros(shape) for shape in shapes]\n",
    "        gradients_norm_sum = [K.zeros(shape) for shape in shapes]\n",
    "    \n",
    "        for p, g, li, ri, twi, gsi, gns in zip(params, grads, L, reward, tilde_w,gradients_sum, gradients_norm_sum):\n",
    "            grad_sum_update = gsi + g\n",
    "            grad_norm_sum_update = gns + K.abs(g)\n",
    "            l_update = K.maximum(li, K.abs(g))\n",
    "            reward_update = K.maximum(ri - g * twi, 0)\n",
    "            new_w = - grad_sum_update / (l_update * (K.maximum(grad_norm_sum_update + l_update, self._alpha * l_update))) * (reward_update + l_update)\n",
    "            param_update = p - twi + new_w\n",
    "            tilde_w_update = new_w            \n",
    "            self.updates.append(K.update(gsi, grad_sum_update))\n",
    "            self.updates.append(K.update(gns, grad_norm_sum_update))\n",
    "            self.updates.append(K.update(li, l_update))\n",
    "            self.updates.append(K.update(ri, reward_update))\n",
    "            self.updates.append(K.update(p, param_update))\n",
    "            self.updates.append(K.update(twi, tilde_w_update))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):        \n",
    "        config = {'alpha': float(K.get_value(self._alpha)) }\n",
    "        base_config = super(COCOB, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.COCOB at 0x1688f443d08>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COCOB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(COCOB(), Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Could not interpret optimizer identifier:', <__main__.COCOB object at 0x0000016890119FC8>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-463980bad610>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCOCOB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"categorical_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\morit\\anaconda3\\envs\\master_thesis\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, **kwargs)\u001b[0m\n\u001b[0;32m    326\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_eagerly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'run_eagerly'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m       self.compiled_loss = compile_utils.LossesContainer(\n\u001b[0;32m    330\u001b[0m           loss, loss_weights, output_names=self.output_names)\n",
      "\u001b[1;32mc:\\users\\morit\\anaconda3\\envs\\master_thesis\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_get_optimizer\u001b[1;34m(self, optimizer)\u001b[0m\n\u001b[0;32m    348\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_get_single_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\morit\\anaconda3\\envs\\master_thesis\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 617\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\morit\\anaconda3\\envs\\master_thesis\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 617\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\morit\\anaconda3\\envs\\master_thesis\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_get_single_optimizer\u001b[1;34m(opt)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_single_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m       \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m       if (self._dtype_policy.loss_scale is not None and\n\u001b[0;32m    346\u001b[0m           not isinstance(opt, lso.LossScaleOptimizer)):\n",
      "\u001b[1;32mc:\\users\\morit\\anaconda3\\envs\\master_thesis\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    900\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 902\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Could not interpret optimizer identifier:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: ('Could not interpret optimizer identifier:', <__main__.COCOB object at 0x0000016890119FC8>)"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation = \"relu\", input_shape = (X_train_scaled.shape[1],)))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "model.compile(optimizer = COCOB(), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
