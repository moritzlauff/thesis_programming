{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
     ]
    }
   ],
   "source": [
    "# what are your inputs, and what operation do you want to \n",
    "# perform on each input. For example...\n",
    "inputs = range(10) \n",
    "def processInput(i):\n",
    "    return i * i\n",
    " \n",
    "num_cores = multiprocessing.cpu_count()\n",
    "     \n",
    "results = Parallel(n_jobs=num_cores)(delayed(processInput)(i) for i in inputs)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([[1., 1.],\n",
      "       [1., 1.]]), 1], [array([[2., 2.],\n",
      "       [2., 2.]]), 2], [array([[3., 3.],\n",
      "       [3., 3.]]), 3], [array([[4., 4.],\n",
      "       [4., 4.]]), 4]]\n",
      "<class 'list'>\n",
      "4\n",
      "[array([[1., 1.],\n",
      "       [1., 1.]]), 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "d = {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4}\n",
    "\n",
    "def test_function(val):\n",
    "    return [np.ones((2,2))*val, val]\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "     \n",
    "results = Parallel(n_jobs=num_cores)(delayed(test_function)(val) for val in list(d.values()))\n",
    "\n",
    "print(results)\n",
    "print(type(results))\n",
    "print(len(results))\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "d = {\n",
    "    \"a\": np.random.randint(0, 100, size = (10000,10000)),\n",
    "    \"b\": np.random.randint(0, 100, size = (10000,10000)),\n",
    "    \"c\": np.random.randint(0, 100, size = (10000,10000)),\n",
    "    \"d\": np.random.randint(0, 100, size = (10000,10000)),\n",
    "    \"e\": np.random.randint(0, 100, size = (10000,10000)),\n",
    "    \"f\": np.random.randint(0, 100, size = (10000,10000)),\n",
    "    \"g\": np.random.randint(0, 100, size = (10000,10000)),\n",
    "    \"h\": np.random.randint(0, 100, size = (10000,10000)),\n",
    "    }\n",
    "\n",
    "def test_function(val):\n",
    "    return val**2\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time with for loop: 25.75818419456482 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for i in range(len(d)):\n",
    "    a = list(d.values())[i]**2\n",
    "end_time = time.time()\n",
    "print(\"Time with for loop: {} seconds\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time with parallelization: 13.483019590377808 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "results = Parallel(n_jobs=num_cores, backend = \"threading\")(delayed(test_function)(val) for val in list(d.values()))\n",
    "end_time = time.time()\n",
    "print(\"Time with parallelization: {} seconds\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "# XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../python/architecture\\reproducible.py:14: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, \"../python/functions\")\n",
    "sys.path.insert(2, \"../python/architecture\")\n",
    "\n",
    "from data_prep_functions import mnist_prep\n",
    "from model_functions import *\n",
    "\n",
    "X_train, X_val, y_train, y_val = mnist_prep()\n",
    "\n",
    "# use samller dataset for increased speed\n",
    "X_train_small = X_train[:1000, :]\n",
    "X_val_small = X_val[:500, :]\n",
    "y_train_small = y_train[:1000]\n",
    "y_val_small = y_val[:500]\n",
    "\n",
    "n_cols = X_train_small.shape[1]\n",
    "\n",
    "# EnKF\n",
    "\n",
    "X_train = X_train_small\n",
    "X_test = X_val_small\n",
    "y_train = y_train_small\n",
    "y_test = y_val_small\n",
    "\n",
    "batch_size = 50     # len(X_train)\n",
    "epochs = 10\n",
    "particles = 50\n",
    "early_stopping = False\n",
    "early_stopping_diff = 0.001\n",
    "batch_normal = False # evtl. noch einbauen, obwohl im Paper nicht gemacht (aber Achtung mit den Dimensionen unten!!!)\n",
    "shuffle = True\n",
    "randomization = False\n",
    "\n",
    "layers = 5\n",
    "neurons = [128, 128, 64, 32, 10]\n",
    "n_cols = X_train.shape[1]\n",
    "\n",
    "delta = 0.005\n",
    "h_0 = 2\n",
    "epsilon = 0.5\n",
    "\n",
    "n = len(X_train)\n",
    "num_batches = int(np.ceil(n / batch_size))\n",
    "batch_indices = np.cumsum([0] + list(np.ones(num_batches) * batch_size))\n",
    "batch_indices[-1] = n\n",
    "\n",
    "model_dict = {}\n",
    "weights_dict = {}\n",
    "y_pred_dict = {}\n",
    "jacobian_dict = {}\n",
    "weights_vector_dict = {}\n",
    "train_acc_dict = {}\n",
    "test_acc_dict = {}\n",
    "iteration_dict = {}\n",
    "\n",
    "# init_model already has weights and biases following the Glorot distribution\n",
    "# it can already be used to predict and evaluate, but it is very bad (<10% accuracy)\n",
    "# only used to determine shapes and shape_elements via its weights\n",
    "init_model = nn_model_structure(layers = layers,\n",
    "                                neurons = neurons,\n",
    "                                n_cols = n_cols)\n",
    "init_model = nn_model_compile(init_model,\n",
    "                              optimizer = \"sgd\")\n",
    "weights = init_model.get_weights()\n",
    "# shape contains the shapes of the weight matrices and bias vectors as a list of arrays\n",
    "shapes = [np.array(params.shape) for params in weights]\n",
    "# shape_elements contains the indices of the weights as a vector and tells where to cut\n",
    "shape_elements = np.cumsum([0] + [np.prod(shape) for shape in shapes])\n",
    "\n",
    "for i in range(particles):\n",
    "    # just an initial model with the correct structure regarding neurons, layers, activation functions, Glorot initialization\n",
    "    model = nn_model_structure(layers = layers,\n",
    "                               neurons = neurons,\n",
    "                               n_cols = n_cols)\n",
    "    model = nn_model_compile(model,\n",
    "                             optimizer = \"sgd\")\n",
    "    # for every particle write the model in a dictionary\n",
    "    model_dict[\"model_{}\".format(str(i+1))] = model\n",
    "    \n",
    "    # for every particles write the weights and biases in a dictionary\n",
    "    weights_dict[\"model_{}\".format(str(i+1))] = model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                    .get_weights()\n",
    "    \n",
    "    train_acc_dict[\"model_{}\".format(str(i+1))] = []\n",
    "    test_acc_dict[\"model_{}\".format(str(i+1))] = []\n",
    "    iteration_dict[\"model_{}\".format(str(i+1))] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_function(weights, batch):\n",
    "    \n",
    "    init_model.set_weights(weights)\n",
    "    preds = init_model.predict(batch)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10838141, 0.08183446, 0.1090081 , ..., 0.04664212, 0.07889169,\n",
       "        0.11503457],\n",
       "       [0.03725749, 0.06413373, 0.2979479 , ..., 0.04504841, 0.02094523,\n",
       "        0.22180282],\n",
       "       [0.10595883, 0.04892116, 0.09535667, ..., 0.04889838, 0.05527588,\n",
       "        0.13485013],\n",
       "       ...,\n",
       "       [0.07289157, 0.08365307, 0.11067791, ..., 0.05995196, 0.06411313,\n",
       "        0.14524467],\n",
       "       [0.13053435, 0.08308028, 0.13707957, ..., 0.06505536, 0.06666206,\n",
       "        0.12386499],\n",
       "       [0.12124198, 0.06473883, 0.1367102 , ..., 0.06252322, 0.06677867,\n",
       "        0.10869979]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_dict.values())[0].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = multiprocessing.cpu_count()\n",
    "     \n",
    "results = Parallel(n_jobs=num_cores, backend=\"threading\")(delayed(predict_function)(weight, X_test) for weight in list(weights_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 976us/step - loss: 2.3047 - accuracy: 0.0940\n",
      "16/16 [==============================] - 0s 977us/step - loss: 2.3061 - accuracy: 0.0860\n"
     ]
    }
   ],
   "source": [
    "mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "mean_model = init_model\n",
    "mean_model.set_weights(mean_weights)\n",
    "\n",
    "mean_model_train_acc = np.array(mean_model.evaluate(X_train, y_train)[1])\n",
    "mean_model_test_acc = np.array(mean_model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy after Epoch 1: 0.089\n",
      "Training Accuracy after Epoch 2: 0.086\n",
      "Training Accuracy after Epoch 3: 0.069\n",
      "Training Accuracy after Epoch 4: 0.095\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-673fee9e41ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mmodel_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model_{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model_{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0my_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_cores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"threading\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparticles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;31m# for every particle write the predictions on the training batches in a dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test_env\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test_env\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test_env\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test_env\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loop over all epochs\n",
    "for epoch in range(epochs):\n",
    "    # early stopping\n",
    "    if early_stopping:\n",
    "        if epoch == 0:\n",
    "            test_acc_old = 0\n",
    "        else:\n",
    "            test_acc_new = mean_model_test_acc[epoch]\n",
    "            if np.absolute(test_acc_new - test_acc_old) <= early_stopping_diff:\n",
    "                print(\"STOP: Early Stopping after epoch {} because improvement in test accuracy is only {}.\"\\\n",
    "                                                                     .format(epoch+1, test_acc_new - test_acc_old))\n",
    "                break\n",
    "            test_acc_old = test_acc_new\n",
    "    # shuffle the data\n",
    "    if shuffle:\n",
    "        indices = y_train.sample(frac=1).index\n",
    "        X_batches = [X_train[indices][int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]\n",
    "        y_batches = [y_train.iloc[indices].reset_index(drop = True)[int(batch_indices[i]):int(batch_indices[i+1])] for i in range(len(batch_indices)-1)]   \n",
    "    # loop over all batches\n",
    "    for b in range(num_batches):    \n",
    "        for i in range(particles):\n",
    "            # set new weights for model\n",
    "            model_dict[\"model_{}\".format(str(i+1))].set_weights(weights_dict[\"model_{}\".format(str(i+1))])\n",
    "        \n",
    "        y_preds = Parallel(n_jobs=num_cores, backend=\"threading\")(delayed(predict_function)(weight, X_batches[b]) for weight in list(weights_dict.values()))\n",
    "        for i in range(particles):\n",
    "            # for every particle write the predictions on the training batches in a dictionary\n",
    "            y_pred_dict[\"model_{}\".format(str(i+1))] = y_preds[i]\n",
    "        \n",
    "        for i in range(particles):\n",
    "\n",
    "            # for every particle write the Jacobian in a dictionary\n",
    "            jacobian_dict[\"model_{}\".format(str(i+1))] = (-1) * np.multiply(np.array(y_batches[b]), \n",
    "                                                                            np.array(1 / (y_pred_dict[\"model_{}\".format(str(i+1))] + delta)))\n",
    "            \n",
    "        # compute the mean of the predictions\n",
    "        y_pred_mean = np.mean(list(y_pred_dict.values()), axis = 0)\n",
    "        \n",
    "        # compute the matrix D elementwise\n",
    "        d = np.zeros(shape = (particles, particles))\n",
    "        for k in range(particles):\n",
    "            y_pred_centered = y_pred_dict[\"model_{}\".format(str(k+1))] - y_pred_mean\n",
    "            for j in range(particles):\n",
    "                d[k][j] = np.sum(np.multiply(y_pred_centered, jacobian_dict[\"model_{}\".format(str(j+1))]))\n",
    "        d = np.transpose(d)  \n",
    "        \n",
    "        # compute the scalar h_t\n",
    "        h_t = h_0 / (np.sqrt(np.sum(d**2)) + epsilon)\n",
    "        \n",
    "        # Reshape the weights and biases so that they are no longer matrices and vectores, but now one single vector\n",
    "        for i in range(particles):\n",
    "            weights_array = np.array([])\n",
    "            for j in range(len(weights_dict[\"model_{}\".format(str(i+1))])):\n",
    "                weights_array = np.append(weights_array, np.reshape(weights_dict[\"model_{}\".format(str(i+1))][j], (1, -1)).ravel())\n",
    "            weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_array\n",
    "          \n",
    "        # matrix with particle parameters as row vectors\n",
    "        weights_all_ptcls = np.array(list(weights_vector_dict.values()))\n",
    "\n",
    "        # compute the matrix with the updates for each particle\n",
    "        weights_all_ptcls = weights_all_ptcls - h_t * np.matmul(d, weights_all_ptcls)\n",
    "\n",
    "        for i in range(particles):\n",
    "            # write the updates back into the dictionary\n",
    "            weights_vector_dict[\"model_{}\".format(str(i+1))] = weights_all_ptcls[i]\n",
    "            # reshape the updates, so that they are of the original matrx and vector shape\n",
    "            for l in range(len(shape_elements)-1):\n",
    "                start = shape_elements[l]\n",
    "                end = shape_elements[l+1]\n",
    "                weights_dict[\"model_{}\".format(str(i+1))][l] = np.reshape(weights_vector_dict[\"model_{}\".format(str(i+1))][start:end], tuple(shapes[l]))\n",
    "            if randomization:\n",
    "                # add randomization/ noise to each particle\n",
    "                new_weights = []\n",
    "                # standard deviation for scaled Glorot distribution\n",
    "                for s in range(len(shapes)):\n",
    "                    if shapes[s].shape[0] == 2:\n",
    "                        fan_in = shapes[s][0]\n",
    "                        fan_out = shapes[s][1]\n",
    "                    if shapes[s].shape[0] == 1:\n",
    "                        fan_in = shapes[s-1][0]\n",
    "                        fan_out = shapes[s][0]\n",
    "                    stddev = np.sqrt(np.sqrt(h_t)) * np.sqrt(2 / (fan_in + fan_out))\n",
    "                    noise = np.random.normal(loc = 0.0,\n",
    "                                             scale = stddev,\n",
    "                                             size = tuple(shapes[s]))\n",
    "                    new_weights.append(weights_dict[\"model_{}\".format(str(i+1))][s] + noise)\n",
    "                weights_dict[\"model_{}\".format(str(i+1))] = new_weights\n",
    "                \n",
    "    if randomization:\n",
    "        # randomize particles around their mean\n",
    "        weights_mean = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "        for i in range(particles):\n",
    "            new_weights = []\n",
    "            # standard deviation for Glorot distribution\n",
    "            for s in range(len(shapes)):\n",
    "                if shapes[s].shape[0] == 2:\n",
    "                    fan_in = shapes[s][0]\n",
    "                    fan_out = shapes[s][1]\n",
    "                if shapes[s].shape[0] == 1:\n",
    "                    fan_in = shapes[s-1][0]\n",
    "                    fan_out = shapes[s][0]\n",
    "                stddev = np.sqrt(2 / (fan_in + fan_out))\n",
    "                noise = np.random.normal(loc = 0.0,\n",
    "                                         scale = stddev,\n",
    "                                         size = tuple(shapes[s]))\n",
    "                new_weights.append(weights_mean[s] + noise)\n",
    "            weights_dict[\"model_{}\".format(str(i+1))] = new_weights\n",
    "            \n",
    "    for i in range(particles):\n",
    "        # for every particle write the training accuracy of the current iteration in a dictionary\n",
    "        train_acc_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                  .evaluate(X_train, y_train, verbose = 0)[1])\n",
    "\n",
    "        # for every particle write the test accuracy of the current iteration in a dictionary\n",
    "        test_acc_dict[\"model_{}\".format(str(i+1))].append(model_dict[\"model_{}\".format(str(i+1))]\\\n",
    "                                                                  .evaluate(X_test, y_test, verbose = 0)[1])\n",
    "\n",
    "        # for every particle write the current iteration in a dictionary\n",
    "        iteration_dict[\"model_{}\".format(str(i+1))].append(\"Epoch: {}, Batch: {}.\".format(epoch+1, b+1))\n",
    "            \n",
    "    # update the mean_model\n",
    "    mean_weights = list(np.mean(list(weights_dict.values()), axis = 0))\n",
    "    mean_model.set_weights(mean_weights)\n",
    "    \n",
    "    mean_model_train_acc = np.append(mean_model_train_acc, np.array(mean_model.evaluate(X_train, y_train, verbose = 0)[1]))\n",
    "    mean_model_test_acc = np.append(mean_model_test_acc, np.array(mean_model.evaluate(X_test, y_test, verbose = 0)[1]))\n",
    "    \n",
    "    print(\"Training Accuracy after Epoch {}: {}\".format(str(epoch+1), str(np.round(mean_model_train_acc[-1], 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(\"Calculation time: {} minutes.\".format((end_time - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.094 0.097 0.091 0.087 0.085 0.097 0.101 0.097 0.098 0.095 0.087]\n",
      "[0.086 0.116 0.092 0.09  0.092 0.094 0.106 0.108 0.116 0.11  0.1  ]\n"
     ]
    }
   ],
   "source": [
    "print(np.round(mean_model_train_acc, 3))\n",
    "print(np.round(mean_model_test_acc, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_up_to(number):\n",
    "    return sum(range(1, number + 1))\n",
    "\n",
    "a_pool = multiprocessing.Pool()\n",
    "result = a_pool.map(sum_up_to, range(10))\n",
    "\n",
    "print(reuslt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env (Python 3.7)",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
